<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Java防止SQL注入]]></title>
    <url>%2F2019%2F08%2F07%2FJava%E9%98%B2%E6%AD%A2SQL%E6%B3%A8%E5%85%A5%2F</url>
    <content type="text"><![CDATA[架构和环境 spring-cloud版本：Dalston.SR1 spring-boot版本：1.5.3.RELEASE； ORM：spring-data-jpa 数据库连接池：Druid 问题描述系统被检测出多处大量接口有SQL注入风险，并且指明了存在多种注入的方式 原因框架架构初期约定的JPA与数据库交互没有被程序员恪守，存在大量直接拼接的SQL执行语句，既没有预编译，也没有做危险SQL关键词的统一过滤 解决方案 方案一：采用预编译方式运行SQL，如今基本所有数据库都支持预编译，所以直接使用数据库连接包中的预编译类即可实现，不过所有拼接SQL的代码都需要更改，工作量非常巨大，此篇不再赘述，百度java SQL预编译即可 方案二：在网关拦截处做处理，拦截获取的请求的所有参数，判断参数是否有SQL注入风险的关键词，有危险关键词就一律报错不予执行，这样侵入性太强，大量参数传入多少会给系统造成一定负担，而且有些关键词有可能是需要的 方案三：这个方案是后期才发现的，因为系统使用的数据库连接池是Druid，Druid自带防止SQL注入的配置，在第一个方案被否决，第二个方案发布测试时很多接口被测试和开发怼会有问题，不得已的情况下发现了这个最佳的解决方案 实现代码开门见山，最佳方案三的配置添加配置文件中Druid配置filters= wall表示防止SQL注入 1234spring: datasource: type: com.alibaba.druid.pool.DruidDataSource filters: wall 当然有的项目不是使用Druid做数据库连接池，其他的连接池有些同样自带防SQL注入的配置，可以检索一下 方案二的实现代码SqlInjectionFilter类继承ZuulFilter重写run方法（此类还带有预防XSS攻击的代码，如果配置了Druid配置，可以删除此处预防sql注入相关代码，改为预防XSS攻击的类） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119@Component@Slf4jpublic class SqlInjectionFilter extends ZuulFilter &#123; @Value("$&#123;custom.sql-injection-filter.enabled&#125;") private boolean enabled; @Override public String filterType() &#123; return FilterConstants.PRE_TYPE; &#125; // 自定义过滤器执行的顺序，数值越大越靠后执行，越小就越先执行 @Override public int filterOrder() &#123; return FilterConstants.PRE_DECORATION_FILTER_ORDER - 2; &#125; @Override public boolean shouldFilter() &#123; return enabled; &#125; @Override public Object run() &#123; RequestContext ctx = RequestContext.getCurrentContext(); HttpServletRequest request = ctx.getRequest(); try &#123; //判断需要跳过过滤的url for (String skipUrl : SqlInjectionConfig.getSkipUrls()) &#123; if(-1 != request.getRequestURI().indexOf(skipUrl))&#123; return null; &#125; &#125; // 执行过滤逻辑 InputStream in = ctx.getRequest().getInputStream(); String body = StreamUtils.copyToString(in, Charset.forName("UTF-8")); if (StringUtils.isBlank(body)) &#123; body = JSONObject.fromObject(request.getParameterMap()).toString(); if (StringUtils.isBlank(body)) &#123; return null; &#125; &#125; Map&lt;String, Object&gt; stringObjectMap = cleanXSS(body); JSONObject json = JSONObject.fromObject(stringObjectMap); String newBody = json.toString(); // 如果存在sql注入,直接拦截请求 if (newBody.contains("forbid")) &#123; setUnauthorizedResponse(ctx); &#125; final byte[] reqBodyBytes = newBody.getBytes(); ctx.setRequest(new HttpServletRequestWrapper(request) &#123; @Override public ServletInputStream getInputStream() throws IOException &#123; return new ServletInputStreamWrapper(reqBodyBytes); &#125; @Override public int getContentLength() &#123; return reqBodyBytes.length; &#125; @Override public long getContentLengthLong() &#123; return reqBodyBytes.length; &#125; &#125;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; private Map&lt;String, Object&gt; cleanXSS(String value) &#123; value = value.replaceAll("&lt;", "&amp; lt;").replaceAll("&gt;", "&amp; gt;"); value = value.replaceAll("\\(", "&amp; #40;").replaceAll("\\)", "&amp; #41;"); value = value.replaceAll("'", "&amp; #39;"); value = value.replaceAll("eval\\((.*)\\)", ""); value = value.replaceAll("[\\\"\\\'][\\s]*javascript:(.*)[\\\"\\\']", "\"\""); value = value.replaceAll("script", ""); value = value.replaceAll("[*]", "[" + "*]"); value = value.replaceAll("[+]", "[" + "+]"); value = value.replaceAll("[?]", "[" + "?]"); String badStr = "'|and|exec|execute|insert|select|delete|update|count|drop|chr|mid|master|truncate|" + "char|declare|sitename|net user|xp_cmdshell|;|or|+|create|table|from|grant|group_concat|" + "column_name|information_schema.columns|table_schema|union|where|--|,|like|//|/|%|#"; JSONObject json = JSONObject.fromObject(value); String[] badStrs = badStr.split("\\|"); Map&lt;String, Object&gt; map = json; Map&lt;String, Object&gt; mapjson = new HashMap&lt;&gt;(); for (Map.Entry&lt;String, Object&gt; entry : map.entrySet()) &#123; String value1 = entry.getValue().toString().toLowerCase(); for (String bad : badStrs) &#123; if (-1 != value1.indexOf(bad)) &#123; log.info("拦截的参数#####################"+value1); log.info("拦截的关键字#####################"+bad); value1 = "forbid"; mapjson.put(entry.getKey(), value1); break; &#125; else &#123; mapjson.put(entry.getKey(), entry.getValue()); &#125; &#125; &#125; return mapjson; &#125; private void setUnauthorizedResponse(RequestContext requestContext) &#123; Gson gson = new Gson(); BaseResponse result = new BaseResponse(); result.setCode(ErrorCode.ERR_GLOBAL_PARA_CHECK); result.setMsg("SQL Injection Risk"); requestContext.setResponseBody(gson.toJson(result)); &#125;&#125; SqlInjectionConfig类读取配置文件中需要跳过检查的接口数组 12345678910111213141516171819import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.stereotype.Component;@ConfigurationProperties(prefix = "custom.sql-injection-filter")@Componentpublic class SqlInjectionConfig &#123; public static String[] skipUrls; public static String[] getSkipUrls() &#123; return skipUrls; &#125; public void setSkipUrls(String[] skipUrls) &#123; SqlInjectionConfig.skipUrls = skipUrls; &#125;&#125; yml配置文件,如果想要关闭SQL注入检测将enable改为false即可，如果想要某个接口跳过检测，添加到skipUrls数组即可 1234custom: sql-injection-filter: enabled: true skipUrls: /api-file,/api-source,/api-user/user/info.do]]></content>
      <tags>
        <tag>Java</tag>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件服务经过zuul无法设置Content-Length]]></title>
    <url>%2F2019%2F07%2F15%2F%E6%96%87%E4%BB%B6%E6%9C%8D%E5%8A%A1%E7%BB%8F%E8%BF%87zuul%E6%97%A0%E6%B3%95%E8%AE%BE%E7%BD%AEContent-Length%2F</url>
    <content type="text"><![CDATA[在下载文件的接口中设置Content-Length··· String filePath = &quot;/home/img/123456.jpg&quot; File file = new File(filePath); response.addHeader(&quot;Content-Length&quot;, String.valueOf(file.length())); ···直接访问文件服务接口，接口的Response Headers可以中看到Content-Length的值Content-Disposition: attachment;filename=123456.jpg Content-Length: 125997 Content-Type: application/x-download Date: Mon, 15 Jul 2019 08:28:01 GMT但是经过zuul网关之后，接口的Response Headers就没有这个值了Access-Control-Allow-Credentials: true Access-Control-Allow-Headers: Authenticator Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Authenticator Cache-Control: private Content-Disposition: attachment;filename=123456.jpg Content-Type: application/x-download Date: Mon, 15 Jul 2019 06:55:43 GMT Expires: Thu, 01 Jan 1970 08:00:00 CST Transfer-Encoding: chunked X-Application-Context: demo-zuul:443可以看到此处多了一个参数Transfer-Encoding: chunked,可以简单的理解一下，如果有Transfer-Encoding: chunked存在,那么Content-Length就不存在，问题就出在这里，经过了zuul的接口会默认设置一个Transfer-Encoding: chunked解决办法:只需要在zuul的配置文件中加入如下参数即可zuul.set-content-length = true再次访问接口，查看Response Headers的参数Access-Control-Allow-Credentials: true Access-Control-Allow-Headers: Authenticator Access-Control-Allow-Origin: * Access-Control-Expose-Headers: Authenticator Cache-Control: private Content-Disposition: attachment;filename=123456.jpg Content-Length: 125997 Content-Type: application/x-download Date: Mon, 15 Jul 2019 07:51:45 GMT Expires: Thu, 01 Jan 1970 08:00:00 CST X-Application-Context: demo-zuul:443]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud项目通过Redis共享Session]]></title>
    <url>%2F2019%2F07%2F10%2FSpring%20Cloud%E9%A1%B9%E7%9B%AE%E9%80%9A%E8%BF%87Redis%E5%85%B1%E4%BA%ABSession%2F</url>
    <content type="text"><![CDATA[当分布式系统单个服务启动多次，会面临session不可用的情况，当然也有很多方法可以解决，比如采用nginx中的的ip_hash机制，比如将Session加密存在cookie中等等，此篇简单介绍Spring Cloud项目通过Redis共享session的配置，因为不是用来存储用户信息，只是临时保存一个正确验证码，所以没有考虑跨域的事情，如果需要单点登录的功能建议转战JWT 场景描述用户发送短信之前需要输入正确的图形验证码才能发送短信。 实现思路给前端返回验证码的图片流，将这个正确的验证码值存在session中，每次只要request获取session中塞入的正确验证码的值与用户输入的验证码做对比，判断是否通过，但是服务启动多次不可用 不使用session共享的解决方案将”imgcode_”字段与客户端ip拼接作为key值，正确的验证码作为value值一起存入redis，加上半小时的过期时间 通过Redis共享Sessionpom.xml添加依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt; &lt;/dependency&gt;在子服务的springboot启动类上加上@EnableRedisHttpSession实现session共享 @EnableRedisHttpSession没有其他多余代码，Redis的基础配置不做赘述，重新启动项目会发现客户端不管访问哪一个服务都是同样的session，这样就可以往session塞入想要的值自由发挥了]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[并发过高导致经过zuul的接口无返回]]></title>
    <url>%2F2019%2F07%2F02%2F%E5%B9%B6%E5%8F%91%E8%BF%87%E9%AB%98%E5%AF%BC%E8%87%B4%E7%BB%8F%E8%BF%87zuul%E7%9A%84%E6%8E%A5%E5%8F%A3%E6%97%A0%E8%BF%94%E5%9B%9E%2F</url>
    <content type="text"><![CDATA[废话不多说，先列出几个关键的配置 server: tomcat: max-threads: 2000 ribbon: MaxConnectionsPerHost: 2000 MaxTotalConnections: 10000首先在服务中加个接口，接口中线程等待3秒，用于测试 @RequestMapping(value = &quot;sleep&quot;) public String sleep(){ try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } return &quot;hello sleep&quot;; }用JMeter测试发现接口经过zuul并发数只有50，在zuul服务中添加下面的配置 单个host最大连接数和总连接数 ribbon: MaxConnectionsPerHost: 2000 MaxTotalConnections: 10000这时候再次测试发现并发数到了200，但是远远没到我们设置的2000,在zuul服务中添加下面配置： tomcat处理最大线程数2000，这些配置都只添加在了zuul服务中，发现还是不起作用并发数还是200。原因是我们测试的服务中也需要添加同样的如下配置。再次测试发现并发到了设置的2000。 server: tomcat: max-threads: 2000]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis通过防火墙配置开放局域网连接，禁用外网连接]]></title>
    <url>%2F2019%2F06%2F11%2FRedis%E9%80%9A%E8%BF%87%E9%98%B2%E7%81%AB%E5%A2%99%E9%85%8D%E7%BD%AE%E5%BC%80%E6%94%BE%E5%B1%80%E5%9F%9F%E7%BD%91%E8%BF%9E%E6%8E%A5%EF%BC%8C%E7%A6%81%E7%94%A8%E5%A4%96%E7%BD%91%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[注意：从安全考虑一定要优先配置防火墙规则再开放redis端口，不然一直扫描你端口的大佬会在极短的时间内通过Redis在你的服务器上部署一个挖矿程序（亲身经历） 添加规则允许本地127.0.0.1和局域网的172.19.161.14连接Redis（根据自己的局域网IP更改）iptables -A INPUT -s 127.0.0.1 -p tcp --dport 6379 -j ACCEPT iptables -A INPUT -s 172.19.161.14 -p tcp --dport 6379 -j ACCEPT禁用其他ip连接当前机器的6379接口iptables -A INPUT -p TCP --dport 6379 -j REJECTRedis开放端口在redis.config文件中有个默认配置是 bind 127.0.0.1改成 bind 0.0.0.0重启Redisredis-server redis.config &amp;至此结束 延伸学习查看防火墙规则[root@izuf6barrnre7hruzsy99hz brain]# iptables -nL --line-number Chain INPUT (policy ACCEPT) num target prot opt source destination 1 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 2 ACCEPT tcp -- 172.19.161.14 0.0.0.0/0 tcp dpt:6379 3 ACCEPT tcp -- 127.0.0.1 0.0.0.0/0 tcp dpt:6379 4 REJECT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:6379 reject-with icmp-port-unreachable Chain FORWARD (policy ACCEPT) num target prot opt source destination Chain OUTPUT (policy ACCEPT) num target prot opt source destination删除防火墙规则[root@izuf6barrnre7hruzsy99hz brain]# iptables -nL --line-number Chain INPUT (policy ACCEPT) num target prot opt source destination 1 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 2 ACCEPT tcp -- 172.19.161.14 0.0.0.0/0 tcp dpt:6379 3 ACCEPT tcp -- 127.0.0.1 0.0.0.0/0 tcp dpt:6379 4 REJECT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:6379 reject-with icmp-port-unreachable ··· [root@izuf6barrnre7hruzsy99hz brain]# iptables -D INPUT 2 1 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 2 ACCEPT tcp -- 127.0.0.1 0.0.0.0/0 tcp dpt:6379 3 REJECT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:6379 reject-with icmp-port-unreachable ···]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在IDEA中使用Docker启动Spring Boot服务直接发布到远程服务器]]></title>
    <url>%2F2019%2F06%2F04%2F%E5%9C%A8IDEA%E4%B8%AD%E4%BD%BF%E7%94%A8Docker%E5%90%AF%E5%8A%A8Spring%20Boot%E6%9C%8D%E5%8A%A1%E7%9B%B4%E6%8E%A5%E5%8F%91%E5%B8%83%E5%88%B0%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Docker开启远程访问[root@Charlie docker]# vim /usr/lib/systemd/system/docker.service将原本上面这行改为下面这行（搜到的大量文档都是2375端口，可能是官方教程？索性抄一下吧，吐槽一下不细究）#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock重启dockersystemctl daemon-reload #加载docker守护线程 systemctl restart docker #重启docker开放端口本篇是百度云服务器测试的，其他供应商也大致相同，首先需要在管理服务器后台页面操作安全组相关操作，各个平台不尽相同，其次服务器如果不想关闭防火墙需要另外开放端口开放端口方法点此跳转 创建一个基础的Spring Boot项目创建基础的Spring Boot项目点此跳转 创建一个Dockerfile文件，放在项目的根目录下，内容如下Dockerfile运行Spring Boot项目的参数等详情点此跳转FROM anapsix/alpine-java VOLUME /tmp ADD target/demo-0.0.1-SNAPSHOT.jar app.jar RUN bash -c &apos;touch /app.jar&apos; ENTRYPOINT [&quot;java&quot;,&quot;-Djava.security.egd=file:/dev/./urandom&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;] EXPOSE 8080MAVEN将项目打成jar包右侧有个tab是maven，点击package按钮会生成jar包，每次改代码都需要clean然后再package IDEA安装插件右侧没安装过插件的话会有个Install按钮，安装之后重启idea IDEA配置docker如下图进行配置，在engine API URL 填入自己服务器对应的 tcp://ip:port 如果下方出现Connection successful的字样，表示连接成功 添加Docker启动配置点击Edit Configurations··· 然后点击右上角加号，选择Docker的Dockerfile 按照下图配置，镜像和容器名字取自己喜欢的，Bind ports 此处的配置指的是将容器中的8080端口代理到宿主机的8765端口，然后启动之后通过宿主机的ip:8765即可访问到项目 创建镜像并启动找到刚刚创建的Dockerfile文件，右上角会有启动创建镜像和启动容器的按钮 容器启动之后控制台能看到具体的运行信息，也可以管理镜像和容器的开始启动，非常方便 访问接口刚刚Docker启动配置中将docker容器中的8080端口代理到宿主机的8765端口这里访问 ip:8765/test 就可以查看到项目接口细信息 服务器上的Docker相关信息[root@Charile docker]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE aiguoba/demo v1 077953297ce3 3 minutes ago 159MB [root@Charile docker]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5b78d7f5d654 077953297ce3 &quot;java -Djava.securit…&quot; 3 minutes ago Up 3 minutes 0.0.0.0:8765-&gt;8080/tcp demo_v1]]></content>
      <categories>
        <category>Docker</category>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile指令]]></title>
    <url>%2F2019%2F05%2F30%2FDockerfile%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Dockerfile中有许多指令，详细且全面的介绍参考官方文档 FROM指令格式: FROM &lt;image&gt; [AS &lt;name&gt;]或者 FROM &lt;image&gt;[:&lt;tag&gt;] [AS &lt;name&gt;]或者 FROM &lt;image&gt;[@&lt;digest&gt;] [AS &lt;name&gt;]FROM指令设置一个基础镜像，让后面的指令可以在一个镜像中运行，可以是任何在镜像库中存在的镜像，一般作为是Dockerfile的第一条指令 ARG是唯一可以放在FROM之前的指令 FROM可以在同一个Dockerfile里面出现多次 tag和digest参数不是必填的，如果不填默认是 latest 也就是最后的版本 了解FROM和ARG的交互方式FROM指令可以使用在FROM之前声明的任何ARG变量 ARG CODE_VERSION=latest FROM base:${CODE_VERSION} CMD /code/run-app FROM extras:${CODE_VERSION} CMD /code/run-extrasFROM之前声明的变量如果想在FROM之后再次被其他指令使用需要重新声明 ARG VERSION=latest FROM busybox:$VERSION ARG VERSION RUN echo $VERSION &gt; image_versionRUNRUN指令用于指定镜像被构建时要运行的命令。有两种格式: RUN &lt;command&gt; (shell格式，命令在shell中执行) RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (exec格式)可以用反斜杠\表示换行 RUN /bin/bash -c &apos;source $HOME/.bashrc; \ echo $HOME&apos;等同于 RUN /bin/bash -c &apos;source $HOME/.bashrc; echo $HOME&apos;CMDCMD指令用于指定一个容器启动时要运行的命令。有三种格式: CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] (exec格式,首选) CMD [&quot;param1&quot;,&quot;param2&quot;] (as default parameters to ENTRYPOINT) CMD command param1 param2 (shell格式)如docker run命令可以覆盖CMD指令，如果在Dockerfile文件中指定了CMD指令，而同时在docker run命令行中指定了要运行的命令，命令行的命令会覆盖Dockerfile中的CMD指令。 LABEL给镜像指定标签 LABEL &quot;com.example.vendor&quot;=&quot;ACME Incorporated&quot; LABEL com.example.label-with-value=&quot;foo&quot; LABEL version=&quot;1.0&quot; LABEL description=&quot;This text illustrates \ that label-values can span multiple lines.&quot;可以写在一行 LABEL multi.label1=&quot;value1&quot; multi.label2=&quot;value2&quot; other=&quot;value3&quot; LABEL multi.label1=&quot;value1&quot; \ multi.label2=&quot;value2&quot; \ other=&quot;value3&quot;可以使用 docker inspect 命令查看这些标签 MAINTAINER (deprecated)指定作者信息，指令格式: MAINTAINER &lt;name&gt;官方推荐使用LABEL指令添加作者信息而不是使用MAINTAINER指令,这样就可以使用docker inspect命令查看这些信息 LABEL maintainer=&quot;SvenDowideit@home.org.au&quot;EXPOSE开放容器内的端口给外部,可以指定端口是在TCP还是UDP上侦听，如果未指定协议，则默认值为TCP。 这个指令只是开放容器内部端口给外部，并不能指定宿主的映射端口，如果需要配置映射关系需要在 docker run 用 -p 指定端口映射关系。指令格式 EXPOSE &lt;port&gt; [&lt;port&gt;/&lt;protocol&gt;...]例如开放容器内80的TCP和UDP，格式是一行，当然也可以写成两行 EXPOSE 80/tcp EXPOSE 80/udpENVENV指令用来在镜像构建过程中设置环境变量，后续的RUN可以使用它所创建的环境变量。 指令格式： ENV &lt;key&gt; &lt;value&gt; ENV &lt;key&gt;=&lt;value&gt; ...新建一个Dockerfile添加如下内容 FROM ubuntu ENV env varible生成镜像，运行容器，会看到声明的环境变量 [root@Charlie env]# docker build -t test/env . ··· [root@Charlie env]# docker run -it test/env /bin/bash root@3a27a7e4598f:/# env ··· env=varible ···如果在 docker run -e 指定了某个环境变量值，就会覆盖Dockerfile中原先的配置值 [root@Charlie env]# docker run -it -e &quot;env=run_command&quot; test/env /bin/bash root@0cf1a2954574:/# env ··· env=run_command ···ADD是将宿主机文件复制到镜像中，指令格式： ADD [--chown=&lt;user&gt;:&lt;group&gt;] &lt;src&gt;... &lt;dest&gt; ADD [--chown=&lt;user&gt;:&lt;group&gt;] [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;]&lt;dest&gt;路径的填写可以是容器内的绝对路径，也可以是相对于工作目录的相对路径; &lt;src&gt;可以是一个本地文件或者是一个本地压缩文件，还可以是一个url; 如果把&lt;src&gt;写成一个url，那么ADD就类似于wget命令。 例如将构建目录下的 test.txt 文件复制到镜像中 /usr/local/test.txt 文件 ADD test.txt /usr/local/test.txt文件源也可以使用url的格式，例如下载最新的wordpress压缩包 latest.zip 到/usr/local命名为wordpress.zip ADD https://wordpress.org/latest.zip /usr/local/wordpress.zip如果将本地归档文件（tar archive）,合法的归档文件（gzip、bzip2、xz）指向到文件夹，Docker会自动解压,下面会将latest.tar.gz文件自动解压到镜像的/usr/local/wordpress/目录下 ADD latest.tar.gz /usr/local/wordpress/COPY是将宿主机文件复制到镜像中，与ADD功能相同，区别是ADD可以使用url下载远程服务器的文件复制到镜像中，ADD还可以自动解压某些压缩包，这两个功能COPY都不可以。指令格式： COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;src&gt;... &lt;dest&gt; COPY [--chown=&lt;user&gt;:&lt;group&gt;] [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;] (this form is required for paths containing whitespace)ENTRYPOINTCMD指令用于指定一个容器启动时要运行的命令。指令格式： ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (exec格式,首选) ENTRYPOINT command param1 param2 (shell格式)与CMD功能类似，只能写一条，如果写了多条，只有最后一条生效，上文提到在运行docker run命令行时如果指定了要运行的命令会覆盖Dockerfile中的CMD指令，差异就在这里ENTRYPOINT指令是无法被docker run命令行指定的指令覆盖的。 如果在Dockerfile中同时写了ENTRYPOINT和CMD，它们两个会互相覆盖，谁在最后谁生效。 ENTRYPOINT 和CMD不同组合的执行情况（这里展示效果不好可以直接去官网查看）: No ENTRYPOINT ENTRYPOINT exec_entry p1_entry ENTRYPOINT [“exec_entry”, “p1_entry”] No CMD error, not allowed /bin/sh -c exec_entry p1_entry exec_entry p1_entry CMD [“exec_cmd”, “p1_cmd”] exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry exec_cmd p1_cmd CMD [“p1_cmd”, “p2_cmd”] p1_cmd p2_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry p1_cmd p2_cmd CMD exec_cmd p1_cmd /bin/sh -c exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd VOLUMEVOLUME指令相当于创建一个挂载点，保存数据到容器挂载的这个文件夹。指令格式： VOLUME [&quot;/data&quot;]参数可以是个Json串， VOLUME [&quot;/var/log/&quot;]或者一个或多个值 VOLUME /var/log VOLUME /var/log /var/db当容器中的数据需要持久化的时候需要使用这个命令。 USER设置启动容器的用户名（或者UID）和用户组（或者GID）来运行Dockerfile里面RUN, CMD and ENTRYPOINT 指令。指令格式： USER &lt;user&gt;[:&lt;group&gt;] USER &lt;UID&gt;[:&lt;GID&gt;]WORKDIRWORKDIR指令设置RUN, CMD, ENTRYPOINT, COPY 和 ADD指令的工作目录，如果设置的工作目录不存在会自动创建。指令格式： WORKDIR /path/to/workdirWORKDIR指令可以设置多次，下面例子的pwd命令执行结果是 /a/b/c： WORKDIR /a WORKDIR b WORKDIR c RUN pwdWORKDIR指令还可以解析ENV指令设置的环境变量，下面例子pwd命令执行结果是/path/$DIRNAME ENV DIRPATH /path WORKDIR $DIRPATH/$DIRNAME RUN pwdARGARG指令设置变量，指令格式: ARG &lt;name&gt;[=&lt;default value&gt;]在docker build创建镜像的时候，使用 --build-arg &lt;varname&gt;=&lt;value&gt;来指定参数，如果用户在build镜像时指定了一个参数没有定义在Dockerfile种，那么将有一个Warning [Warning] One or more build-args [foo] were not consumed.一个Dockerfile文件中可以有一个或者多个ARG定义的变量，如下的例子ARG定义的参数都是可用的 FROM busybox ARG user1 ARG buildno ···也可以给变量设置一个默认值 FROM busybox ARG user1=someuser ARG buildno=1 ···ONBUILD当一个Dockerfile文件中使用了ONBUILD指令，生成了镜像A，这个镜像A中是不执行ONBUILD指令的内容的，但是如果镜像B是以镜像A作为基础镜像时，镜像B中就会执行这个ONBUILD指令。指令格式： ONBUILD [INSTRUCTION]STOPSIGNALSTOPSIGNAL命令是的作用是当容器退出时给系统发送什么样的指令。指令格式： STOPSIGNAL signalHEALTHCHECK指令格式： HEALTHCHECK [OPTIONS] CMD command (通过在容器内运行命令检查容器健康状况) HEALTHCHECK NONE (在基础镜像中取消健康检查)HEALTHCHECK命令只能出现一次，如果出现了多次，只有最后一个生效。 CMD后面可以接以下选项： –interval=DURATION (default: 30s) –timeout=DURATION (default: 30s) –start-period=DURATION (default: 0s) –retries=N (default: 3) 返回值： 0: success - 表示容器是健康的 1: unhealthy - 表示容器已经不能工作了 2: reserved - 保留值 如下例子：健康检查命令是：curl -f http://localhost/ || exit 1;两次检查的间隔时间是5秒;命令超时时间为3秒。 HEALTHCHECK --interval=5m --timeout=3s CMD curl -f http://localhost/ || exit 1SHELL指令格式： SHELL [&quot;executable&quot;, &quot;parameters&quot;]SHELL指令允许覆盖用于shell形式的命令的默认shell 。Linux上的默认shell是[&quot;/bin/sh&quot;, “-c”]，而在Windows上[“cmd”, “/S”, “/C”]。 SHELL指令必须以JSON格式写入Dockerfile; SHELL指令可以多次出现。每条SHELL指令都会覆盖所有先前的SHELL指令，并影响所有后续指令。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dcokerfile创建镜像运行Spring Boot项目]]></title>
    <url>%2F2019%2F05%2F30%2FDcokerfile%E5%88%9B%E5%BB%BA%E9%95%9C%E5%83%8F%E8%BF%90%E8%A1%8CSpring%20Boot%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[新建一个spring boot项目初始化一个Spring Boot项目,就新建一个eureka注册中心好了，以此举例说明（源码位置：点此跳转） 打包doe-eurka项目成jar包，没改任何配置的话打出的jar包名是doe-eureka-0.0.1-SNAPSHOT.jar 将此jar包放到服务器中与Dockefile文件目录平级 Dockerfile文件中的内容如下：FROM anapsix/alpine-java VOLUME /tmp ADD doe-eureka-0.0.1-SNAPSHOT.jar app-eureka.jar RUN bash -c &apos;touch /app-eureka.jar&apos; ENTRYPOINT [&quot;java&quot;,&quot;-Djava.security.egd=file:/dev/./urandom&quot;,&quot;-jar&quot;,&quot;/app-eureka.jar&quot;] EXPOSE 12111参数说明： FROM anapsix/alpine-java：基础镜像是anapsix/alpine-java，是Docker Hub上的java镜像，体积很小，项目是以这个java镜像为基础运行的 VOLUME /tmp：创建/tmp目录并持久化到Docker数据文件夹，因为Spring Boot使用的内嵌Tomcat容器默认使用/tmp作为工作目录 ADD doe-eureka-0.0.1-SNAPSHOT.jar app-eureka.jar：将当前目录的doe-eureka-0.0.1-SNAPSHOT.jar包复制到/app-eureka.jar RUN bash -c &#39;touch /app-eureka.jar&#39;：touch已有文件，更新一下复制进来jar包的时间信息 ENTRYPOINT ···：表示容器运行要执行的命令 EXPOSE 12111：开放出12111端口 更多参数详情查看官方文档 创建镜像用Dockerfile创建一个镜像取名为doe/eureka版本为v1。 [root@Charlie docker]# docker build -t doe/eureka:v1 . ··· Successfully built 9d9cbec08724 [root@Charlie docker]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE doe/eureka v1 9d9cbec08724 15 seconds ago 218 MB运行容器映射镜像端口12111到宿主的8212端口，将容器命名为doe_eureka [root@vmyy3syb docker]# docker run -d -p 8212:12111 --name doe_eureka doe/eureka:v1 16ae19df0e9ce068be681e08b8a9536c1d8196535ea7033a3e0da9ec2f116a37查看效果运行的宿主机IP:8212即可查看效果]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[网页首帧优化实践 之 预渲染]]></title>
    <url>%2F2019%2F05%2F22%2F%E7%BD%91%E9%A1%B5%E9%A6%96%E5%B8%A7%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5%20%E4%B9%8B%20%E9%A2%84%E6%B8%B2%E6%9F%93%2F</url>
    <content type="text"><![CDATA[一、前端渲染方式 CSR: 客户端渲染 -&gt; 在浏览器中渲染应用程序 Prerendering: 预渲染 -&gt; 在应用程序构建时,使用静态 HTML 作为其初始状态 SSR: 服务端渲染 -&gt; 在服务器上将应用程序渲染为 HTML Rehydration: 同构 -&gt; 在客户端上“启动” JavaScript 视图，复用服务器渲染的 HTML DOM 树和数据 二、什么是预渲染?⁉️ 三、为什么使用预渲染?SPA (Single-Page-Application) 首屏渲染慢, 白屏时间过长问题、SEO 等问题 更好的 SEO 更快的 FCP 四、预渲染使用场景及特定场景对比4.1 性能指标 TTFB: Time to First Byte (首字节时间) FP: First Paint (首次绘制 -&gt; 标记浏览器渲染任何在视觉上不同于导航前屏幕内容之内容的时间点) FCP: First Contentful Paint (首次内容绘制 -&gt; 标记的是浏览器渲染来自 DOM 第一位内容的时间点) FMP: First Meaningful Paint(首次有意义绘制 -&gt;标记应用是否已渲染可以与用户互动的足够内容的时间点) TTI: Time To Interactive (可交互时间 -&gt; 标记应用已进行视觉渲染并能可靠响应用户输入的时间点) 4.2 特定页面特定场景下的对比 不常改动、公开型页面 忽略并发、服务器性能、带宽 服务器渲染 读取本地资源文件、客户端渲染读取 CDN 文件 需请求列表接口 … CSR Prerendering SSR Rehydration TTFB 快 快 慢 CSR(快)/SSR(慢) FP 快 快 慢 CSR(快)/SSR(慢) FCP 慢 快 快 CSR(慢)/SSR(快) FMP 慢 快 快 CSR(慢)/SSR(快) TTI / / / / SEO 不友好 友好 友好 友好 五、如何实现预渲染六、问题七、参考 Definitions MDN Rendering on the Web 构建时预渲染：网页首帧优化实践 大前端时代，如何做好 C 端业务下的 React SSR？ 性能指标都是些什么鬼? 以用户为中心的性能指标 八、工具 Lighthouse 九、SEO 方案 服务端渲染(SSR) 预渲染(Prerender) 区分 user-agent 十、SEO 常见注意事项 网站结构优化 控制首页链接数量适中 目录层次尽可能扁平化 导航尽可能采用文字, &lt;img /&gt;标签尽可能加上alt和title属性 网站结构布局(头部{logo、导航、用户信息}、主体{面包屑、正文等}、底部{版权信息、友情链接等}) 控制页面大小,减少 http 请求,提高网站的加载速度 网站代码优化 合理设计 HTML元信息, &lt;title /&gt;(特定网页的主题) , &lt;meta description/&gt; (总括性描述), &lt;meta keywords/&gt; (网站关键字), 语义化书写 HTML 代码 &lt;a/&gt; 标签加上 title 属性说明, 使用 rel=&quot;nofllow&quot;告知爬虫不去爬取外部链接 &lt;!-- &lt;meta name=&quot;robots&quot; content=&quot;nofollow&quot; /&gt; --&gt; &lt;a href=&quot;https://www.baidu.com&quot; title=&quot;百度搜索&quot;&gt;去搜搜&lt;/a&gt; 正文标题使用 &lt;h1/&gt; 标签 &lt;img /&gt;标签 使用 alt属性进行说明 &lt;img src=&quot;cat.jpg&quot; width=&quot;300&quot; height=&quot;200&quot; alt=&quot;猫&quot; /&gt; &lt;caption /&gt; 标签定义表格标题 &lt;table&gt; &lt;caption&gt; 标题 &lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;line1&lt;/td&gt; &lt;td&gt;line1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;line2&lt;/td&gt; &lt;td&gt;line2&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;br /&gt;标签只用作文本内容的换行 需要进行内容强调时,使用&lt;strong /&gt;、&lt;em/&gt;标签(&lt;b /&gt;、&lt;i /&gt;仅告知浏览器样式变化) 文本缩进不要使用特殊符号 应当使用 CSS 进行设置。版权符号不要使用特殊符号© 可以直接使用输入法打出版权符号 © 重要内容不要用 JS 输出(爬虫不会读取) 尽量少使用 iframe 框架(爬虫不会读取) 谨慎使用 display：none (爬虫过滤) 网站性能优化 尽可能少的网络请求 CSS 精灵 将多个样式文件或脚本文件合并为一个文件 图片懒加载 控制资源文件优先级(样式优先) 减少 DOM 操作,减少重排(Reflow) 使用 CDN, 浏览器缓存, 启用 GZIP 压缩 伪静态设置(.html)]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker容器]]></title>
    <url>%2F2019%2F05%2F22%2FDocker%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[运行容器命令格式：1docker run [OPTIONS] IMAGE [COMMAND] [ARG...] OPTIONS说明： -a stdin: 指定标准输入输出内容类型，可选 STDIN/STDOUT/STDERR 三项； -d: 后台运行容器，并返回容器ID； -i: 以交互模式运行容器，通常与 -t 同时使用； -P: 随机端口映射，容器内部端口随机映射到主机的高端口 -p: 指定端口映射，格式为：主机(宿主)端口:容器端口 -t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用； –name=”my-ubuntu”: 为容器指定一个名称； –dns 8.8.8.8: 指定容器使用的DNS服务器，默认和宿主一致； –dns-search example.com: 指定容器DNS搜索域名，默认和宿主一致； -h “mars”: 指定容器的hostname； -e username=”charlie”: 设置环境变量； –env-file=[]: 从指定文件读入环境变量； –cpuset=”0-2” or –cpuset=”0,1,2”: 绑定容器到指定CPU运行； -m :设置容器使用内存最大值； –net=”bridge”: 指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型； –link=[]: 添加链接到另一个容器； –expose=[]: 开放一个端口或一组端口； –restart 容器退出时要应用的重启策略。 运行命令：启动一个ubuntu容器docker run -i -t ubuntu,如果本地没有镜像会先去下载，然后再启动 123456789101112[root@izur9gz docker]# docker run -i -t ubuntu /bin/bashUnable to find image 'ubuntu:latest' locallylatest: Pulling from library/ubuntu6abc03819f3e: Pull complete 05731e63f211: Pull complete 0bd67c50d6be: Pull complete Digest: sha256:f08638ec7ddc90065187e7eabdfac3c96e5ff0f6b2f1762cf31a4f49b53000a5Status: Downloaded newer image for ubuntu:latestroot@97e685431765:/# ps -auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 18496 2012 pts/0 Ss 07:49 0:00 /bin/bashroot 510 0.0 0.0 34388 1460 pts/0 R+ 08:07 0:00 ps -aux 在这个容器中运行的ubuntu可以像一个正常的ubuntu系统一样运行，可以在里面做任何事情，比如安装个vim软件 123456789root@97e685431765:/# apt update······root@97e685431765:/# apt install -y vim······root@97e685431765:/# vim myfile 当你想退出这个ubuntu命令行界面只需要输入exit就返回到原来状态了,容器也会随之关闭 123root@97e685431765:/# exitexit[root@izur9gz docker]# 列出容器命令格式1docker ps [OPTIONS] OPTIONS说明： -a :显示所有的容器，包括未运行的; -f :根据条件过滤显示的内容; –format :指定返回值的模板文件; -l :显示最近创建的容器; -n :列出最近创建的n个容器; –no-trunc :不截断输出; -q :静默模式，只显示容器编号; -s :显示总的文件大小。 运行命令：承接上文测试内容，没有正在运行的容器，有一个未运行的容器，容器名字叫“pedantic_snyder”这是启动之后docker自动起得容器名字 12345[root@izur9gz docker]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES[root@izur9gz docker]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES97e685431765 ubuntu "/bin/bash" 25 minutes ago Exited (0) 4 minutes ago pedantic_snyder ​ 容器命名前文的运行容器命令中有个参数--name是用来起名字的，这里也特地说下，命名只能有下划线_，圆点·，横线-三种特殊符号 运行命令： 启动个ubuntu容器然后命名为Charlie-ubuntu，退出容器命令行，然后看生成了一个名为Charlie-ubuntu的容器 1234567[root@izur9gz docker]# docker run --name Charlie-ubuntu -i -t ubuntu /bin/bashroot@c9c17b86b97e:/# exitexit[root@izur9gz docker]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc9c17b86b97e ubuntu "/bin/bash" 11 seconds ago Exited (0) 7 seconds ago Charlie-ubuntu97e685431765 ubuntu "/bin/bash" 38 minutes ago Exited (0) 17 minutes ago pedantic_snyder 容器启动命令格式：1docker start [OPTIONS] CONTAINER [CONTAINER...] OPTIONS说明： –attach , -a：Attach STDOUT/STDERR and forward signals –interactive , -i：Attach container’s STDIN 运行命令：可以启动一个或多个，启动可以用CONTAINER ID也可以用NAMES 1234567891011121314151617181920[root@izur9gz docker]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc9c17b86b97e ubuntu "/bin/bash" 11 seconds ago Exited (0) 7 seconds ago Charlie-ubuntu97e685431765 ubuntu "/bin/bash" 38 minutes ago Exited (0) 17 minutes ago pedantic_snyder[root@izur9gz docker]# docker start Charlie-ubuntuCharlie-ubuntu[root@izur9gz docker]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc9c17b86b97e ubuntu "/bin/bash" 33 minutes ago Up 10 seconds Charlie-ubuntu[root@izur9gz docker]# docker stop Charlie-ubuntuCharlie-ubuntu[root@izur9gz docker]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES[root@izur9gz docker]# docker start Charlie-ubuntu 97e685431765Charlie-ubuntu97e685431765[root@izur9gz docker]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc9c17b86b97e ubuntu "/bin/bash" 35 minutes ago Up 5 seconds Charlie-ubuntu97e685431765 ubuntu "/bin/bash" About an hour ago Up 5 seconds pedantic_snyder 容器停止命令格式：1docker stop [OPTIONS] CONTAINER [CONTAINER...] OPTIONS说明： -t int：在杀死容器之前等待停止的秒数,默认10秒 运行命令：停止名为Charlie-ubuntu容器 12[root@izur9gz docker]# docker stop Charlie-ubuntuCharlie-ubuntu 容器重启命令格式：1docker restart [OPTIONS] CONTAINER [CONTAINER...] OPTIONS说明： -t int：在杀死容器之前等待停止的秒数,默认10秒 运行命令：重启名为Charlie-ubuntu容器 12[root@izur9gz docker]# docker restart Charlie-ubuntuCharlie-ubuntu 连接到正在运行中的容器命令格式：1docker attach [OPTIONS] CONTAINER 运行命令：跟run命令-i -t效果相同，exit退出会停止容器 12[root@izur9gz docker]# docker attach Charlie-ubunturoot@c9c17b86b97e:/# 后台运行容器前文提到的run命令的-d参数表示后台运行容器，并返回容器ID； 后台运行一个ubuntu的容器，里面每隔一秒打印一次hello world 12[root@izur9gz docker]# docker run --name Charlie2-ubuntu -d ubuntu /bin/bash -c "while true;do echo hello world;sleep 1;done"2d543a799a38ad2c00204b633dad77f2ac831b5cd44801d48f5cff59c30b40f8 ​ 获取容器的日志命令格式：1docker logs [OPTIONS] CONTAINER OPTIONS说明： -f : 跟踪日志输出 –since :显示某个开始时间的所有日志 -t : 显示时间戳 –tail :仅列出最新N条容器日志 运行命令：前文有个每秒打印一次hello world的ubuntu容器，查看这个容器后五条日志 1234567[root@izur9gz docker]# docker logs --tail 5 Charlie2-ubuntuhello worldhello worldhello worldhello worldhello world[root@izur9gz docker]# 查看容器中运行的进程信息命令格式：1docker top [OPTIONS] CONTAINER [ps OPTIONS] 运行命令：12345[root@izur9gz docker]# docker top Charlie2-ubuntuUID PID PPID C STIME TTY TIME CMDroot 12260 12210 0 18:22 ? 00:00:00 /bin/bash -c while true;do echo hello world;sleep 1;doneroot 12894 12260 0 18:30 ? 00:00:00 sleep 1[root@izur9gz docker]# 在运行的容器中执行命令命令格式：1docker exec [OPTIONS] CONTAINER COMMAND [ARG...] OPTIONS说明： -d :分离模式: 在后台运行 -i :即使没有附加也保持STDIN 打开 -t :分配一个伪终端 运行命令：先后台运行创建一个newFile文件，然后进入终端查看，exit不会停止容器 12345678910[root@izur9gz docker]# docker exec -d Charlie-ubuntu touch ~/newFile[root@izur9gz docker]# docker exec -t -i Charlie-ubuntu /bin/bashroot@c9c17b86b97e:/# cd ~root@c9c17b86b97e:~# lsnewFileroot@c9c17b86b97e:~# exitexit[root@izuf6barrnre7hruzsy99gz docker]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc9c17b86b97e ubuntu "/bin/bash" 2 hours ago Up 6 minutes Charlie-ubuntu 重启策略run命令的参数--restart有四种策略 策略 用途 no 默认策略，退出时不要自动重新启动容器。 on-failure[:max-retries] 指定退出时尝试重启的次数 unless-stop 始终重新启动容器，但是如果容器正常stopped，然后机器重启或docker服务重启，这种情况下容器将不会被restart always 无论退出状态如何，始终重新启动容器。当您指定always时，docker守护进程将尝试无限期地重新启动容器。无论容器的当前状态如何，容器也将始终在守护进程启动时启动 运行命令：运行redis如果退出会尝试启动三次 1docker run --restart=on-failure:3 redis 获取容器详细信息命令格式：1docker inspect [OPTIONS] NAME|ID [NAME|ID...] OPTIONS说明： -f :指定返回值的模板文件。 -s :显示总的文件大小。 –type :为指定类型返回JSON。 运行命令：回去容器名为Charlie-ubuntu的详情，然后获取详情中的ip地址信息 [root@izur9gz docker]# docker inspect Charlie-ubuntu [ { &quot;Id&quot;: &quot;c9c17b86b97e9528535abd1ffce67945442ca985b91ce3dff3ac4fa82e5e2025&quot;, &quot;Created&quot;: &quot;2019-05-22T08:27:53.091120726Z&quot;, ··· ··· ··· }, &quot;NetworkSettings&quot;: { ··· ··· ··· &quot;IPAddress&quot;: &quot;172.17.0.4&quot;, ··· ··· ··· } } } ] [root@izur9gz docker]# docker inspect --format &apos;{ {.NetworkSettings.IPAddress}}&apos; Charlie-ubuntu 172.17.0.4删除容器命令格式:1docker rm [OPTIONS] CONTAINER [CONTAINER...] 运行命令：1234567[root@izur9gz docker]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESdc834aace694 ubuntu "-d" 16 minutes ago Created loving_vaughan[root@izur9gz docker]# docker rm dc834aace694dc834aace694[root@izur9gz docker]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker创建镜像]]></title>
    <url>%2F2019%2F05%2F21%2FDocker%E5%88%9B%E5%BB%BA%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[用commit创建镜像只做了解，一般都是用Dockerfile创建镜像 命令格式：docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]OPTIONS说明： -a :提交的镜像作者； -c :使用Dockerfile指令来创建镜像； -m :提交时的说明文字； -p :在commit时，将容器暂停 运行命令：我们运行一个ubuntu镜像，安装一个redis，然后exit退出，再commit生成一个带有redis的ubuntu镜像起仓库名为myrepo/redis，跟原先的ubuntu镜像对比发现大小增加了，之后在运行myrepo/redis这个镜像里面就会有安装过的redis [root@izur9gz docker]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 7698f282e524 8 days ago 69.9MB [root@izur9gz docker]# docker run -i -t ubuntu /bin/bash root@b030f46323ce:/# apt update ··· root@b030f46323ce:/# apt -y install redis ··· root@b030f46323ce:/# exit exit [root@izur9gz docker]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES [root@izur9gz docker]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b030f46323ce ubuntu &quot;/bin/bash&quot; 2 minutes ago Exited (0) 23 seconds ago recursing_shannon [root@izur9gz docker]# docker commit b030f46323ce Charlie/redis invalid reference format: repository name must be lowercase [root@izur9gz docker]# docker commit b030f46323ce myrepo/redis sha256:f2686eb560e51da9fe4a39048e14f548a4ad55791a794ffcc79a3bdd953d6604 [root@izur9gz docker]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE myrepo/redis latest f2686eb560e5 25 seconds ago 99.2MB ubuntu latest 7698f282e524 8 days ago 69.9MBDockerfile构建镜像用例子来说明Dockerfile构建镜像的步骤，这个例子是在一个ubuntu中安装nginx，构建出一个新的镜像，用这个新的镜像启动容器，然后访问这个容器nginx欢迎页面。 创建文件夹，Dockerfile文件就放着这个文件夹中mkdir /usr/local/docker进入/usr/local/docker文件夹cd /usr/local/docker创建并编辑Dockerfile文件vim Dockerfile给Dockerfile文件中添加下方内容，使用ubuntu镜像，安装nginx，开放80端口FROM ubuntu RUN apt update RUN apt -y install nginx EXPOSE 80用Dockerfile创建镜像,镜像名字是ubuntu/nginx，版本是v1，. 表示在当前文件夹docker build -t ubuntu/nginx:v1 .查看镜像，直接运行这个镜像就可以生成容器了[root@Charlie docker]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu/nginx v1 019971f67182 3 minutes ago 155MB运行镜像[root@Charlie docker]# docker run -d -p 8787:80 ubuntu/nginx:v1 nginx -g &quot;daemon off;&quot; 9ef5f6f800e00db1ccc2aa57e9ea707d307e7a60d5af77ca5fbd21e8651e4622访问8787端口页面，是nginx欢迎页，当然在浏览器上通过ip:端口方式更加直观[root@Charlie docker]# curl localhost:8787 &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Welcome to nginx!&lt;/title&gt; &lt;style&gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Welcome to nginx!&lt;/h1&gt; &lt;p&gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.&lt;/p&gt; &lt;p&gt;For online documentation and support please refer to &lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt; Commercial support is available at &lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker镜像]]></title>
    <url>%2F2019%2F05%2F21%2FDocker%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[Docker运行容器之前需要有对应运行的镜像，如果本地找不到运行的镜像，docker会去默认的公共镜像仓库Docker Hub下载镜像，这个仓库是可配置的，默认是去Docker Hub下载镜像。 搜索镜像命令格式: docker search [OPTIONS] TERMOPTIONS参数说明： –automated=false :只列出 automated build类型的镜像； –no-trunc=false :显示完整的镜像描述； -s :列出收藏数不小于指定值的镜像。 举例说明，查找nginx的收藏数超过1000的镜像： [root@izur9gz docker]# docker search -s 1000 nginx Flag --stars has been deprecated, use --filter=stars=3 instead NAME DESCRIPTION STARS OFFICIAL AUTOMATED nginx Official build of Nginx. 11436 [OK] jwilder/nginx-proxy Automated Nginx reverse proxy for docker con… 1599 [OK]获取镜像语法：参数TAG是可选的，如果不填，会默认选择lastest标签，也就是获取最新的镜像 docker pull [options] name[:tag|@digest]OPTIONS说明： -a :拉取所有 tagged 镜像； –disable-content-trust :忽略镜像的校验,默认开启。 例如下面拉取ubuntu镜像（等同于docker pull ubuntu:lastest） docker pull ubuntu例如下载18.10版本的ubuntu docker pull ubuntu:18.10运行命令： [root@izur9gz docker]# docker pull ubuntu Using default tag: latest latest: Pulling from library/ubuntu 6abc03819f3e: Pull complete 05731e63f211: Pull complete 0bd67c50d6be: Pull complete Digest: sha256:f08638ec7ddc90065187e7eabdfac3c96e5ff0f6b2f1762cf31a4f49b53000a5 Status: Downloaded newer image for ubuntu:latest [root@izur9gz docker]# docker pull ubuntu:18.10 18.10: Pulling from library/ubuntu 89074f19944e: Pull complete 6cd3a42e50df: Pull complete 26b902a7bf04: Pull complete Digest: sha256:50c1dc36867d3caf13f3c07456b40c57b3e6a4dcda20d05feac2c15e357353d4 Status: Downloaded newer image for ubuntu:18.10查看镜像命令格式： docker images [OPTIONS] [REPOSITORY[:TAG]]OPTIONS说明： -a :列出本地所有的镜像（含中间映像层，默认情况下，过滤掉中间映像层）； –digests :显示镜像的摘要信息； -f :显示满足条件的镜像； –format :指定返回值的模板文件； –no-trunc :显示完整的镜像信息； -q :只显示镜像ID。 获取容器/镜像的元数据命令格式： docker inspect [OPTIONS] NAME|ID [NAME|ID...]OPTIONS说明： -f :指定返回值的模板文件； -s :显示总的文件大小； –type :为指定类型返回JSON。 运行命令： [root@izur9gz docker]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 18.10 d23255d3a3b2 5 days ago 73MB ubuntu latest 7698f282e524 5 days ago 69.9MB nginx latest 53f3fd8007f7 13 days ago 109MB [root@izur9gz docker]# docker images u* REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 18.10 d23255d3a3b2 5 days ago 73MB ubuntu latest 7698f282e524 5 days ago 69.9MB [root@izur9gz docker]# docker inspect d23255d3a3b2 [ { &quot;Id&quot;: &quot;sha256:d23255d3a3b2c0a728990e13ef26a8630c2d836d0f76b845dfa280df6960e871&quot;, &quot;RepoTags&quot;: [ &quot;ubuntu:18.10&quot; ], &quot;RepoDigests&quot;: [ &quot;ubuntu@sha256:50c1dc36867d3caf13f3c07456b40c57b3e6a4dcda20d05feac2c15e357353d4&quot; ], &quot;Parent&quot;: &quot;&quot;, ··· ··· ···删除镜像命令格式： docker rmi [OPTIONS] IMAGE [IMAGE...]OPTIONS说明： -f :强制删除； –no-prune :不移除该镜像的过程镜像，默认移除。 运行命令： [root@izur9gz docker]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 18.10 d23255d3a3b2 5 days ago 73MB ubuntu latest 7698f282e524 5 days ago 69.9MB nginx latest 53f3fd8007f7 13 days ago 109MB [root@izur9gz docker]# docker rmi d23255d3a3b2 Untagged: ubuntu:18.10 Untagged: ubuntu@sha256:50c1dc36867d3caf13f3c07456b40c57b3e6a4dcda20d05feac2c15e357353d4 Deleted: sha256:d23255d3a3b2c0a728990e13ef26a8630c2d836d0f76b845dfa280df6960e871 Deleted: sha256:b51db5d74c64ad915bedf6bea784c847d9a5dac9e8ecd95ac4a57607dbe5e588 Deleted: sha256:6e1303552a837a20c0523ac5134c95810324d65eaa6bbfd21a9f9d964747e277 Deleted: sha256:6276206df905e39c37c16b2a54c1875d9d7fadc73bd0fd39cc027c16c868fd69 [root@izur9gz docker]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 7698f282e524 5 days ago 69.9MB nginx latest 53f3fd8007f7 13 days ago 109MB创建镜像点此跳转 存出和载入镜像存出镜像命令格式： docker save [OPTIONS] IMAGE [IMAGE...]OPTIONS 说明： -o :输出到的文件。 运行命令： [root@izur9gz docker]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 7698f282e524 5 days ago 69.9MB nginx latest 53f3fd8007f7 13 days ago 109MB [root@izur9gz docker]# docker save -o ~/ubuntu.tar ubuntu [root@izur9gz docker]# ll ~ total 70664 -rw------- 1 root root 72356352 May 21 16:53 ubuntu.tar载入镜像命令格式： docker load [OPTIONS]OPTIONS 说明： -i :指定导出的文件； -q :精简输出信息。 导入镜像： docker load --input ~/ubuntu.tar docker load &lt; ~/ubuntu.tar上传镜像命令格式： docker push [OPTIONS] NAME[:TAG]OPTIONS参数说明： –disable-content-trust :忽略镜像的校验,默认开启 上传本地镜像mytest:v1到镜像仓库中（将本地的镜像上传到镜像仓库,要先登陆到镜像仓库） docker push mytest:v1]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker修改默认存储位置]]></title>
    <url>%2F2019%2F05%2F21%2FDocker%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E5%AD%98%E5%82%A8%E4%BD%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[docker默认存储路径在/var/lib/docker下，如果空间不足会引起各种问题，所以需要转移到挂载了大硬盘的目录下，本篇演示是转移到/home/docker目录 停止docker服务 systemctl stop docker创建需要转移的目录 mkdir /home/docker如果有必要的话，转移之前的文件，主要是镜像文件占用空间，如果不需要可以跳过这一步 mv /var/lib/docker /home/docker查看/etc/systemd/system/docker.service.d/devicemapper.conf文件是否存在，没有就新建，有的话直接下一步 mkdir /etc/systemd/system/docker.service.d/ vi /etc/systemd/system/docker.service.d/devicemapper.conf给/etc/systemd/system/docker.service.d/devicemapper.conf文件添加配置信息 [Service] ExecStart= ExecStart=/usr/bin/dockerd --graph=/home/dev/docker重启 systemctl daemon-reload systemctl restart docker systemctl enable docker]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker核心概念]]></title>
    <url>%2F2019%2F05%2F21%2FDocker%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[docker是一种linux容器技术。容器有效的将由单个操作系统挂管理的资源划分到孤立的组中，以便更好的在组之间平衡有冲突的资源使用需求。可简单理解为一种沙盒 。每个容器内运行一个应用，不同的容器之间相互隔离，容器之间也可以建立通信机制。容器的创建和停止都十分快速，资源需求远远低于虚拟机。 Docker三大核心概念：镜像（Image）、容器（Container）、仓库（Repository）。 Docker镜像镜像是创建docker容器的基础，软件打包好的镜像，放在docker仓库中。一个镜像可以是一个nginx应用，也可以是一个ubuntu系统，这些都是镜像，Docker Hub提供大量镜像供大家使用，当然也可以自己构建镜像放在本地的仓库。 Docker容器镜像启动后的实例叫做容器，容器类似于一个轻量级的沙箱，Docker利用容器来运行和隔离应用。镜像本身是只读的，容器从镜像启动的时候，Docker会在镜像的最上层创建一个可写层，镜像本身保持不变。 Docker仓库Docker仓库类似于代码的仓库，是Docker镜像集中存放的场所。 Docker Hub是最大的公开仓库，提供大量镜像供大家下载使用，也可以自己搭建自己的私有仓库，和maven的中央仓库和私有仓库类似。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Centos环境下Docker的安装]]></title>
    <url>%2F2019%2F05%2F20%2FCentos%E7%8E%AF%E5%A2%83%E4%B8%8BDocker%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[删除原先的docker,之前有没有装过都运行一下，不会掉块肉yum remove docker安装必备软件包yum install -y yum-utils device-mapper-persistent-data lvm2添加阿里镜像yum源yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo安装Dockeryum -y install docker-ce启动Dockersystemctl start docker设置开机启动systemctl enable docker查看版本docker version]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Jenkins安装，简单配置，新建任务]]></title>
    <url>%2F2019%2F05%2F13%2Fjenkins-e5-ae-89-e8-a3-85%2F</url>
    <content type="text"><![CDATA[安装下载jenkins的war包 wget http://mirrors.jenkins.io/war-stable/latest/jenkins.war启动jenkins包(此处指定9301端口，不指定默认是8080端口，本篇测试机8080端口被占用所以换个端口),启动完成访问这个端口即可，按照页面的步骤一步一步配置 java -jar jenkins.war --httpPort=9301 &amp;配置如果jenkins安装在远程服务器，需要开放权限，不然首页登录会是一片空白。需要编辑配置文件 vim /root/.jenkins/config.xml将配置文件中原先的配置改为下面的配置 &lt;authorizationStrategy class=&quot;hudson.security.AuthorizationStrategy$Unsecured&quot;/&gt; &lt;securityRealm class=&quot;hudson.security.SecurityRealm$None&quot;/&gt;全局配置：进入界面点击左侧菜单系统管理&gt;全局工具配置，配置一些jdk，maven，git等工具位置 全局安全配置：系统管理&gt;全局安全配置，配置不同账户权限，详见下图注意红框位置的勾选 如果不谨慎没有勾选Jenkins专有用户数据库这个选项，页面将不可用，这时候还是去改配置文件 vim /root/.jenkins/config.xml将useSecurity标签改为false，权限就会回到最初设置jenkins的状态 &lt;useSecurity&gt;false&lt;/useSecurity&gt;新建任务这里胡乱说下，只是用到了执行shell的功能，用法很鸡肋，没有体现出jenkins的强大，个人认为shell可以解决一切问题，所以不论是更新代码还是maven打包全都是shell操作的，没有使用jenkins提供的强大功能 第一步：页面首页左上角菜单第一个新建任务 第二步：选择构建一个自由风格的软件项目，点击确定 第三步：直接到构建这里，选择执行shell，输入自己想执行的shell即可 注意点：例如执行nohup之类的命令，需要在命令前加上 BUILD_ID=dontKillMe]]></content>
      <categories>
        <category>Jekins</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[微信小程序解密手机号]]></title>
    <url>%2F2019%2F05%2F09%2F%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E8%A7%A3%E5%AF%86%E6%89%8B%E6%9C%BA%E5%8F%B7%2F</url>
    <content type="text"><![CDATA[接收三个参数：分别是wx.login()接口获取的code，调起获取手机号按钮的两个参数encryptedData和iv； 调用auth.code2Session接口可以解析出openid，session_key和unionid，session_key是用来解密个人信息的关键参数。 查看登录凭证校验官方文档 查看获取手机号官方文档 注意点：前端在吊起获取手机号的按钮之前一定要先调用wx.login()这个先后顺序一定不能错，不然会出现解密错误的情况 获取手机号方法，结合自己需求改造import net.sf.json.JSONObject; public class getWeixinTelephone { public static String getTelePhone(String code,String encryptedData,String iv) { String code2SessionUrl = &quot;https://api.weixin.qq.com/sns/jscode2session?appid=APPID&amp;secret=SECRET&amp;js_code=JSCODE&amp;grant_type=authorization_code&quot;; String appId = &quot;1234321234&quot;;//填写自己真实的appid String appSecret = &quot;qewretryrutitoyoupu&quot;;//填写自己真实的appSecret code2SessionUrl = code2SessionUrl.replace(&quot;APPID&quot;, appId); code2SessionUrl = code2SessionUrl.replace(&quot;SECRET&quot;, appSecret); code2SessionUrl = code2SessionUrl.replace(&quot;JSCODE&quot;, code); JSONObject jsonObject; try { //后台调用接口获得返回值 jsonObject = JSONObject.fromObject(HttpUtil.get(code2SessionUrl)); } catch (Exception e) { e.printStackTrace(); return &quot;系统错误&quot;; } //调用上面的url的返回值可以解析出openid，session_key和unionid，这里只取用sessionKey用于解密手机号码 String sessionKey = jsonObject.get(&quot;session_key&quot;).toString(); //工具类调用解密方法解密 String result = WeiXinUtils.decrypt(encryptedData, sessionKey, iv); //转化为json格式获取手机号码 JSONObject josn = JSONObject.fromObject(result); return (String) josn.get(&quot;phoneNumber&quot;); } }WeiXinUtils解密工具类import sun.misc.BASE64Decoder; import javax.crypto.Cipher; import javax.crypto.spec.IvParameterSpec; import javax.crypto.spec.SecretKeySpec; import java.nio.charset.StandardCharsets; import java.security.AlgorithmParameters; public class WeiXinUtils { public static String decrypt(String encryptedData, String sessionKey, String ivStr) { try { BASE64Decoder decoder = new BASE64Decoder(); AlgorithmParameters params = AlgorithmParameters.getInstance(&quot;AES&quot;); params.init(new IvParameterSpec(org.apache.commons.codec.binary.Base64.decodeBase64(ivStr))); Cipher cipher = Cipher.getInstance(&quot;AES/CBC/PKCS5Padding&quot;); cipher.init(Cipher.DECRYPT_MODE, new SecretKeySpec(decoder.decodeBuffer(sessionKey), &quot;AES&quot;), params); String originalString = new String(cipher.doFinal(decoder.decodeBuffer(encryptedData)), StandardCharsets.UTF_8); return originalString; } catch (Exception e) { throw new RuntimeException(&quot;AES解密失败&quot;, e); } } }HttpUtil工具类import javax.net.ssl.HostnameVerifier; import javax.net.ssl.HttpsURLConnection; import javax.net.ssl.SSLContext; import javax.net.ssl.SSLSession; import javax.net.ssl.SSLSocketFactory; import javax.net.ssl.TrustManager; import java.io.BufferedReader; import java.io.InputStream; import java.io.InputStreamReader; import java.io.OutputStream; import java.net.HttpURLConnection; import java.net.URL; import java.net.URLEncoder; import java.util.Map; import java.util.Map.Entry; public class HttpUtil { private static final String DEFAULT_CHARSET = &quot;UTF-8&quot;; private static final String _GET = &quot;GET&quot;; // GET private static final String _POST = &quot;POST&quot;;// POST public static final int DEF_CONN_TIMEOUT = 30000; public static final int DEF_READ_TIMEOUT = 30000; /** * 初始化http请求参数 * * @param url * @param method * @param headers * @return * @throws Exception */ private static HttpURLConnection initHttp(String url, String method, Map&lt;String, String&gt; headers) throws Exception { URL _url = new URL(url); HttpURLConnection http = (HttpURLConnection) _url.openConnection(); // 连接超时 http.setConnectTimeout(DEF_CONN_TIMEOUT); // 读取超时 --服务器响应比较慢，增大时间 http.setReadTimeout(DEF_READ_TIMEOUT); http.setUseCaches(false); http.setRequestMethod(method); http.setRequestProperty(&quot;Content-Type&quot;, &quot;application/x-www-form-urlencoded&quot;); http.setRequestProperty( &quot;User-Agent&quot;, &quot;Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.146 Safari/537.36&quot;); if (null != headers &amp;&amp; !headers.isEmpty()) { for (Entry&lt;String, String&gt; entry : headers.entrySet()) { http.setRequestProperty(entry.getKey(), entry.getValue()); } } http.setDoOutput(true); http.setDoInput(true); http.connect(); return http; } /** * 初始化http请求参数 * * @param url * @param method * @return * @throws Exception */ private static HttpsURLConnection initHttps(String url, String method, Map&lt;String, String&gt; headers) throws Exception { TrustManager[] tm = {new MyX509TrustManager()}; System.setProperty(&quot;https.protocols&quot;, &quot;TLSv1&quot;); SSLContext sslContext = SSLContext.getInstance(&quot;TLS&quot;); sslContext.init(null, tm, new java.security.SecureRandom()); // 从上述SSLContext对象中得到SSLSocketFactory对象 SSLSocketFactory ssf = sslContext.getSocketFactory(); URL _url = new URL(url); HttpsURLConnection http = (HttpsURLConnection) _url.openConnection(); // 设置域名校验 http.setHostnameVerifier(new HttpUtil().new TrustAnyHostnameVerifier()); // 连接超时 http.setConnectTimeout(DEF_CONN_TIMEOUT); // 读取超时 --服务器响应比较慢，增大时间 http.setReadTimeout(DEF_READ_TIMEOUT); http.setUseCaches(false); http.setRequestMethod(method); http.setRequestProperty(&quot;Content-Type&quot;, &quot;application/x-www-form-urlencoded&quot;); http.setRequestProperty( &quot;User-Agent&quot;, &quot;Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.146 Safari/537.36&quot;); if (null != headers &amp;&amp; !headers.isEmpty()) { for (Entry&lt;String, String&gt; entry : headers.entrySet()) { http.setRequestProperty(entry.getKey(), entry.getValue()); } } http.setSSLSocketFactory(ssf); http.setDoOutput(true); http.setDoInput(true); http.connect(); return http; } /** * @return 返回类型: * @throws Exception * @description 功能描述: get 请求 */ public static String get(String url, Map&lt;String, String&gt; params, Map&lt;String, String&gt; headers) throws Exception { HttpURLConnection http = null; if (isHttps(url)) { http = initHttps(initParams(url, params), _GET, headers); } else { http = initHttp(initParams(url, params), _GET, headers); } InputStream in = http.getInputStream(); BufferedReader read = new BufferedReader(new InputStreamReader(in, DEFAULT_CHARSET)); String valueString = null; StringBuffer bufferRes = new StringBuffer(); while ((valueString = read.readLine()) != null) { bufferRes.append(valueString); } in.close(); if (http != null) { http.disconnect();// 关闭连接 } return bufferRes.toString(); } public static String get(String url) throws Exception { return get(url, null); } public static String get(String url, Map&lt;String, String&gt; params) throws Exception { return get(url, params, null); } public static String post(String url, String params) throws Exception { HttpURLConnection http = null; if (isHttps(url)) { http = initHttps(url, _POST, null); } else { http = initHttp(url, _POST, null); } OutputStream out = http.getOutputStream(); out.write(params.getBytes(DEFAULT_CHARSET)); out.flush(); out.close(); InputStream in = http.getInputStream(); BufferedReader read = new BufferedReader(new InputStreamReader(in, DEFAULT_CHARSET)); String valueString = null; StringBuffer bufferRes = new StringBuffer(); while ((valueString = read.readLine()) != null) { bufferRes.append(valueString); } in.close(); if (http != null) { http.disconnect();// 关闭连接 } return bufferRes.toString(); } /** * 功能描述: 构造请求参数 * * @return 返回类型: * @throws Exception */ public static String initParams(String url, Map&lt;String, String&gt; params) throws Exception { if (null == params || params.isEmpty()) { return url; } StringBuilder sb = new StringBuilder(url); if (url.indexOf(&quot;?&quot;) == -1) { sb.append(&quot;?&quot;); } sb.append(map2Url(params)); return sb.toString(); } /** * map构造url * * @return 返回类型: * @throws Exception */ public static String map2Url(Map&lt;String, String&gt; paramToMap) throws Exception { if (null == paramToMap || paramToMap.isEmpty()) { return null; } StringBuffer url = new StringBuffer(); boolean isfist = true; for (Entry&lt;String, String&gt; entry : paramToMap.entrySet()) { if (isfist) { isfist = false; } else { url.append(&quot;&amp;&quot;); } url.append(entry.getKey()).append(&quot;=&quot;); String value = entry.getValue(); if (null != value &amp;&amp; value != &quot;&quot;) { url.append(URLEncoder.encode(value, DEFAULT_CHARSET)); } } return url.toString(); } /** * 检测是否https * * @param url */ private static boolean isHttps(String url) { return url.startsWith(&quot;https&quot;); } public class TrustAnyHostnameVerifier implements HostnameVerifier { @Override public boolean verify(String hostname, SSLSession session) { return true;// 直接返回true } } }]]></content>
      <categories>
        <category>微信开发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kafka消费者（customer）—— 从Kafka读取数据]]></title>
    <url>%2F2019%2F04%2F18%2Fkafka%E6%B6%88%E8%B4%B9%E8%80%85%EF%BC%88customer%EF%BC%89%E2%80%94%E2%80%94%20%E4%BB%8EKafka%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[KafkaCustomer概念消费者和消费者群组 如果我们有一个应用程序需要从一个Kafka主题读取消息并验证这些消息，然后再把它们保存起来。应用程序需要创建一个消费者对象，订阅主题并开始接受消息，然后验证消息并保存结果。过一阵子，生产者往主题写入消息的速度超过了应用程序验证证据的速度，这个时候该怎么办？如果只使用单个消费者处理消息，应用程序会远跟不上消息生成的速度。显然，此时很有必要对消费者进行横向伸缩。就像多个生产者可以向同样的主题写入消息一样，我们也可以使用多个消费者从同一个主题读取消息，对消息进行分流。 Kafka消费者从属于消费者群组。一个群组里的消费者订阅的是同一个主题，每个消费者接手主题一部分分区的消息。假设主题T1有4个分区我们创建了消费者C1，它是群组G1里唯一的消费者，我们用它订阅主题T1。消费者C1将收到主题T1全部四个分区的消息，如下图所示： 如果在群组G1里新增一个消费者C2，那么每个消费者将分别从两个分区接收消息。我们假设消费者C1接收分区0和分区2的消息，消费者C2接收分区1和分区3的消息，如下图所示： 如果群组G1有四个消费者，那么每个消费者可以分配到一个分区，如下图所示： 如果我们往群组里添加更多的消费者，超过了主题的分区数量，那么有一部分消费者就会被闲置，不会接收到任何消息，如下图所示： 往群组里面增加消费者是横向伸缩消费能力的最主要方式。Kafka消费者经常会做一些高延迟的操作，比如把数据写入到数据库或者HDFS，或者使用数据进行比较耗时的计算。在这些情况下，单个消费者无法跟上数据生成的速度，所以可以增加更多的消费者，让它们分担负载，每个消费者只处理部分分区的消息，这就是横向伸缩的主要手段。我们有必要为主题创建大量的分区，在负载增长时可以加入更多的消费者。不过不要让消费者的数量超过分区的数量，多余的消费者只会被闲置。 除了通过增加消费者横向伸缩单个应用程序外，还经常出现多个应用程序从同一个主题读取数据的情况。实际上，kafka设计的主要目标之一，就是让Kafka主题里的数据能够满足企业各种应用场景的需求。在这些场景里，每个应用程秀可以获取到所有的消息，而不只是其中一部分。只要保证每个应用程序有自己的消费者群组，就可以让它们获取到主题所有的消息。不同于传统的消息系统，横向伸缩Kafka消费者和消费者群组并不会对性能造成负面影响。 在上面的例子里，如果新增一个只包含一个消费者的群组G2，那么这个消费者将从主题T1上接收所有的消息，与群组G1之间互不影响。群组G2可以增加更多的消费者，每个消费者可以像群组G1那样消费若干个分区，不管有没有其他群组的存在。如下图所示。 简而言之，为每个需要获取一个或者多个主题全部消息的应用程序创建一个消费者群组，然后往群组里添加消费者来伸缩读取能力和处理能力，群组里的每个消费者只处理一部分消息。 消费者群组和分区再平衡 从上文了解到，群组里的消费者共同读取主题的分区。一个新的消费者加入群组时，它读取的是原本由其他消费者读取的消息。当一个消费者被关闭发生崩溃时，它就离开了群组，原本由它读取的分区将由群组里的其他消费者来读取。在主题发生变化时，比如管理员添加了新的分区，会发生分区重分配。 分区的所有权从一个消费者转移到另外一个消费者，这样的行为被称为再均衡。再均衡非常重要，它为消费者群组带来了高可用和伸缩性（我们可以放心地添加或者移除消费者），不过在正常情况下，我们并不希望发生这样的行为。再均衡期间，消费者无法读取消息，造成整个群组一段时间的不可用。另外，当分区被重新分配给另外一个消费者时，消费者当前的读取状态会丢失，它有可能还需要去刷新缓存，在它重新恢复状态之前会拖慢应用程序。 消费者通过向被指派为群组协调器的broker（不同的群组可以有不同的协调器）发送心跳来维持它们和群组的从属关系以及它们对分区的所有权关系。只要消费者以正常的时间间隔发送心跳，就被认为是活跃的，说明它还在读取分区的消息。消费者会在轮询消息（为了获取消息）或者提交偏移量时发送心跳。如果消费者停止发送心跳的时间足够长，会话就会过期，群组协调器认为它已经死亡，就会触发一次再均衡。 如果一个消费者发生崩溃，并停止读取消息，群组协调器会等待几秒钟，确认它死亡了才会触发再均衡。在这几秒的时间里，死掉的消费者不会读取分区的消息。在清理消费者时，消费者会通知协调器它将要离开群组，协调器会立即触发一次再均衡，尽量降低处理停顿。 分配分区时怎么样的一个过程 当消费者要加入群组时，它会向群组协调器发送一个JoinGroup请求。第一个加入群组的消费者将成为“群主”。群主从协调器那里获得群组的成员列表（列表包含了所有最近发送过心跳的消费者，它们被认为是活跃的），并负责给每一个消费者分配分区。它使用一个实现了PartitionAssignor接口的类来决定哪些分区应该被分配给哪个消费者。 分配完毕之后，群组把分配情况列表发送给群组协调器，协调器再把这些信息发送给所有的消费者，每个消费者只能看到自己的分配信息，只有群主知道群组里所有消费者的分配信息。这个过程会在每次再均衡时重复发生。 创建Kafka消费者在读取消息之前，需要创建一个KafkaConsumer对象。创建KafkaConsumer对象和创建KafkaProducer对象非常相似，把想要传给消费者的属性放在Properties对象里。在这里首先介绍三个必要的属性：bootstrap.server、key.deserializer和value.deserializer。 bootstrap.server 该属性指定broker的地址清单，地址的格式为host:port。清单里不需要包含所有broker地址，消费者会从给定的broker里查找到其他的broker信息。不过建议至少要提供两个broker信息，一旦其中一个宕机，消费者仍然能连上集群。 key.deserializer 消费者需要知道如何将这些字节数组转化为java对象。key.deserializer必须被设置为一个实现了org.apache.kafka.common.serializetion.Deserializer接口的类，消费者会使用这个类把接收到的字节数组反序列化成java对象 value.deserializer value.deserializer指定的类会将值反序列化。如果获取的字节数组是字符串序列化的值，可以使用与key.deserializer一样的反序列化器。如果键是整数类型而值是字符串，那么需要使用不同的反序列化器。 group.id 指定KafkaConsumer属于哪一个消费者群组。创建不属于任何一个群组的消费者也是可以的，只是这样做不太正常。 下面演示如何创建一个KafkaConsumer对象： Properties props = new Properties(); props.put(&quot;bootstrap.server&quot;,&quot;broker1:9092,broker2:9092&quot;); props.put(&quot;group.id&quot;,&quot;CountryCounter&quot;); props.put(&quot;key.deserializer&quot;,&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;,&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); KafkaConsumer&lt;String,String&gt; consumer = new KafkaConsumer&lt;String,String&gt;(props);订阅主题创建好消费者之后，下一步可以开始订阅主题了。subscribe()方法接受一个主题列表作为参数，使用起来很简单： //这里为了简单，只创建了一个只包含单个元素的列表，主题名字叫做“customerCountries” consumer.subscribe(Collections.singletonList(&quot;customerCountries&quot;));也可以在调用subscribe()方法时传入一个正则表达式。正则表达式可以匹配多个主题，如果有人创建了新的主题，并且主题的名字与正则表达式匹配，那么会立即触发一次再均衡，消费者就可以读取新添加的主题。如果应用程序需要读取多个主题，并且可以处理不同类型的数据，那么这种订阅方式就很管用。在Kafka和其他系统之间复制数据时，使用正则表达式的方式订阅多个主题是很常见的做法。 要订阅所有与test相关的主题： consumer.subscribe(&quot;test.*&quot;);轮询消息轮询是消费者API的核心，通过一个简单的轮询像服务器请求数据。一旦消费者订阅了主题，轮询就会处理所有的细节，包括群组协调、分区再均衡、发送心跳和获取数据，开发者只需要使用一组简单的API来处理从分区返回的数据。消费者代码的主要部分如下所示： try{ //这是一个无限循环。消费者实际上是一个长期运行的应用程序，他通过持续轮询像Kafka请求数据。 while(true){ ConsumerRecords&lt;String,String&gt; records = consumer.poll(100); for(ConsumerRecord&lt;String,String&gt; record:records){ log.debug(&quot;topic = %s, Partition = %s, offset = %d, customer = %s, country = %s\n&quot;, record.topic(),record.partition(),record.offset(),record.key(),record.value()); int updatedCount = 1; if(custCountryMap.countainsValue(record.value())){ updatedCount = custCountryMap.get(record.value()) + 1; } custCountryMap.put(record.value(),updateCount); JSONObject json = new JSONObject(custCountryMap); System.out.println(json.toString(4)); } }finally{ /** * 关闭消费者。网络连接和socket也会随之关闭，并立即触发一次再均衡， * 而不是等待群组协调器发现它不再发送心跳认定它死亡，因为这需要更长时间，导致整个群组在一段时间内无法读取消息。 */ consumer.close(); } }]]></content>
      <categories>
        <category>kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kafka生产者（producer）—— 向Kafka写入数据]]></title>
    <url>%2F2019%2F04%2F15%2Fkafka%E7%94%9F%E4%BA%A7%E8%80%85%EF%BC%88producer%EF%BC%89%E2%80%94%E2%80%94%20%E5%90%91Kafka%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[生产者发送消息向kafka发送消息的主要步骤如下图： 创建ProducerRecord对象，其中包含主题(topic)和内容(value)两个必传参数，也可以添加键(key)和分区(partition)两个非必传参数； 然后在发送ProducerRecord对象，生产者要先把键值对象序列化成字节数组，这样他们才能在网络上传输； 接着数据传入分区器，如果之前在ProducterRecord对象指定了分区，那么分区器直接把指定的分区返回，如果没有指定分区，那么分区器会根据ProducerRecord对象的键来选择一个分区。选好分区后，生产者就知道往哪个主题和分区发送这条记录了； 然后这条记录被添加到一个记录批次里，这个批次里的所有消息会被发送到相同的主题和分区上； 有一个独立的线程负责把这些记录批次发送到相应的broker上； 服务器在收到这些消息时会返回一个响应，如果消息成功写入kafka，就返回一个RecordMetaData对象，他包含了主题和分区信息，以及记录在分区里的偏移量，如果写入失败则会返回一个错误； 生产者在收到错误之后会尝试重新发送消息，几次之后如果还是失败就返回报错信息。 创建kafka生产者要往kafka写入消息，首先要创建一个生产者对象，并设置一些属性，kafka生产者有三个必选的属性。 bootstrap.servers 该属性指定broker的地址清单，地址的格式为host:port。清单里不需要包含所有broker地址，生产者会从给定的broker里查找到其他的broker信息。不过建议至少要提供两个broker信息，一旦其中一个宕机，生产者仍然能连上集群。 key.serializer broker希望接受到的消息的键值都是字节数组。生产者接口允许使用参数化类型，因此可以把java对象作为键值发送给broker。这样的代码具有良好的可读性，不过生产者需要知道如何将这些java对象转化为字节数组。key.serializer必须被设置为一个实现了org.apache.kafka.common.serializetion.Serializer接口的类，生产者会使用这个类把键对象序列化成字节数组 value.serializer 与key.serialier一样，value.serializer指定的类会将值序列化。如果键和值都是字符串，可以使用与key.serializer一样的序列化器。如果键是整数类型而值是字符串，那么需要使用不同的序列化器。 下面代码片段演示了如何创建一个新的生产者，这里只指定了必要的属性，其他使用默认设置 private Properties kafkaProps = new Properties(); kafkaProps.put(&quot;bootstrap.servers&quot;,&quot;broker1:9092,borker2:9092&quot;); kafkaProps.put(&quot;key.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;); kafkaProps.put(&quot;value.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;); producer = new KafkaProducer&lt;String,String&gt;(kafkaProps);这个接口很简单，通过配置生产者的不同属性就可以很大程度地控制它的行为。 实例化生产者对象后，接下来就是可以开始发送消息了。发行送消息主要有以下三种方式： 发送并忘记（fire-and-forget） 我们把消息发送给服务，但并不关心它是否正常到达，大多数情况下，消息会正常到达，因为kafka是高可用的，而且生产者会自动尝试重发，不过，使用这种方式有时候会丢失一些消息。 同步发送 我们使用send()方法发送消息，他会返回一个Future对象，调用get()方法进行等待，就可以知道消息是否发送成功。 异步发送 我们调用send()方法，并指定一个回调函数，服务器在返回响应时调用该函数。 下面会举几个例子，介绍如何使用上述几种方法来发送消息，以及如何处理可能发生的异常情况。 本篇所有的例子都是使用单线程，但其实生产者是可以使用多线程来发送消息的，刚开始的时候可以使用单个消费者和单个线程。如果需要更高的吞吐量，可以再生产者数量不变的前提下增加线程数量，无果这样做还不够，可以增加生产者数量。 发送消息到Kafka发送并忘记 最简单的消息发送方式如下所示： ProducerRecord&lt;String,String&gt; record = new ProducerRecord&lt;&gt;(&quot;CustomerCountry&quot;,&quot;Precision Products&quot;,&quot;France&quot;); try { producer.send(record); } catch(Exception e){ e.printStackTrace(); }生产者的send()方法将ProducerRecord对象作为参数，所以首先需要创建一个ProducerRecord对象。ProducerRecord有多个构造函数。这里使用其中一个构造函数，需要目标主题的名字好要发送的键和值对象，他们都是字符串。键和值对象的类型必须与序列化器和生产者对象相匹配。 producer的send()方法发送ProducerRecord对象，从生产者的架构图里可以看到，消息先是被放进缓冲区，然后使用单独的线程发送到服务器端。send()方法会返回一个包含RecordMetadata的Future对象，不过因为我们会忽略返回值，所以无法知道消息是否发送成功。如果不关心发送结果，那么可以使用这种发送方式。比如，记录不太重要的应用程序日志。 虽然忽略了发消息时或者服务端可能发生的错误，但是发送消息之前，生产者还是有可能发生其他异常。这些异常有可能是SerializationException(说明序列化消息失败)，BufferExhaustedException或TimeoutException(说明缓冲区已满)，又或者是InterruptException(说明发送线程被中断)。 同步发送消息 最简单的同步发送消息方式如下所示： ProducerRecord&lt;String,String&gt; record = new ProducerRecord&lt;&gt;(&quot;CustomerCountry&quot;,&quot;Precision Products&quot;,&quot;France&quot;); try{ producer.send(record).get(); }catch(Exception e){ e.printStackTrace(); }producer.send()方法返回一个Future对象，然后调用Future对象的get()方法等待Kafka响应。如果服务器返回错误，get()方法会抛出异常。如果没有发生错误，我们会得到一个RecordMetadata对象，可以用它获取消息的偏移量。 如果在发送数据之前或者在发送过程中发生了任何错误，比如broker返回了一个不允许重发消息的异常或者已经超过了重发的次数，那么就会抛出异常被catch捕获。 KafkaProducer一般会发生两类错误。其中一类是可重试错误，这类错误可以通过重发消息来解决。比如对于连接错误，可以通过再次建立连接来解决，“无主（no leader）”错误则可以通过重新为分区选举首领来解决。KafkaProduer可以被配置成自动重试，如果在多次重试后仍无法解决问题，应用程序会收到一个重试异常。另一类错误无法通过重试解决，比如“消息太大”异常。对于这类错误，KakfaPrducer不会进行任何重试，直接抛出异常。 异步发送消息 假设消息在应用程序和Kafka集群之间一个来回需要10ms。如果在发送完每个消息后都等待回应，那么发送100个消息就需要1秒。但是如果只发送消息而不等待回应，那么发送你个100个消息所需要的时间会少很多。大多数时候，我们并不需要等待响应，尽管Kafka会把目标主题，分区信息和消息的偏移量发送回来，但对于发送端的应用程序来说不是必需的。不过在遇到消息发送失败时，我们需要抛出异常、记录错误日志或者把消息写入“错误消息”文件以便日后分析。 为了在异步发送消息的同时能够对异常情况进行处理，生产者提供了回调支持。下面使用回调的一个例子： private class DemoProducerCallback implements Callback{ @Override public void onCompletion(RecordMetadata recordMetadata,Exception e){ if(e != null){ e.printStackTrace(); } } ProducerRecord&lt;String,String&gt; record = new ProducerRecord&lt;&gt;(&quot;CustomerCountry&quot;,&quot;Biomedical Materials&quot;,&quot;USA&quot;); producer.send(record,new DemoProducerCallback()); }为了使用回调，需要一个实现了org.apache.kafka.clients.producer.Callback接口的类，这个接口只有一个onCompletion方法。 如果Kafka返回一个错误，onCompletion方法会抛出一个非空（non null）异常。这里我们只是简单地打印出来，但是生产环境应该有更好的处理方式。 在producer.send()发送消息时传入一个回调对象。 生产者的配置之前介绍了生产者的几个必要的配置参数，接着说说生产者其他很多可配置的参数，在Kafka文档里面都有说明，它们大部分都有很合理的默认值，所以没有必要去修改它们。不过有几个参数在内存使用、性能和可靠性方面对生产者影响比较大，下面介绍一下这些配置参数： acks acks参数指定了必须要有多少个分区副本接受消息，生产者才会认为消息写入是成功的。这个参数对消息丢失的可能性有重要影响。该参数有如下选项： 如果acks=0，生产者在成功写入消息之前不会等待任何来自服务器的响应。也就是说，如果当中出现问题，导致服务器没有收到消息，那么生产者就无从得知，消息也就丢失了。不过，因为生产者不需要等待服务器的响应，所以它可以以网络能够支持的最大速度发送消息，从而达到很高的吞吐量。 如果acks=1,只要集群的首领节点收到消息，生产者就会收到一个来自服务器的成功响应。如果消息无法达到首领节点（比如首领节点崩溃，新的首领还没有被选举出来），生产者会收到一个错误响应，为了避免数据丢失，生产者会重发消息。不过如果一个没有收到消息的节点成为首领，消息还是会丢失。这个时候的吞吐量取决于会用的是同步发送还是异步发送。如果让发送客户端等待服务器的响应（通过调用Future对象的get()方法），显然会增加延迟（在网络上传输一个来回的延迟）。如果客户端使用回调，延迟问题就可以得到缓解，不过吞吐量还是会受发送中消息数量的限制（比如生产者在收到服务器响应之前可以发送多少个消息）。 如果acks=all,只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。这种模式是最安全的，它可以保证布置一个服务器收到消息，就算有服务发生崩溃，整个集群仍然可以运行。不过，他的延迟比acks=1时更高，因为我们要等待不止一个服务器节点接收消息。 buffer.memory 该参数用于设置生产者内存缓冲区的大小，生产者用它缓冲要发送到服务器的消息。如果应用程序发送消息的速度超过发送到服务器的速度，会导致生产者空间不足。这个时候，send()方法调用要么被阻塞，要么抛出异常，取决于如何设置block.on.buffer.full参数(在0.9.0.0版本里被替换成了max.block.ms,表示在抛出异常之前可以阻塞一段时间)。 compression.type 默认情况下，消息发送时不会被压缩。该参数可以设置为snappy、gzip或lz4，它指定了消息被发送给broker之前使用哪一种压缩算法进行压缩。snappy压缩算法由谷歌发明，它占用较少的cpu，却能提供较好的性能和相当可观的压缩比，如果比较关注性能和网络带宽，可以使用这种算法。gzip压缩算法一般会占用较多的cpu，但会提供更高的压缩比，所以如果网络带宽比较有限，可以使用这种算法。使用压缩可以降低网络传输开销和存储开销，而这往往是向kafka发送消息的瓶颈所在。 retries 生产者从服务器收到的错误有可能是临时性的错误（比如分区找不到首领）。在这种情况下，retries参数的值决定了生产者可以重发消息的次数，如果达到这个次数，生产者会放弃重试并返回错误。默认情况下，生产者会在每次重试之间等待100ms，不过可以通过retry.backoff.ms参数来改变这个时间间隔。建议在设置重试次数和重试时间间隔之前，先测试一下恢复一个崩溃节点需要多少时间（比如所有分区选举出首领需要多长时间），让总的重试时间比kafka集群从崩溃中恢复的时间长，否则生产者会过早地放弃重试。不过这些错误不是临时性错误，没办法通过重试来解决（比如“消息太大”错误）。一般情况下，因为生产者会自动进行重试，所以就没必要在代码逻辑里处理那些可重试的错误。你只需要处理那些不可重试的错误或重试次数超出上限的情况。 batch.size 当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算（而不是消息个数）。当批次被填满，批次里的所有消息会被发送出去。不过生产者并不一定都会等到批次被填满才发送，半满的批次，甚至只包含一个消息的批次也有可能被发送。所以就算把批次大小设置得很大，也不会造成延迟，只会占用更多内存而已。但是如果设置得太小，因为生产者需要更加频繁地发送消息，会增加一些额外的开销。 linger.ms 该参数指定了生产者在发送批次之前等待更多消息加入批次的时间。KafkaProducer会在批次填满或者linger.ms达到上限时把批次发送出去。默认情况下，只要有可用的线程，生产者就会把消息发送出去，就算批次里只有一个消息。把linger.ms设置成比0大的数，让生产者在发送批次之前等待一会儿，使更多的消息加入到这个批次。虽然这样会增加延迟，但也会提升吞吐量（因为一次性发送更多的消息，每个消息的开销就变小了）。 client.id 该参数可以是任意的字符串，服务器会用它来识别消息的来源，还可以用在日志和配额指标里。 max.in.flight.requests.per.connection 该参数指定了生产者在收到服务器响应之前可以发送多少个消息。他的值越高，就会占用越多的内存，不过也会提升吞吐量。把它设置为1可以保证消息是按照发送的顺序写入服务器的，即使发生了重试。 timeout.ms、request.timeout.ms和metadata.fetch.timeout.ms request.timeout.ms指定了生产者在发送数据时等待服务器返回响应的时间，metadata.fetch.timeout.ms指定了生产者在获取元数据（比如目标分区的首领是谁）时等待服务器返回响应的时间。如果等待响应超时，那么生产者要么重试发送数据，要么返回一个错误（抛出异常或者执行回调）。timeout.ms指定了broker等待同步副本返回消息确认的时间，与asks的配置相匹配——如果在指定时间内没有收到同步副本的确认，那么boker就会返回一个错误。 max.block.ms 该参数指定了在调用send()方法或使用partitionFor()方法获取元数据时生产者的阻塞时间。当生产者的发送缓冲区已满，或者没有可用的元数据时，这些方法就会阻塞。在阻塞时间达到max.block.ms时，生产者会抛出超时异常。 max.request.size 该参数用于控制生产者发送的请求大小。它可以指能发送的单个消息的最大值，也可以指单个请求里所有消息总的大小。例如，假设这个值为1MB，那么可以发送的单个最大消息为1M，或者生产者可以在单个请求里发送一个批次，该批次包含了1000个消息，每个消息大小为1KB。另外，broker对可接收的消息最大值也有自己的限制（message.max.bytes）,所以两边的配置最好可以匹配，避免生产者发送的消息被broker拒绝。 receive.buffer.bytes和send.buffer.bytes 这两个参数分别指定了TCP socket接收和发送数据包的缓冲区大小。如果它们被设为-1，就使用操作系统的默认值。如果生产者和消费者与broker处于不同的数据中心，那么可以适当地增大这些值，因为跨数据中心的网络一般都会有比较高的延迟和比较低的带宽。 顺序保证 Kafka可以保证同一个分区里的消息是有序的，也就是说，如果生产者按照一定的顺序发送消息，broker就会按照这个顺序把它们写入分区，消费者也会按照同样的顺序读取它们。在某种情况下。顺序是非常重要的。例如，往一个账户存入100元再取出来，这个与先取钱再存钱是截然不同的，不过，有些场景对顺序的要求不是很敏感。 如果把retries设置为非零整数，同时把max.in.flight.requests.per.connection设为比1大的数，那么，如果第一个批次消息写入失败，而第二个批次写入成功，broker会重试写入第一个批次。如果此时第一个批次也写入成功，那么这两个批次的顺序就反过来了。 一般来说，如果某些场景要求消息时有序的，那么消息是否写入成功也是很关键的，所以不建议把retries设为0.可以把max.in.flight.requests.per.connection设为1，这样在生产者尝试发送第一批消息时，就不会有其他的消息发送给broker。不过这样会严重影响生产者的吞吐量，所以只有在对消息的顺序有严格要求的情况下才能这么做。 序列化器创建一个生产者对象必须制定序列化器。前文演示过如何使用默认的字符串序列化器，kafka还提供了整型和字节数组序列化器，不过它们还不足以满足大部分场景的需求。到最后，我们需要序列化的记录类型会越来越多。接下来演示如何开发自己的序列化器，并介绍Avro序列化器作为推荐的备选方案。 自定义序列化器 如果发送到Kafka的对象不是简单的字符串或者整型，那么可以使用序列化框架来创建消息记录，比如Avro、Thrift或者Protobuf，或者使用自定义序列化器。强烈建议使用通用的序列化框架。不过为了了解序列化器的工作原理，也为了说明为什么要使用序列化框架，下面来看看如何自定义一个序列化器。 假设你创建了一个简单的类来表示一个客户： public class Customer { private int customerId; private String customerName; public Customer(int customerId,String customerName){ this.id = customerId; this.name = customerName; } public int getCustomerId(){ return customerId; } public String getCustomerName(){ return customerName; } }现在我们要为这个类创建一个序列化器，它看起来可能是这样的： import org.apache.kafka.common.errors.SerializationException; import java.nio.ByteBuffer; import java.util.Map; public class CustomerSerializer implements Serializer&lt;Customer&gt;{ @Override public void configure(Map configs,boolean isKey){ //不做任何配置 } /** *Customer对象被序列化成： * 表示id的4字节整数 * 表示name长度的4字节整数（如果name为空，则长度为0） * 表示name的N个字节 */ @Override public byte[] serialize(String topic,Customer data){ try{ byte[] serializedName; int stringSize; if(data == null){ return null }else{ if(data.getName != null){ serializedName = data.getCustomerName().getBytes(&quot;utf-8&quot;); stringSize = serializedName.length; }else{ serializedName = new bytes[0]; stringSize = 0; } } ByteBuffer buffer = ByteBuffer.allocate(4 + 4 + stringSize); buffer.putInt(data.getCustomerId()); buffer.putInt(stringSize); buffer.put(serializedName); return buffer.array(); }catch(Exception e){ throw new SerializationException(&quot;Error when serializing Customer to byte[]&quot; + e); } } @Override public void close(){ //不需要关闭任何东西 } }只要使用这个CustomerSerializer，就可以把消息记录定义成ProducerRecord&lt;String,Customer&gt;,并且可以直接把Customer对象传给生产者。这个例子很简单，不过代码看起来很脆弱，如果有多种类型的消费者，可能需要把customerId字段变成长整型,或者为Customer添加startDate字段，这样就会出现新旧消息兼容性的问题。在不同版本的序列化器和反序列化器之间调试兼容性问题着实是个挑战。更糟糕的是如果同一个公司的不同团队都往Kafka写入Customer数据，那么他们就需要使用相同的序列化器，如果序列化器发生改动，他们几乎要在同一时间修改代码。 基于以上原因，所以强烈不建议使用自定义序列化器，推荐直接使用StringSerializer和StringDeserializer，然后使用json作为标准的数据传输格式。 分区在上文书，ProducerRecord对象包含了目标主题、键和值。Kafka的消息时一个个键值对，ProducerRecord对象可以只包含目标主题和值，键可以设置为默认的null，不过大多数应用程序会用到键。键有两个用途：可以作为消息的附加信息，也可以用来决定消息该被写到主题的哪个分区。拥有相同键的消息将被写到同一个分区。也就是说，如果一个进程只从一个主题的分区读取数据，那么具有相同键的所有记录都会被该进程读取。要创建一个包含键值的记录，只需要下面这样创建ProducerRecord对象： ProducerRecord&lt;Integer,String&gt; record = new ProducerRecord&lt;&gt;(&quot;CustomerCountry&quot;,&quot;Laboratory Equipment&quot;,&quot;USA&quot;);如果要创建键为null的消息，不指定键就可以了： ProducerRecord&lt;Integer,String&gt; record = new ProducerRecord&lt;&gt;(&quot;CustomerCountry&quot;,&quot;USA&quot;);如果键值为null，并且使用了默认的分区器，那么记录将被随机地发送到主题内各个可用的分区上。分区器使用轮询（Round Robin）算法将消息均衡地分布到各个分区上。 如果键不为空，并且使用了默认的分区器，那么Kafka会对键进行散列，然后根据散列值把消息映射到特定的分区上，这里的关键之处在于，同一个键总会被映射到同一个分区上，所以在进行映射时，我们会使用主题所有的分区，而不仅仅是可用的分区。这也意味着，如果写入数据的分区时不可用的，那么就会发生错误。但是这种情况很少发生。 只有在不改变分区数量的情况下，键与分区之间的映射才能保持不变。比如，在分区数量保持不变的情况下，可以保证用户12138的记录总是被写到分区3中。在从分区读取数据时，可以进行各种优化。不过，一旦主题增加了新的分区，这些就无法保证了，旧数据仍然在分区3上，但是新的数据有可能被写到其他分区上了，如果要使用键来映射分区，那么最好在创建主题的时候就把分区规划好，而且永远不要增加新的分区。 实现自定义分区策略 除了散列分区之外，有时候也需要对数据进行不一样的分区，假设你是一个B2B供应商，你有一个大客户，他是手持设备Banana的供应商。Banana占据了你的整体业务的10%的份额。如果使用默认的散列分区算法，Banana的账号记录将和其他账号记录一起被分配给相同的分区，导致这个分区比其他分区要大一些。服务器有可能因此出现存储空间不足、处理缓慢等问题。我们需要给Banana分配单独的分区，然后使用散列分区算法处理其他账号。 下面举一个自定义分区器的例子： import org.apache.kafka.clients.producer.Partitioner; import org.apache.kafka.common.Cluster; import org.apache.kafka.common.PartitionInfo; import org.apache.kafka.common.record.InvalidRecordException; import org.apache.kafka.common.utils.Utils; /** *Partitioner接口包含了configure、partition和close这三个方法 *不过此处只是作为演示的硬代码，仅供学习，“Banana”应该通过configure方法传入 */ public class BananaPartitioner implements Partitioner { public void configure(Map&lt;String,?&gt; configs){ } public int partition(String topic,Object key,byte[] keyBytes,Object value,byte[] valueBytes,Cluster cluster){ List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if((keyBytes == null)||(!(key instanceOf String))){ throw new InvalidRecordException(&quot;We expect all messages to have customer name as key&quot;); } if(((String)key).equals(&quot;Banana&quot;)){ //Banana总是被分配到最后一个分区 return numPartitions; } return(Math.abs(Utils.murmur2(keyBytes))%(numPartitions - 1)); } public void close(){ } }]]></content>
      <categories>
        <category>kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kafka简介]]></title>
    <url>%2F2019%2F04%2F12%2Fkafka%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Kafka is a distributed,partitioned,replicated commit logservice。 Kafka是一个分布式、分区、复制的提交日志服务。 kafka对消息保存时根据主题（Topic）进行归类，发送消息者成为生产者（Producer）,消息接受者成为消费者（Consumer）,此外kafka集群有多个kafka实例组成，一个实例就是一个broker。无论是kafka集群，还是producer和consumer都依赖于zookeeper来保证系统可用性为集群保存一些信息 主题（topic）和分区（partition）kafka的消息通过主题（topic）来分类，一个主题（topic）可以分成若干个分区（partition），一个分区就是一个提交日志，消息以追加的方式写入分区（如下图所示），然后以先入先出的顺序读取。 一个主题一般包含若干个分区，因此无法在整个主题范围内保证消息的顺序，但是可以保证消息在单个分区的顺序。 kafka通过分区来实现数据冗余和伸缩性。分区可以分布在不同的服务器，也就是说一个主题可以横跨多个服务器，以此提供比单个服务器更强大的性能。 生产者（producer）和消费者（consumer）生产者（producer）创建消息发布到指定的主题上，生产者默认情况下将消息均衡地分布到主题的所有分区，不关系特定消息被写入到哪个分区；当然消费者也可以指定主题的分区写入。 消费者（consumer）读取消息，订阅一个或者多个主题，并按照消息生成的顺序读取他们，消费者通过检查数据的偏移量（offset）来区分已经读取过的消息。偏移量是一种元数据，是一个不断递增的整数，在创建消息时，kafka会把偏移量添加到消息里面。在给定的分区里，每个消息的偏移量都是唯一的。消费者把每个分区最后读取的消息偏移量保存在zookeeper或者kafka上，如果消费者关闭或者重启他的读取状态不会丢失。 消费者是消费者群组（consumer group）的一部分，也就是说会有一个或者多个消费者共同读取一个主题。群组保证每个分区只能被一个消费者使用（如下图所示），通过这种方式消费者可以消费一个大量消息的主题，如果一个消费者失效，其他的消费者可以接管这个失效的消费者的工作。 broker和集群一个独立的kafka服务器被称为broker。broker接收生产者的消息，为消息设置偏移量，并且提交消息到磁盘保存。broker为消费者提供服务，对读取分区的请求作出响应，返回已经提交到磁盘上的消息，硬件支持的情况下，单个broker可以轻松处理数千个分区以及每秒百万级的消息量。 broker是集群的组成部分，每个集群都有一个broker同时充当了集群控制器的角色（自动从活跃的成员中选举出来）。控制器负责管理工作，包括将分区分配给broker和监控broker。在集群中，一个分区从属于一个broker，该broker被称为分区的首领（leader）。一个分区可以分配给多个broker，这个时候会产生分区复制（如下图所示）。这种复制机制为分区提供了消息冗余，如果有一个broker失效，其他broker可以接管领导权。 保留策略：kafka broker默认的消息保留策略要么是保留一段时间，要么保留到数据达到一定大小，当消息上限达到这些要求时，旧消息会被删除，所以根据配置消息不会无限增长保留。主题也可以单独配置自己的保留策略。 多集群如果使用多个数据中心，就需要在他们之间复制消息，但是kafka的消息复制机制只能在单个集群里进行，不能在多集群中进行。 kafka提供了一个叫做MirrorMaker的工具，可以用来实现集群中的消息复制，MirrorMaker的核心组件包含一个生产者和消费者，两者间通过一个队列相连，这个消费者从一个集群读取消息，生产者把消息发送到另外一个集群上（如下图所示）。]]></content>
      <categories>
        <category>kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[初识kafka—发布与订阅消息系统]]></title>
    <url>%2F2019%2F04%2F11%2F%E5%88%9D%E8%AF%86kafka%E2%80%94%E5%8F%91%E5%B8%83%E4%B8%8E%E8%AE%A2%E9%98%85%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[发布与订阅消息系统的数据的发送者不会把消息直接发送给接受者； 发送者以某种方式对消息进行分类，接受者订阅他们，以便接受特定类型的消息； 发布和订阅系统一般会有一个broker,也就是发布消息的中心点。 没有使用发布订阅消息系统的结构如下图: 如果多个服务之间需要互相获取消息，需要专门写对应的接口，如果只有两个还好说，但是如果服务很多，那就需要给每个服务都写相应的接口，更有甚者需要调动对方数据库直接存取数据，这样显然是不合理的，耦合性很高，应对这样的需求，消息中间件应运而生。 使用了发布订阅消息系统的结构如下图: 这样只需要大家往统一的中间件传递规定的数据和拿数据就可以了]]></content>
      <categories>
        <category>kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[zookeeper+kafka单机安装]]></title>
    <url>%2F2019%2F04%2F09%2Fzookeeper%2Bkafka%E5%8D%95%E6%9C%BA%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[本篇使用的zookeeper+kafka单机安装的版本见下文描述，软件版本不一致可能配置不尽相同，慎重参考。 不论是zookeeper还是kafka的安装都需要安装jdk环境，不再赘述 zookeeper的安装下载zookeeper（本篇使用3.4.14版本） 1# wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz 解压 1# tar -zxvf zookeeper-3.4.14.tar.gz 改解压文件夹名字为zookeeper，方便本文的描述 1# mv zookeeper-3.4.14 zookeeper 进入zookeeper文件夹的conf文件夹，复制一份配置文件为zoo.cfg 12# cd zookeeper/conf/# cp zoo_sample.cfg zoo.cfg 本文是单机zookeeper不配置集群，所以不用修改任何配置，默认的2181端口和其他默认配置即可 启动zookeeper，启动脚本在bin目录下，脚本名是zkServer.sh 12# cd ../# bin/zkServer.sh start kafka安装下载kafka（本篇使用2.11-0.10.2.0版本） 1# wget https://archive.apache.org/dist/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz 解压 1# tar -zxvf kafka_2.11-0.10.2.0.tgz 改解压文件夹名字为kafka，方便本文的描述 1# mv kafka_2.11-0.10.2.0 kafka 修改配置文件(其他默认配置不用动)，这里主要是配置了外网访问 1234567broker.id=0#打开listeners的注释，改为:listeners=PLAINTEXT://0.0.0.0:9092#打开advertised.listeners的注释，改为,本机公网ip是106.12.77.102advertised.listeners=PLAINTEXT://106.12.77.102:9092#zookeeper连接端口zookeeper.connect=192.168.0.4:2181 启动kafka,启动脚本是bin目录下的kafka-server-start.sh，在kafka目录下运行启动（-daemon表示在后台启动，如果查看进程没有启动成功，可以先去掉启动看看启动日志排错） 1# bin/kafka-server-start.sh -daemon config/server.properties]]></content>
      <categories>
        <category>kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记系列目录]]></title>
    <url>%2F2019%2F04%2F04%2Fredis-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-e7-b3-bb-e5-88-97-e7-9b-ae-e5-bd-95%2F</url>
    <content type="text"><![CDATA[本系列参考书是《Redis开发与运维》，没有通篇看完，摘取自己认为比较重要的章节做成笔记，杂糅一些网友智慧结晶，为有缘人的学习和日后自己的复习提供一个材料。 Redis特性 Redis安装、配置、启动、关闭 初识Redis 单线程架构 数据类型：字符串（string） 数据类型：哈希（hash） 数据类型：列表（list） 数据类型：集合（set） 数据类型：有序集合（zset） 单个键管理 遍历键 数据库管理 慢查询分析 redis-cli详解 redis-server详解 redis-benchmark详解 Pipeline 事务 redis.conf配置文件（英文原文） redis.conf配置文件（中文翻译） Lua的基本使用 Redis与Lua 发布订阅 地理信息定位（GEO） 客户端通讯协议RESP Java客户端 持久化机制之RDB 持久化机制之AOF 主从复制 哨兵（Redis Sentinel） 集群（Redis Cluster）]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—集群（Redis Cluster）]]></title>
    <url>%2F2019%2F04%2F02%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E9%9B%86%E7%BE%A4%EF%BC%88Redis%20Cluster%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Redis Cluster是Redis的分布式解决方案，在3.0版本正式推出，有效地解决了Redis分布式方面的需求。当遇到单机内存、并发、流量等瓶颈时，可以采用Cluster架构方案达到负载均衡的目的 数据分布Redis数据分区Redis Cluser采用虚拟槽分区，所有的键根据哈希函数映射到0~16383整数槽内， 计算公式：slot=CRC16(key)&amp;16383。每一个节点负责维护一部分槽以及槽所映射的键值数据,使用CRC16key16383将键映射到槽上如下图所示： Redis虚拟槽分区的特点： - 解耦数据和节点之间的关系，简化了节点扩容和收缩难度 - 节点自身维护槽的映射关系，不需要客户端或者代理服务维护槽分区元数据 - 支持节点、槽、键之间的映射查询，用于数据路由、在线伸缩等场景 数据分区是分布式存储的核心，理解和灵活运用数据分区规则对于掌握Redis Cluster非常有帮助 集群功能限制①key批量操作支持有限。如mset、mget，目前只支持具有相同slot值的key执行批量操作。对于映射为不同slot值的key由于执行mget、mget等操作可能存在于多个节点上因此不被支持 ②key事务操作支持有限。同理只支持多key在同一节点上的事务操作，当多个key分布在不同的节点上时无法使用事务功能 ③key作为数据分区的最小粒度，因此不能将一个大的键值对象如hash、list等映射到不同的节点 ④不支持多数据库空间。单机下的Redis可以支持16个数据库，集群模式下只能使用一个数据库空间，即db0 ⑤复制结构只支持一层，从节点只能复制主节点，不支持嵌套树状复制结构 搭建准备节点Redis集群一般由多个节点组成，节点数量至少为6个才能保证组成完整高可用的集群。每个节点需要开启配置cluster-enabled yes，让Redis运行在集群模式下，建议为集群内所有节点统一目录，一般划分三个目录：conf、data、log，分别存放配置、数据和日志相关文件。把6个节点配置统一放在conf目录下，集群相关配置如下： #节点端口 port 6380 #后台运行 daemonize yes #启动生成的pidfile文件 pidfile /var/run/redis_6380.pid #开启集群模式 cluster-enabled yes #节点超时时间，单位毫秒 cluster-node-timeout 15000 #集群内部配置文件 cluster-config-file &quot;nodes-6380.conf&quot;虽然本篇是说集群，但是还是在单机学习集群的部署，如果是多主机部署需要开启外网访问的相关配置： #如果是同一局域网多主机需要bind指向本地的内网ip, #通过ipconfig查看本地ip，本篇测试机IP为192.168.9.15，所以写成如下配置 bind 192.168.9.15 #如果不是同一局域网，需要开启外网访问，需要注释所有指向的bind配置， #默认配置只需要注释下面这一行并关闭保护模式 #bind 127.0.0.1 #关闭保护模式把默认配置的yes改成no protected-mode no可选配置 #如果开启RDB要改一下文件名 dbfilename &quot;dump-6380.rdb&quot; #AOF日志开启，可以根据自身需求设置 appendonly yes #如果开启AOF记录则改一下记录的文件名对应端口 appendfilename &quot;appendonly-6380.aof&quot;其他配置和单机模式一致即可，配置文件命名规则redis-{port}.conf，准备好配置后启动所有节点，命令如下： redis-server conf/redis-6379.conf redis-server conf/redis-6380.conf redis-server conf/redis-6381.conf redis-server conf/redis-6382.conf redis-server conf/redis-6383.conf redis-server conf/redis-6384.conf查看集群服务是否都已经启动，集群后面带一个[cluster]的标识 [root@vmzq1l0l redis]# ps -ef|grep redis|grep -v grep root 20868 1 0 09:14 ? 00:00:11 redis-server 127.0.0.1:6379 [cluster] root 20877 1 0 09:14 ? 00:00:06 redis-server 127.0.0.1:6383 [cluster] root 20882 1 0 09:14 ? 00:00:05 redis-server 127.0.0.1:6384 [cluster] root 31781 1 0 Apr03 ? 00:01:17 redis-server 127.0.0.1:6380 [cluster] root 31786 1 0 Apr03 ? 00:01:16 redis-server 127.0.0.1:6381 [cluster] root 31791 1 0 Apr03 ? 00:01:16 redis-server 127.0.0.1:6382 [cluster]注意：如果有某一个节点无法启动，大概率是配置文件配置疏忽大意漏改造成冲突，可以查看启动日志排查原因。 6379节点启动成功，第一次启动时如果没有集群配置文件，它会自动创建一份，文件名称采用cluster-config-file参数项控制，建议采用node-{port}.conf格式定义，通过使用端口号区分不同节点，防止同一机器下多个节点彼此覆盖，造成集群信息异常。如果启动时存在集群配置文件，节点会使用配置文件内容初始化集群信息。Redis集群模式启动过程如下图所示： 集群模式的Redis除了原有的配置文件之外又加了一份集群配置文件。当集群内节点信息发生变化，如添加节点、节点下线、故障转移等。节点会自动保存集群状态到配置文件中。需要注意的是，Redis自动维护集群配置文件，不要手动修改，防止节点重启时产生集群信息错乱。 查看6379启动生成的集群配置文件 [root@vmzq1l0l redis]# cat nodes-6379.conf 4799ea69c91712f3f0b38d786b2de9fca10343ea :6379@16379 myself,master - 0 0 0 connected 12807 15350 vars currentEpoch 0 lastVoteEpoch 0文件内容记录了集群初始状态，这里最重要的是节点ID，它是一个40位16进制字符串，用于唯一标识集群内一个节点，之后很多集群操作都要借助于节点ID来完成。 也可以使用cluster nodes命令在客户端获取集群节点状态： 127.0.0.1:6379&gt; cluster nodes 4799ea69c91712f3f0b38d786b2de9fca10343ea :6379@16379 myself,master - 0 0 0 connected 12807 15350创建集群官方提供了redis-trib.rb工具用于创建集群 redis-trib.rb是采用Ruby实现的Redis集群管理工具，内部通过Cluster相关命令帮我们简化集群创建、检查、槽迁移和均衡等常见运维操作，使用之前需要安装Ruby依赖环境 # yum -y install ruby ruby-devel rubygems rpm-build ··· # gem install redis ···安装redis-trib.rb，在redis的src文件夹中，将redis-trib.rb直接复制到/usr/local/bin目录 [root@vmzq1l0l ~]# cd /usr/lcoal/redis/src [root@vmzq1l0l src]# cp redis-trib.rb /usr/local/bin/运行redis-trib.rb创建集群 [root@vmzq1l0l src]# redis-trib.rb create --replicas 1 127.0.0.1:6379 127.0.0.1:6380 127.0.0.1:6381 127.0.0.1:6382 127.0.0.1:6383 127.0.0.1:6384 WARNING: redis-trib.rb is not longer available! You should use redis-cli instead. All commands and features belonging to redis-trib.rb have been moved to redis-cli. In order to use them you should call redis-cli with the --cluster option followed by the subcommand name, arguments and options. Use the following syntax: redis-cli --cluster SUBCOMMAND [ARGUMENTS] [OPTIONS] Example: redis-cli --cluster create 127.0.0.1:6379 127.0.0.1:6380 127.0.0.1:6381 127.0.0.1:6382 127.0.0.1:6383 127.0.0.1:6384 --cluster-replicas 1 To get help about all subcommands, type: redis-cli --cluster help如果你装的版本是redis5.0之前的的这里应该不会有问题，直接输入yes就可以部署成功， 但是这里报了个错：“警告：redis-trib.rb不再可用”，而且告知我们使用redis-cli创建集群的语法，Redis5.0之后所有原来redis-trib.rb命令都可以用redis-cli –cluster替换 WARNING: redis-trib.rb is not longer available!redis5.0之后不再使用redis-trib.rb创建集群了，而是使用redis-cli作为创建集群的命令,这意味着不用再装ruby，有如下命令创建集群（–cluster-replicas参数指定集群中每个主节点配备几个从节点，这里设置为1。） [root@vmzq1l0l redis]# redis-cli --cluster create 127.0.0.1:6379 127.0.0.1:6380 127.0.0.1:6381 127.0.0.1:6382 127.0.0.1:6383 127.0.0.1:6384 --cluster-replicas 1 &gt;&gt;&gt; Performing hash slots allocation on 6 nodes... Master[0] -&gt; Slots 0 - 5460 Master[1] -&gt; Slots 5461 - 10922 Master[2] -&gt; Slots 10923 - 16383 Adding replica 127.0.0.1:6382 to 127.0.0.1:6379 Adding replica 127.0.0.1:6383 to 127.0.0.1:6380 Adding replica 127.0.0.1:6384 to 127.0.0.1:6381 &gt;&gt;&gt; Trying to optimize slaves allocation for anti-affinity [WARNING] Some slaves are in the same host as their master M: 6d9658051472fd42328e0c2c0fa25d5f77051343 127.0.0.1:6379 slots:[0-5460] (5461 slots) master M: e27fae258be099a4930fe40854d30384646e33f2 127.0.0.1:6380 slots:[5461-10922] (5462 slots) master M: 8c60e2b86699769ae3d7473712ba1eb5034894c6 127.0.0.1:6381 slots:[10923-16383] (5461 slots) master S: bff23ec0350b4e5d0be694f1efa9a9385786a443 127.0.0.1:6382 replicates 8c60e2b86699769ae3d7473712ba1eb5034894c6 S: cfaf4e715eede56ec39a939f826ec394c92678c3 127.0.0.1:6383 replicates 6d9658051472fd42328e0c2c0fa25d5f77051343 S: ef69743523f99bfeb39272eeef98f7b29e1e2217 127.0.0.1:6384 replicates e27fae258be099a4930fe40854d30384646e33f2 Can I set the above configuration? (type &apos;yes&apos; to accept):从上述输出内容看出，集群自动分配了主从信息，而不是根据命令中输入的顺序分配的，但是如果是不同的主机ip，redis会尽量做到主从在不同的主机上，虽然上面信息写着“127.0.0.1:6382 to 127.0.0.1:6379···”但是关键还是看下面的“M”和“S”的对应关系，不要被上面几行迷惑； 同时还分配了各自的虚拟槽分区的槽值，如果接受这个分配的配置，输入yes，开始执行节点握手和槽分配操作，输出如下： ······ Can I set the above configuration? (type &apos;yes&apos; to accept): yes &gt;&gt;&gt; Nodes configuration updated &gt;&gt;&gt; Assign a different config epoch to each node &gt;&gt;&gt; Sending CLUSTER MEET messages to join the cluster Waiting for the cluster to join .. &gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:6379) M: 6d9658051472fd42328e0c2c0fa25d5f77051343 127.0.0.1:6379 slots:[0-5460] (5461 slots) master 1 additional replica(s) S: ef69743523f99bfeb39272eeef98f7b29e1e2217 127.0.0.1:6384 slots: (0 slots) slave replicates e27fae258be099a4930fe40854d30384646e33f2 M: 8c60e2b86699769ae3d7473712ba1eb5034894c6 127.0.0.1:6381 slots:[10923-16383] (5461 slots) master 1 additional replica(s) M: e27fae258be099a4930fe40854d30384646e33f2 127.0.0.1:6380 slots:[5461-10922] (5462 slots) master 1 additional replica(s) S: cfaf4e715eede56ec39a939f826ec394c92678c3 127.0.0.1:6383 slots: (0 slots) slave replicates 6d9658051472fd42328e0c2c0fa25d5f77051343 S: bff23ec0350b4e5d0be694f1efa9a9385786a443 127.0.0.1:6382 slots: (0 slots) slave replicates 8c60e2b86699769ae3d7473712ba1eb5034894c6 [OK] All nodes agree about slots configuration. &gt;&gt;&gt; Check for open slots... &gt;&gt;&gt; Check slots coverage... [OK] All 16384 slots covered.最后输出报告说明：16384个槽全部被分配，集群创建成功。这里需要注意给集群的节点地址必须是不包含任何槽/数据的节点，否则会拒绝创建集群。 检查集群完整性命令redis-cli –cluster check IP:port，Redis5.0之前的版本可以用 redis-trib.rb check IP:port命令 [root@vmzq1l0l redis]# redis-cli --cluster check 127.0.0.1:6379 127.0.0.1:6379 (6d965805...) -&gt; 0 keys | 5461 slots | 1 slaves. 127.0.0.1:6381 (8c60e2b8...) -&gt; 0 keys | 5461 slots | 1 slaves. 127.0.0.1:6380 (e27fae25...) -&gt; 0 keys | 5462 slots | 1 slaves. [OK] 0 keys in 3 masters. 0.00 keys per slot on average. &gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:6379) M: 6d9658051472fd42328e0c2c0fa25d5f77051343 127.0.0.1:6379 slots:[0-5460] (5461 slots) master 1 additional replica(s) S: ef69743523f99bfeb39272eeef98f7b29e1e2217 127.0.0.1:6384 slots: (0 slots) slave replicates e27fae258be099a4930fe40854d30384646e33f2 M: 8c60e2b86699769ae3d7473712ba1eb5034894c6 127.0.0.1:6381 slots:[10923-16383] (5461 slots) master 1 additional replica(s) M: e27fae258be099a4930fe40854d30384646e33f2 127.0.0.1:6380 slots:[5461-10922] (5462 slots) master 1 additional replica(s) S: cfaf4e715eede56ec39a939f826ec394c92678c3 127.0.0.1:6383 slots: (0 slots) slave replicates 6d9658051472fd42328e0c2c0fa25d5f77051343 S: bff23ec0350b4e5d0be694f1efa9a9385786a443 127.0.0.1:6382 slots: (0 slots) slave replicates 8c60e2b86699769ae3d7473712ba1eb5034894c6 [OK] All nodes agree about slots configuration. &gt;&gt;&gt; Check for open slots... &gt;&gt;&gt; Check slots coverage... [OK] All 16384 slots covered.重点是最后这四句，告诉我们集群所有的槽都已经分配到节点 Redis集群客户端redis cluster client平时连接命令行客户端用redis-cli即可，如果想连接集群客户端需要加个-c的参数 redis-cli -c -h ip地址 -p 端口连接到127.0.0.1:6379的默认节点客户端,可以看出如果key名算出的槽值（slot）不在当前节点分配的范围内，会重定向到属于的槽值节点，并且切换连接到那个节点 [root@vmzq1l0l redis]# redis-cli -c 127.0.0.1:6379&gt; set Jerry mouse OK 127.0.0.1:6379&gt; set Tom cat -&gt; Redirected to slot [9233] located at 127.0.0.1:6380 OK 127.0.0.1:6380&gt; set Jerry &apos;bad mouse&apos; -&gt; Redirected to slot [1468] located at 127.0.0.1:6379 OK 127.0.0.1:6379&gt; get Tom -&gt; Redirected to slot [9233] located at 127.0.0.1:6380 &quot;cat&quot; 127.0.0.1:6380&gt; get Jerry -&gt; Redirected to slot [1468] located at 127.0.0.1:6379 &quot;bad mouse&quot;查看集群中的节点 cluster nodes命令 127.0.0.1:6379&gt; cluster nodes ef69743523f99bfeb39272eeef98f7b29e1e2217 127.0.0.1:6384@16384 slave e27fae258be099a4930fe40854d30384646e33f2 0 1554358007269 6 connected 8c60e2b86699769ae3d7473712ba1eb5034894c6 127.0.0.1:6381@16381 master - 0 1554358009283 3 connected 10923-16383 e27fae258be099a4930fe40854d30384646e33f2 127.0.0.1:6380@16380 master - 0 1554358007000 2 connected 5461-10922 cfaf4e715eede56ec39a939f826ec394c92678c3 127.0.0.1:6383@16383 slave 6d9658051472fd42328e0c2c0fa25d5f77051343 0 1554358008277 5 connected bff23ec0350b4e5d0be694f1efa9a9385786a443 127.0.0.1:6382@16382 slave 8c60e2b86699769ae3d7473712ba1eb5034894c6 0 1554358008000 4 connected 6d9658051472fd42328e0c2c0fa25d5f77051343 127.0.0.1:6379@16379 myself,master - 0 1554358007000 1 connected 0-5460使用命令redis-cli –cluster help可以查看所有关于集群的命令，诸如添加节点删除节点,不再逐个实验 [root@vmzq1l0l redis]# redis-cli --cluster help Cluster Manager Commands: create host1:port1 ... hostN:portN --cluster-replicas &lt;arg&gt; check host:port --cluster-search-multiple-owners info host:port fix host:port --cluster-search-multiple-owners reshard host:port --cluster-from &lt;arg&gt; --cluster-to &lt;arg&gt; --cluster-slots &lt;arg&gt; --cluster-yes --cluster-timeout &lt;arg&gt; --cluster-pipeline &lt;arg&gt; --cluster-replace rebalance host:port --cluster-weight &lt;node1=w1...nodeN=wN&gt; --cluster-use-empty-masters --cluster-timeout &lt;arg&gt; --cluster-simulate --cluster-pipeline &lt;arg&gt; --cluster-threshold &lt;arg&gt; --cluster-replace add-node new_host:new_port existing_host:existing_port --cluster-slave --cluster-master-id &lt;arg&gt; del-node host:port node_id call host:port command arg arg .. arg set-timeout host:port milliseconds import host:port --cluster-from &lt;arg&gt; --cluster-copy --cluster-replace help For check, fix, reshard, del-node, set-timeout you can specify the host and port of any working node in the cluster.后续做简单说明，如果主服务挂点了，从服务会顶上来变成主服务，这时候这个主服务再次启动会变成从服务；比如6379节点是个主服务，6382是6379的从服务，这时候由于种种原因，6379挂掉了，6382会被提升为主服务，如果6379再次重启，6379会变成6382的从服务。]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据分布理论—常见的哈希分区规则]]></title>
    <url>%2F2019%2F04%2F02%2F%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E7%90%86%E8%AE%BA%E2%80%94%E5%B8%B8%E8%A7%81%E7%9A%84%E5%93%88%E5%B8%8C%E5%88%86%E5%8C%BA%E8%A7%84%E5%88%99%2F</url>
    <content type="text"><![CDATA[分布式数据库首先要解决把整个数据集按照分区规则映射到多个节点的问题，即把数据集划分到多个节点上，每个节点负责整体数据的一个子集,如下图： 数据分区规则,常见的分区规则有哈希分区和顺序分区两种： 分区方式 特点 代表产品 哈希分区 ①离散性好②数据分布业务无关③无法顺序访问 Redis clusterCassandraDynamo 顺序分区 ①离散度易倾斜②数据分布业务相关③可顺序访问 BigtableHbaseHypertable 常见的哈希分区规则有以下几种： 节点取余分区使用特定的数据，如Redis的键，再根据节点数量N使用公式：hash（key）%N计算出哈希值，用来决定数据映射到哪一个节点上。 这种方案存在一个问题：当节点数量变化时，如扩容或收缩节点，数据节点映射关系需要重新计算，会导致数据的重新迁移。 这种方案的优点是简单性，常用于数据库的分库分表规则，一般采用预分区的方式，提前根据数据量规划好分区数，比如划分为512或1024张表，保证可支撑未来一段时间的数据量，再根据负载情况将表迁移到其他数据库中。扩容时通常采用翻倍扩容，避免数据映射全部被打乱导致全量迁移的情况，如下图所示： 一致性哈希分区一致性哈希分区（Distributed Hash Table）实现思路是为系统中每个节点分配一个token，范围一般在0~2^32,这些token构成一个哈希环。数据读写执行节点查找操作时，先根据key计算hash值，然后顺时针找到第一个大于等于该哈希值的token节点，如下图所示： 这种方式相比节点取余最大的好处在于加入和删除节点只影响哈希环中相邻的节点，对其他节点无影响。但一致性哈希分区存在几个问题： - 加减节点会造成哈希环中部分数据无法命中，需要手动处理或者忽略这部分数据，因此一致性哈希常用于缓存场景。 - 当使用少量节点时，节点变化将大范围影响哈希环中数据映射，因此这种方式不适合少量数据节点的分布式方案。 - 普通的一致性哈希分区在增减节点时需要增加一倍或减去一半节点才能保证数据和负载的均衡 正因为一致性哈希分区的这些缺点，一些分布式系统采用虚拟槽对一致性哈希进行改进 虚拟槽分区虚拟槽分区巧妙地使用了哈希空间，使用分散度良好的哈希函数把所有数据映射到一个固定范围的整数集合中，整数定义为槽（slot）。这个范围一般远远大于节点数，比如Redis Cluster槽范围是0~16383。槽是集群内数据管理和迁移的基本单位。采用大范围槽的主要目的是为了方便数据拆分和集群扩展。每个节点会负责一定数量的槽，如下图所示： 当前集群有5个节点，每个节点平均大约负责3276个槽。由于采用高质量的哈希算法，每个槽所映射的数据通常比较均匀，将数据平均划分到5个节点进行数据分区]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[记一次内存溢出的调试经历，线程无限创建，直到报OOM]]></title>
    <url>%2F2019%2F04%2F02%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%E7%9A%84%E8%B0%83%E8%AF%95%E7%BB%8F%E5%8E%86%EF%BC%8C%E7%BA%BF%E7%A8%8B%E6%97%A0%E9%99%90%E5%88%9B%E5%BB%BA%EF%BC%8C%E7%9B%B4%E5%88%B0%E6%8A%A5OOM%2F</url>
    <content type="text"><![CDATA[问题一个java服务运行差不多一天就会崩溃，报java.lang.OutOfMemoryError: unable to create new native thread 分析可能原因1：服务器设置无法满足服务创建的线程数量 可能原因2：jvm配置无法满足创建的线程数量 可能原因3：代码问题，有死循环，线程在无限创建 解决1.查看服务器linux用户的最大进程数，挺大的跟这个应该没关系 [root@localhost]# ulimit -u 1269852.调节jvm启动时设置的每个线程的堆栈大小参数 Xss，原来是默认值1m，可以改成Xss256k，先看下一步； 3.其实分析下来大概率代码是有问题了，应该直接从这一步切入的，前面两步算是一种拓展学习了 先用“ps -ef|grep java”查看服务的PID（本文出问题的服务PID是134859，后文也有用到） 然后查看服务的进程数命令“ps hH p PID | wc -l”,显示304个进程，看似很正常 [root@localhost]# ps hH p 134859 | wc -l 304过一会再看，没多长时间就多了两百多 [root@localhost]# ps hH p 134859 | wc -l 568很明显这不正常，有什么代码在循环创建线程 jstack命令可以用来查看Java线程的调用堆栈的，可以用来分析线程问题，打印服务的线程日志到dump.txt文件 [root@localhost]# jstack -l 134859 &gt;&gt; dump.txt 补充说明：可以使用top -Hp PID 命令查看PID进程中的线程占用信息，然后根据查看到的占用高的线程PID换算成16进制，再从jstack导出的线程堆栈信息中查找这个16进制PID的报错信息 然后查看这个文件，发现大量类似于下面的线程TIMED_WAITING的记录 &quot;threadPoolTaskExecutor-11&quot; #54 prio=5 os_prio=0 tid=0x00007fc760036800 nid=0xbec3 waiting on condition [0x00007fc81fffe000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at cn.doeat.blog.test.thr.PostStatusThr.run(PostStatusThr.java:97) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Locked ownable synchronizers: - &lt;0x00000005d20dc8e0&gt; (a java.util.concurrent.ThreadPoolExecutor$Worker) &quot;threadPoolTaskExecutor-10&quot; #53 prio=5 os_prio=0 tid=0x00007fc760034800 nid=0xbec2 waiting on condition [0x00007fc824191000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at cn.doeat.blog.test.thr.PostStatusThr.run(PostStatusThr.java:97) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Locked ownable synchronizers: - &lt;0x00000005d20dbf68&gt; (a java.util.concurrent.ThreadPoolExecutor$Worker) ······ ······ ······日志中有明确的出问题代码的位置，PostStatusThr.java文件第97行 at cn.doeat.blog.test.thr.PostStatusThr.run(PostStatusThr.java:97)去看代码,是个线程的run方法，97行只是个break，看代码逻辑没有什么错误，就不贴出代码了 public class PostStatusThr implements Runnable { private boolean endFlag = true; private long sleepTime; private int maxMessageCount = 10; public PostStatusThr(boolean endFlag, long sleepTime,int maxMessageCount) { this.endFlag = endFlag; this.sleepTime = sleepTime; this.maxMessageCount = maxMessageCount; } @Override public void run() { ··· ··· ··· } }PostStatusThr中有个有参的构造方法，那就看谁调用了这个类，发现只有PostStatusTask类调用了PostStatusThr的有参构造方法，此处也没有什么问题，只是运行创建线程池 @Component public class PostStatusTask { @Autowired private ThreadPoolTaskExecutor executor; public void run() { try { int i = 0; while (i &lt; 2) { executor.execute(new PostSensorStatusThr(true, 5000,10)); i++; } } catch (Exception exc) { exc.printStackTrace(); } } }那再看看哪里调用了PostStatusTask类，寻根溯源找到了一处代码,这里相当于每隔3秒初始化一次线程池····，代码原本意图是项目启动时初始化调用创建线程池 @Component public class InitializationTask { @Scheduled(fixedDelay =3000) public static synchronized void pollingLogs() { ServiceUtil.getPostStatusTask().run(); } }修改代码使符合预期效果，springboot项目实现ApplicationRunner接口会在项目启动时运行实现类中的run方法 @Component public class InitializationTask implements ApplicationRunner { public static synchronized void pollingLogs() { ServiceUtil.getPostStatusTask().run(); } @Override public void run(ApplicationArguments applicationArguments) throws Exception { pollingLogs(); } }]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—哨兵（Redis Sentinel）]]></title>
    <url>%2F2019%2F03%2F28%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E5%93%A8%E5%85%B5%EF%BC%88Redis%20Sentinel%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Redis的主从复制模式下，一旦主节点由于故障不能提供服务，需要人工将从节点晋升为主节点，同时还要通知应用方更新主节点地址，对于很多应用场景这种故障处理的方式是无法接受的。 Redis从2.8开始正式提供了Redis Sentinel（哨兵）架构来解决这个问题，Redis Sentinel是Redis的高可用实现方案，在实际的生产环境中，对提高整个系统的高可用性是非常有帮助的。 主从复制的问题主从解决的问题 - 如果主节点出故障了，从节点可以作为后备力量顶上来，保证了数据的一致性 - 从节点拓展了主节点的读能力，主节点如果支撑不住，从节点会帮助主节点分担压力 主从可能出现的问题 - 一旦主节点出现故障，需要手动将从节点晋升为主节点，然后将其他从节点的主节点配置改为这个，需要人工干预 - 主节点所只能在一台主机，限制了主节点的写能力 - 主节点只能在一台主机，存储能力受到单机的限制 Redis Sentinel的高可用性Redis Sentinel是一个分布式架构，其中包含若干个Sentinel节点和Redis数据节点，每个Sentinel节点会对数据节点和其余Sentinel节点进行监控，当它发现节点不可达时，会对节点做下线标识。如果被标识的是主节点，它还会和其他Sentinel节点进行“协商”，当大多数Sentinel节点都认为主节点不可达时，它们会选举出一个Sentinel节点来完成自动故障转移的工作，同时会将这个变化实时通知给Redis应用方。整个过程完全是自动的，不需要人工来介入，所以这套方案很有效地解决了Redis的高可用问题 主从复制模式 Redis-sentinel架构 安装和部署先画个拓扑图，接下来就按照这个拓扑图为例完成部署 简要说明，会开启6379、6666和7777三个端口的Redis实例，其中6379默认配置的为主节点，6666和7777为从节点，配置三个Sentinel节点端口分别是26379、26666和27777,下面进行详细说明： 启动主节点12345678910111213[root@vmzq1l0l ~]# redis-cli127.0.0.1:6379&gt; info replication# Replicationrole:masterconnected_slaves:0master_replid:14cffeedb4d9635f130d446c9013dfe439ab06c1master_replid2:0000000000000000000000000000000000000000master_repl_offset:1279second_repl_offset:-1repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:1279 启动两个从节点用配置文件的方式启动，复制不同配置文件改相关配置（下面以6666端口实例举例，7777操作完全相同，只是配置从6666改为7777） 1cp redis.conf redis-6666.conf 需要修改redis-6666.conf文件的以下配置(由于种种原因在5.0以后比较新的版本中，原本配置中slaveof改成了replicaof，会依然保留slaveof命令，本篇使用的是5.0.3版本) 123456789#端口号port 6666#保护进程启动（后台启动）daemonize yes#日志文件logfile "6666.log"#rdb备份文件名dbfilename "dump-6666.rdb"replicaof 127.0.0.1 6379 启动两个实例（7777实例的配置文件按照上述套路再次执行一遍改成7777的配置） 12redis-server redis-6666.confredis-server redis-7777.conf 如果上述步骤没问题的话，现在看看主节点的主从信息 123456789[root@vmzq1l0l redis]# redis-cli info replication# Replicationrole:masterconnected_slaves:2slave0:ip=127.0.0.1,port=7777,state=online,offset=1699,lag=1slave1:ip=127.0.0.1,port=6666,state=online,offset=1699,lag=1master_replid:471c9174da388dee8f1ac9cd23e00e0778f7658amaster_replid2:0000000000000000000000000000000000000000··· 6666端口实例从节点的主从信息 123456789101112131415[root@vmzq1l0l redis]# redis-cli -h 127.0.0.1 -p 6666 info replication# Replicationrole:slavemaster_host:127.0.0.1master_port:6379master_link_status:upmaster_last_io_seconds_ago:7master_sync_in_progress:0slave_repl_offset:1867slave_priority:100slave_read_only:1connected_slaves:0master_replid:471c9174da388dee8f1ac9cd23e00e0778f7658amaster_replid2:0000000000000000000000000000000000000000··· 至此只是完成了一主两从的主从配置 部署Redis Sentinel节点配置Sentinel节点redis安装目录下有个sentinel.conf这个配置文件，默认是26379端口，修改以下配置参数： 12345678port 26379daemonize yeslogfile "26379.log"dir /usr/local/redissentinel monitor mymaster 127.0.0.1 6379 2sentinel down-after-milliseconds mymaster 60000sentinel failover-timeout mymaster 180000sentinel parallel-syncs mymaster 1 sentinel monitor mymaster 127.0.0.1 6379 2配置代表sentinel节点需要监控127.0.0.1:6379这个主节点，2代表判断主节点失败至少需要2个Sentinel节点同意,mymaster是主节点的别名 down-after-milliseconds 选项指定了 Sentinel 认为服务器已经断线所需的毫秒数 parallel-syncs 选项指定了在执行故障转移时， 最多可以有多少个从服务器同时对新的主服务器进行同步， 这个数字越小， 完成故障转移所需的时间就越长 启动Sentinel节点方法一，使用redis-sentinel命令： 1redis-sentinel sentinel.conf 方法二，使用redis-server命令加–sentinel参数： 1redis-server sentinel.conf --sentinel 两种方法本质上是一样的。]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—主从复制]]></title>
    <url>%2F2019%2F03%2F27%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[在分布式系统中为了解决单点问题，通常会把数据复制多个副本部署到其他机器，满足故障恢复和负载均衡等需求。Redis也是如此，它为我们提供了复制功能，实现了相同数据的多个Redis副本。复制功能是高可用Redis的基础 建立复制 参与复制的Redis实例分为主节点（master）和从节点（slave）。每个从节点只能有一个主节点，主节点可以有多个从节点，复制的数据流是单向的，只能由主节点复制到从节点，有以下三种方式： 在配置文件中加入slaveof{masterHost}{masterPort}随Redis启动生效 在redis-server启动命令后加入–slaveof{masterHost}{masterPort}生效 直接使用命令：slaveof{masterHost}{masterPort}生效 在本地开启两个不同端口的服务，分别是默认的6379端口和自己启动的6666端口，在6666端口的服务中开启复制： 123127.0.0.1:6666&gt; slaveof 127.0.0.1 6379....Finished with success 此时先看一个6666端口实例key为myname的值是没有的 12127.0.0.1:6666&gt; get myname(nil) 在主节点6379实例进行插入操作 12127.0.0.1:6379&gt; set myname CharlieOK 这时候再看6666端口实例中，值已经复制过来了 12127.0.0.1:6666&gt; get myname"Charlie" 通过info查看主从信息 主节点6379端口 12345678910111213127.0.0.1:6379&gt; info replication# Replicationrole:masterconnected_slaves:1slave0:ip=127.0.0.1,port=6666,state=online,offset=957,lag=0master_replid:3e32aa6882ae57742f8d12bd9eb3c530c3ff5a74master_replid2:0000000000000000000000000000000000000000master_repl_offset:957second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:957 从节点6666端口 12345678910111213141516171819# Replicationrole:slavemaster_host:127.0.0.1master_port:6379master_link_status:upmaster_last_io_seconds_ago:6master_sync_in_progress:0slave_repl_offset:915slave_priority:100slave_read_only:1connected_slaves:0master_replid:3e32aa6882ae57742f8d12bd9eb3c530c3ff5a74master_replid2:0000000000000000000000000000000000000000master_repl_offset:915second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:113repl_backlog_histlen:803 断开复制 1slaveof no one 断开6666的从节点，这时 123127.0.0.1:6666&gt; slaveof no one···OK 这时候info replication命令查看会发现状态由从节点变成主节点了 123456789101112127.0.0.1:6666&gt; info replication# Replicationrole:masterconnected_slaves:0master_replid:f15dd49506a152bd5fbbeaab728314e7cc62fc15master_replid2:3e32aa6882ae57742f8d12bd9eb3c530c3ff5a74master_repl_offset:1279second_repl_offset:1280repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:113repl_backlog_histlen:1167 安全性 对于数据比较重要的节点，主节点会通过设置requirepass参数进行密码验证，这时所有的客户端访问必须使用auth命令实行校验。从节点与主节点的复制连接是通过一个特殊标识的客户端来完成，因此需要配置从节点的masterauth参数与主节点密码保持一致，这样从节点才可以正确地连接到主节点并发起复制流程 只读 replica-serve-stale-data yes默认从节点为只读模式，保证数据一致性 1234567891011121314# When a replica loses its connection with the master, or when the replication# is still in progress, the replica can act in two different ways:## 1) if replica-serve-stale-data is set to 'yes' (the default) the replica will# still reply to client requests, possibly with out of date data, or the# data set may just be empty if this is the first synchronization.## 2) if replica-serve-stale-data is set to 'no' the replica will reply with# an error "SYNC with master in progress" to all the kind of commands# but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG,# SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB,# COMMAND, POST, HOST: and LATENCY.#replica-serve-stale-data yes 传输延迟 主从节点往往不在同一个服务器上，需要通过网络进行复制，这时候网络延迟会成为一个不稳定因素，Redis为我们提供了repl-disable-tcp-nodelay参数用于控制是否关闭TCP_NODELAY，默认关闭 1234567891011121314# Disable TCP_NODELAY on the replica socket after SYNC?## If you select "yes" Redis will use a smaller number of TCP packets and# less bandwidth to send data to replicas. But this can add a delay for# the data to appear on the replica side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select "no" the delay for data to appear on the replica side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and replicas are many hops away, turning this to "yes" may# be a good idea.repl-disable-tcp-nodelay no 当关闭时，主节点产生的命令数据无论大小都会及时地发送给从节点，这样主从之间延迟会变小，但增加了网络带宽的消耗。适用于主从之间的网络环境良好的场景，如同机架或同机房部署 当开启时，主节点会合并较小的TCP数据包从而节省带宽。默认发送时间间隔取决于Linux的内核，一般默认为40毫秒。这种配置节省了带宽但增大主从之间的延迟。适用于主从网络环境复杂或带宽紧张的场景，如跨机房部署]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—持久化机制之AOF]]></title>
    <url>%2F2019%2F03%2F27%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6%E4%B9%8BAOF%2F</url>
    <content type="text"><![CDATA[AOF(append only file)持久化：以独立日志的方式记录每次写命令，重启时再重新执行AOF文件中的命令达到恢复数据的目的。AOF的主要作用是解决了数据持久化的实时性，目前已经是Redis持久化的主流方式。 开启AOF功能需要设置配置：appendonly yes，AOF文件名通过appendfilename配置设置，默认文件名是appendonly.aof。保存路径同RDB持久化方式一致，通过dir配置指定。 appendonly默认是no，需要改成yes 12345678910111213141516171819# By default Redis asynchronously dumps the dataset on disk. This mode is# good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on# the configured save points).## The Append Only File is an alternative persistence mode that provides# much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a# dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is# still running correctly.## AOF and RDB persistence can be enabled at the same time without problems.# If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.## Please check http://redis.io/topics/persistence for more information.appendonly no 文件名，默认是appendonly.aof 12# The name of the append only file (default: "appendonly.aof")appendfilename "appendonly.aof" AOF的工作流程操作：命令写入(append)、文件同步(sync)、文件重写(rewrite)、重启加载(load)，如下图 所有的写入命令会追加到aof_buf（缓冲区）中 AOF缓冲区根据对应的策略向硬盘做同步操作 随着AOF文件越来越大，需要定期对AOF文件进行重写，达到压缩的目的 当Redis服务器重启时，可以加载AOF文件进行数据恢复。 命令写入AOF命令写入的内容直接是文本协议格式，即是RESP，Redis序列化协议 例如set hello world这条命令，在AOF缓冲区会追加如下文本 1*3\r\n$3\r\nset\r\n$5\r\nhello\r\n$5\r\nworld\r\n AOF会把命令追加到aof_buf中的好处：Redis使用单线程响应命令，如果每次写AOF文件命令都直接追加到硬盘，那么性能完全取决于当前硬盘负载。先写入缓冲区aof_buf中，还有另一个好处，Redis可以提供多种缓冲区同步硬盘的策略，在性能和安全性方面做出平衡 文件同步Redis提供了多种AOF缓冲区同步文件策略，由参数appendfsync控制 1234567891011121314151617181920212223242526# The fsync() call tells the Operating System to actually write data on disk# instead of waiting for more data in the output buffer. Some OS will really flush# data on disk, some other OS will just try to do it ASAP.## Redis supports three different modes:## no: don't fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.## The default is "everysec", as that's usually the right compromise between# speed and data safety. It's up to you to understand if you can relax this to# "no" that will let the operating system flush the output buffer when# it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that's snapshotting),# or on the contrary, use "always" that's very slow but a bit safer than# everysec.## More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use "everysec".# appendfsync alwaysappendfsync everysec# appendfsync no 系统调用write和fsync说明 write操作会触发延迟写（delayed write）机制。Linux在内核提供页缓冲区用来提高硬盘IO性能。write操作在写入系统缓冲区后直接返回。同步硬盘操作依赖于系统调度机制，例如：缓冲区页空间写满或达到特定时间周期。同步文件之前，如果此时系统故障宕机，缓冲区内数据将丢失 fsync针对单个文件操作（比如AOF文件），做强制硬盘同步，fsync将阻塞直到写入硬盘完成后返回，保证了数据持久化 配置为always时，每次写入都要同步AOF文件，在一般的SATA硬盘上，Redis只能支持大约几百TPS写入，显然跟Redis高性能特性背道而驰，不建议配置 配置为no，由于操作系统每次同步AOF文件的周期不可控，而且会加大每次同步硬盘的数据量，虽然提升了性能，但数据安全性无法保证 配置为everysec，是建议的同步策略，也是默认配置，做到兼顾性能和数据安全性。理论上只有在系统突然宕机的情况下丢失1秒的数据 重写机制 随着命令不断写入AOF，文件会越来越大，为了解决这个问题，Redis引入AOF重写机制压缩文件体积。AOF文件重写是把Redis进程内的数据转化为写命令同步到新AOF文件的过程 重写后的AOF文件会变小的原因： 进程内已经超时的数据不再写入文件 旧的AOF文件含有无效命令，如del key1、hdel key2、srem keys、set a 111、set a 222等。重写使用进程内数据直接生成，这样新的AOF文件只保留最终数据的写入命令 多条写命令可以合并为一个，如：lpush list a、lpush list b、lpush list c可以转化为：lpush list a b c。为了防止单条命令过大造成客户端缓冲区溢出，对于list、set、hash、zset等类型操作，以64个元素为界拆分为多条 AOF重写降低了文件占用空间，除此之外，另一个目的是：更小的AOF文件可以更快地被Redis加载 AOF重写过程可以手动触发和自动触发 手动触发：直接调用bgrewriteaof命令 自动触发：根据auto-aof-rewrite-min-size和auto-aof-rewrite-percentage参数确定自动触发时机 12345678910111213141516171819# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size grows by the specified percentage.## This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a percentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb auto-aof-rewrite-min-size：表示运行AOF重写时文件最小体积，默认为64MB auto-aof-rewrite-percentage：代表当前AOF文件空间(aof_current_size)和上一次重写后AOF文件空间(aof_base_size)的比值 1自动触发时机=aof_current_size&gt;auto-aof-rewrite-min-size&amp;&amp;（aof_current_size-aof_base_size）/aof_base_size&gt;=auto-aof-rewrite-percentage 其中aof_current_size和aof_base_size可以在info Persistence统计信息中查看 AOF重写运作流程如下图 1.执行AOF重写请求 2.父进程执行fork创建子进程，开销等同于bgsave过程 3.1 主进程fork操作完成后，继续响应其他命令。所有修改命令依然写入AOF缓冲区并根据appendfsync策略同步到硬盘，保证原有AOF机制正确性 3.2 由于fork操作运用写时复制技术，子进程只能共享fork操作时的内存数据。由于父进程依然响应命令，Redis使用“AOF重写缓冲区”保存这部分新数据，防止新AOF文件生成期间丢失这部分数据 4 子进程根据内存快照，按照命令合并规则写入到新的AOF文件。每次批量写入硬盘数据量由配置aof-rewrite-incremental-fsync控制，默认为32MB，防止单次刷盘数据过多造成硬盘阻塞 5.1 新AOF文件写入完成后，子进程发送信号给父进程，父进程更新统计信息，具体见info persistence下的aof_*相关统计 5.2 父进程把AOF重写缓冲区的数据写入到新的AOF文件 5.3 使用新AOF文件替换老文件，完成AOF重写 重启加载AOF和RDB文件都可以用于服务器重启时的数据恢复，下图表示Redis持久化文件加载流程 文件校验加载损坏的AOF文件时会拒绝启动，并打印如下日志： 1# Bad file format reading the append only file: make a backup of your AOF file,then use ./redis-check-aof --fix 对于错误格式的AOF文件，先进行备份，然后采用redis-check-aof–fix命令进行修复，修复后使用diff-u对比数据的差异，找出丢失的数据，有些可以人工修改补全。 AOF文件可能存在结尾不完整的情况，比如机器突然掉电导致AOF尾部文件命令写入不全。Redis为我们提供了aof-load-truncated配置来兼容这种情况，默认开启。加载AOF时，当遇到此问题时会忽略并继续启动，同时打印如下警告日志： 123# !!! Warning: short read while loading the AOF file !!!# !!! Truncating the AOF at offset 397856725 !!!# AOF loaded anyway because aof-load-truncated is enabled 此为配置文件中aof-load-truncated的默认配置和官方的详细说明文档 1234567891011121314151617181920212223# An AOF file may be found to be truncated at the end during the Redis# startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running# crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can't happen when Redis itself# crashes or aborts but the operating system still works correctly).## Redis can either exit with an error when this happens, or load as much# data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.## If aof-load-truncated is set to yes, a truncated AOF file is loaded and# the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error# and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the "redis-check-aof" utility before to restart# the server.## Note that if the AOF file will be found to be corrupted in the middle# the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes# will be found.aof-load-truncated yes AOF追加阻塞当开启AOF持久化时，常用的同步硬盘的策略是everysec，用于平衡性能和数据安全性。对于这种方式，Redis使用另一条线程每秒执行fsync同步硬盘。当系统硬盘资源繁忙时，会造成Redis主线程阻塞。如下图所示： 1.主线程负责写入AOF缓冲区 2.AOF线程负责每秒执行一次同步磁盘操作，并记录最近一次同步时间 3.主线程负责对比上次AOF同步时间： 如果距上次同步成功时间在2秒内，主线程直接返回 如果距上次同步成功时间超过2秒，主线程将会阻塞，直到同步操作完成 AOF阻塞问题定位: 1.发生AOF阻塞时，Redis输出如下日志，用于记录AOF fsync阻塞导致拖慢Redis服务的行为： 12Asynchronous AOF fsync is taking too long (disk is busy). Writing the AOF bufferwithout waiting for fsync to complete, this may slow down Redis 2.每当发生AOF追加阻塞事件发生时，在info Persistence统计中，aof_delayed_fsync指标会累加，查看这个指标方便定位AOF阻塞问题 3.AOF同步最多允许2秒的延迟，当延迟发生时说明硬盘存在高负载问题，可以通过监控工具如iotop，定位消耗硬盘IO资源的进程]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—持久化机制之RDB]]></title>
    <url>%2F2019%2F03%2F26%2Fredis-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-e6-8c-81-e4-b9-85-e5-8c-96-e6-9c-ba-e5-88-b6-e4-b9-8brdb%2F</url>
    <content type="text"><![CDATA[Redis支持RDB和AOF两种持久化机制，持久化功能有效地避免因进程退出造成的数据丢失问题，当下次重启时利用之前持久化的文件即可实现数据恢复；本篇介绍RDB。 RDB （Redis DataBase）持久化是把当前进程数据生成快照保存到硬盘的过程，触发RDB持久化过程分为手动触发和自动触发 手动触发 两个用于持久化的手动触发命令：save和bgsave save命令：阻塞当前Redis服务器，直到RDB过程完成为止，对于内存比较大的实例会造成长时间阻塞，线上环境不建议使用。运行save命令对应的Redis日志如下： 127.0.0.1:6379&gt; saveOK bgsave命令：显然save命令在庞大数据量的情况下是不允许使用的，所以Redis提供了bgsave 127.0.0.1:6379&gt; bgsaveBackground saving started 自动触发 因为save命令会阻塞Redis服务，所以所有的自动触发操作都是使用的bgsave方式， 自动触发的场景： 根据配置文件中save配置，比如“save m n”。表示m秒内数据集存在n次修改时，自动触发bgsave 如果从节点执行全量复制操作，主节点自动执行bgsave生成RDB文件并发送给从节点 执行debug reload命令重新加载Redis时，会自动触发save操作 默认情况下执行shutdown命令时，如果没有开启AOF持久化功能则自动执行bgsave bgsave命令的运作流程 恢复数据 将备份文件 (dump.rdb) 移动到 redis 安装目录并启动服务即可，redis就会自动加载文件数据至内存了。Redis 服务器在载入 RDB 文件期间，会一直处于阻塞状态，直到载入工作完成为止。 获取 redis 的安装目录可以使用 config get dir 命令 127.0.0.1:6379&gt; config get dir1) “dir”2) “/usr/local/redis-5.0.3” 停止 RDB 持久化 1.可以直接删除配置文件中的所有save配置 2.通过命令 redis-cli config set save RDB的优缺点 优点： 可以做定时执行备份用于灾难恢复，而且恢复数据远远快于AOF方式 缺点：RDB备份的数据没办法做到实时持久化/秒级持久化，因为bgsave每次运行都要执行fork操作创建子进程，属于重量级操作，频繁执行成本过高 RDB相关参数配置：（redis.conf配置文件中） 1.save 比如“save m n”。表示m秒内数据集存在n次修改时，自动触发bgsave，可以自行添加；下面为redis.conf默认配置和详细的注释 ################################ SNAPSHOTTING ################################## Save the DB on disk:## save ## Will save the DB if both the given number of seconds and the given# number of write operations against the DB occurred.## In the example below the behaviour will be to save:# after 900 sec (15 min) if at least 1 key changed# after 300 sec (5 min) if at least 10 keys changed# after 60 sec if at least 10000 keys changed## Note: you can disable saving completely by commenting out all “save” lines.## It is also possible to remove all the previously configured save# points by adding a save directive with a single empty string argument# like in the following example:## save “” save 900 1save 300 10save 60 10000 save 900 1 表示900秒内发生1次更改会执行持久化save 300 10 表示300秒内发生10次更改会执行持久化save 60 10000 表示60秒内发生10000次更改会执行持久化 2.stop-writes-on-bgsave-error 默认是yes，启用了RDB且最后一次后台保存数据失败，Redis是否停止接收数据。这会让用户意识到数据没有正确持久化到磁盘上，否则没有人会注意到灾难（disaster）发生了。如果Redis重启了，那么又可以重新开始接收数据了 # By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.stop-writes-on-bgsave-error yes 3.rdbcompression 默认值是yes，对于存储到磁盘中的快照，可以设置是否进行压缩存储。如果是的话，redis会采用LZF算法进行压缩。如果你不想消耗CPU来进行压缩的话，可以设置为关闭此功能，但是存储在磁盘上的快照会比较大 # Compress string objects using LZF when dump .rdb databases?# For default that’s set to ‘yes’ as it’s almost always a win.# If you want to save some CPU in the saving child set it to ‘no’ but# the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes 4.rdbchecksum 默认值是yes，在存储快照后，我们还可以让redis使用CRC64算法来进行数据校验，但是这样做会增加大约10%的性能消耗，如果希望获取到最大的性能提升，可以关闭此功能 # Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.rdbchecksum yes 5.dbfilename 设置快照的文件名，默认是 dump.rdb # The filename where to dump the DBdbfilename “dump.rdb” 6.dir 设置快照文件的存放路径，这个配置项一定是个目录，而不能是文件名。默认是和当前配置文件保存在同一目录。也就是说通过在配置文件中配置的 save 方式，当实际操作满足该配置形式时就会进行 RDB 持久化，将当前的内存快照保存在 dir 配置的目录中，文件名由配置的 dbfilename 决定。 # The working directory.## The DB will be written inside this directory, with the filename specified# above using the ‘dbfilename’ configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir “/usr/local/redis-5.0.3”]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—Java客户端]]></title>
    <url>%2F2019%2F03%2F26%2Fredis-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-java-e5-ae-a2-e6-88-b7-e7-ab-af%2F</url>
    <content type="text"><![CDATA[现在spring-boot项目直接使用spring-data-redis和配置文件的方式非常的方便快捷，不算复杂，jedis作为之前java使用率非常高的redis java客户端，在此稍做介绍： Jedis可以通过new一个Jedis对象直接连接，不建议使用，可用于本地测试 Jedis直连代码举例： //不建议使用Jedis jedis = new Jedis(“127.0.0.1”,6379);jedis.set(key,value)jedis.close() 一般使用连接池的方式对Jedis连接进行管理，所有Jedis对象预先放在池子中（JedisPool），每次要连接Redis，只需要在池子中借，用完了再归还给池子，流程如下图，具体可以参考下面给出的文档 Jedis工具类参考这篇文章 官方文档 http://tool.oschina.net/uploads/apidocs/ 网友智慧结晶（现成的工具类） https://blog.csdn.net/liuxiao723846/article/details/50401406 spring-data-redis整合参考下面 官方文档 https://spring.io/projects/spring-data-redishttps://www.cnblogs.com/phil_jing/p/7468586.html 网友智慧结晶 https://www.cnblogs.com/phil_jing/p/7468586.html]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—客户端通讯协议RESP]]></title>
    <url>%2F2019%2F03%2F26%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%80%9A%E8%AE%AF%E5%8D%8F%E8%AE%AERESP%2F</url>
    <content type="text"><![CDATA[Redis制定了RESP（REdis Serialization Protocol，Redis序列化协议）实现客户端与服务端的正常交互，这种协议简单高效，既能够被机器解析，又容易被人类识别 发送命令格式RESP的规定一条命令的格式如下，CRLF代表”\r\n” *&lt; 参数数量 &gt; CRLF $&lt; 参数 1 的字节数量 &gt; CRLF &lt; 参数 1&gt; CRLF ... $&lt; 参数 N 的字节数量 &gt; CRLF &lt; 参数 N&gt; CRLF以set hell world这条命令进行说明，参数数量为3个，因此第一行为： *3参数字节数分别是355，因此后面几行为： $3 SET $5 hello $5 world这样传输的格式就是： *3\r\n$3\r\nSET\r\n$5\r\nhello\r\n$5\r\nworld\r\n返回结果格式Redis的返回结果类型分为以下五种: - 状态回复：在RESP中第一个字节为”+” - 错误回复：在RESP中第一个字节为”-“ - 整数回复：在RESP中第一个字节为”：” - 字符串回复：在RESP中第一个字节为”$” - 多条字符串回复：在RESP中第一个字节为”*”]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—地理信息定位（GEO）]]></title>
    <url>%2F2019%2F03%2F25%2Fredis-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-geo%2F</url>
    <content type="text"><![CDATA[Redis3.2版本提供了GEO（地理信息定位）功能，支持存储地理位置信息用来实现诸如附近位置、摇一摇这类依赖于地理位置信息的功能，对于需要实现这些功能的开发者来说是一大福音 增加地理位置信息 geoadd key longitude latitude member [longitude latitude member …] longitude、latitude、member分别是该地理位置的经度、纬度、成员 添加北京的位置信息 127.0.0.1:6379&gt; geoadd cities 116.28 39.55 beijing(integer) 1 如果添加成功返回1，已存在会返回0，如果是做修改操作同样使用geoadd，返回0 同时添加三个城市位置信息天津、上海和重庆 127.0.0.1:6379&gt; geoadd cities 117.12 39.08 tianjin 121.47 31.23 shanghai 106.55 29.57 chongqing(integer) 3 获取地理位置信息 geopos key member [member …] 获取上海位置信息，同时获取上海和重庆的位置信息 127.0.0.1:6379&gt; geopos cities shanghai1) 1) “121.47000163793563843” 2) “31.22999903975783553”127.0.0.1:6379&gt; geopos cities shanghai chongqing1) 1) “121.47000163793563843” 2) “31.22999903975783553”2) 1) “106.5499994158744812” 2) “29.5700000136221135” 获取两个地理位置的距离 geodist key member1 member2 [unit] unit代表单位，不填写默认返回单位是 m（米）；可以输入以下四个单位 m（meters）代表米 km（kilometers）代表公里 mi（miles）代表英里 ft（feet）代表尺 获取上海和重庆的距离 127.0.0.1:6379&gt; geodist cities shanghai chongqing“1442064.7968”127.0.0.1:6379&gt; geodist cities shanghai chongqing km“1442.0648” 获取指定位置范围内的地理信息位置集合 georadius key longitude latitude radiusm|km|ft|mi [withcoord] [withdist] [withhash] [COUNT count] [asc|desc] [store key] [storedist key] georadiusbymember key member radiusm|km|ft|mi [withcoord] [withdist] [withhash] [COUNT count] [asc|desc] [store key] [storedist key] georadius和georadiusbymember两个命令的作用是一样的，都是以一个地理位置为中心算出指定半径内的其他地理信息位置，不同的是georadius命令的中心位置给出了具体的经纬度，georadiusbymember只需给出成员即可。其中radiusm|km|ft|mi是必需参数，指定了半径（带单位），这两个命令有很多可选参数，如下所示 withcoord：返回结果中包含经纬度 withdist：返回结果中包含离中心节点位置的距离 withhash：返回结果中包含geohash，有关geohash后面介绍 COUNT count：指定返回结果的数量 asc|desc：返回结果按照离中心节点的距离做升序或者降序 store key：将返回结果的地理位置信息保存到指定键 storedist key：将返回结果离中心节点的距离保存到指定键 获取北京1200km以内的城市 127.0.0.1:6379&gt; georadiusbymember cities beijing 1200 km1) “beijing”2) “tianjin”3) “shanghai” 获取geohash geohash key member [member …] Redis使用geohash将二维经纬度转换为一维字符串，下面操作会返回beijing的geohash值 127.0.0.1:6379&gt; geohash cities beijing1) “wx48ypbe2q0” 删除地理位置信息 GEO没有提供删除成员的命令，但是因为GEO的底层实现是zset，所以可以借用zrem命令实现对地理位置信息的删除 zrem key member 删除北京的位置信息 127.0.0.1:6379&gt; zrem cities beijing(integer) 1]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—发布订阅]]></title>
    <url>%2F2019%2F03%2F25%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85%2F</url>
    <content type="text"><![CDATA[Redis提供了基于“发布/订阅”模式的消息机制，此种模式下，消息发布者和订阅者不进行直接通信，发布者客户端向指定的频道（channel）发布消息，订阅该频道的每个客户端都可以收到该消息 和很多专业的消息队列系统（例如Kafka、RocketMQ）相比，Redis的发布订阅略显粗糙，例如无法实现消息堆积和回溯。但胜在足够简单，如果当前场景可以容忍的这些缺点，也不失为一个不错的选择 Redis主要提供了发布消息、订阅频道、取消订阅以及按照模式订阅和取消订阅等命令； 发布消息 1publish channel message 发布一条”channel:sports”体育频道消息”James in Los Angeles” 12127.0.0.1:6379&gt; publish channel:sports "James in Los Angeles" (integer) 0 订阅消息 1subscribe channel [channel ...] 订阅消息可以订阅多条消息 订阅这个体育频道 12345127.0.0.1:6379&gt; subscribe channel:sports Reading messages... (press Ctrl-C to quit) 1) "subscribe" 2) "channel:sports" 3) (integer) 1 此时新开一个客户端往这个频道发布一条消息 12127.0.0.1:6379&gt; publish channel:sports "Harden in Houston" (integer) 1 这时候这个订阅者会受到刚刚发布消息的推送 12345678127.0.0.1:6379&gt; subscribe channel:sports Reading messages... (press Ctrl-C to quit) 1) "subscribe" 2) "channel:sports" 3) (integer) 1 1) "message" 2) "channel:sports" 3) "Harden in Houston" 客户端在执行订阅命令之后进入了订阅状态，只能接收subscribe、psubscribe、unsubscribe、punsubscribe的四个命令 新开启的订阅客户端，无法收到该频道之前的消息，因为Redis不会对发布的消息进行持久化 取消订阅 1unsubscribe channel [channel ...] 取消订阅体育频道 1234127.0.0.1:6379&gt; unsubscribe channel:sports 1) "unsubscribe" 2) "channel:sports" 3) (integer) 0 按照模式订阅和取消订阅 12psubscribe pattern [pattern...] punsubscribe [pattern [pattern ...]] 订阅以it开头的所有频道 12345127.0.0.1:6379&gt; psubscribe it* Reading messages... (press Ctrl-C to quit) 1) "psubscribe" 2) "it*" 3) (integer) 1 查询订阅 1.查看活跃的频道 1pubsub channels [pattern] 所谓活跃指的是最少有一个人订阅，其中[pattern]是可以指定具体的模式 12127.0.0.1:6379&gt; pubsub channels 1) "channel:sports" 2.查看频道订阅数 1pubsub numsub [channel ...] 查询channel:sports频道订阅数是1个 123127.0.0.1:6379&gt; pubsub numsub channel:sports 1) "channel:sports" 2) (integer) 1 3.查看模式订阅数 1pubsub numpat 当前通过模式订阅的有1个客户端 12127.0.0.1:6379&gt; pubsub numpat (integer) 1]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—Redis与Lua]]></title>
    <url>%2F2019%2F03%2F22%2Fredis-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-redis-e4-b8-8elua%2F</url>
    <content type="text"><![CDATA[使用Lua的好处 Lua脚本在Redis中是原子执行的，执行过程中间不会插入其他命令 Lua脚本可以帮助开发和运维人员创造出自己定制的命令，并可以将这些命令常驻在Redis内存中，实现复用的效果 Lua脚本可以将多条命令一次性打包，有效地减少网络开销 在Redis执行Lua脚本有两种方法：eval和evalsha eval eval 脚本内容 key 个数 key 列表 参数列表 下面例子使用了key列表和参数列表来为Lua脚本提供更多的灵活性 127.0.0.1:6379&gt; eval ‘return “hello “ ..KEYS[1]..ARGV[1]‘ 1 redis world“hello redisworld” 此时KEYS[1]=”redis”，ARGV[1]=”world”，所以最终的返回结果是”hello redisworld” 如果Lua脚本较长，还可以使用redis-cli–eval直接执行文件，eval命令和–eval参数本质是一样的，客户端如果想执行Lua脚本，首先在客户端编写好Lua脚本代码，然后把脚本作为字符串发送给服务端，服务端会将执行结果返回给客户端，执行流程如下图 evalsha 首先要将Lua脚本加载到Redis服务端，得到该脚本的SHA1校验和，evalsha命令使用SHA1作为参数可以直接执行对应Lua脚本，避免每次发送Lua脚本的开销。这样客户端就不需要每次执行脚本内容，而脚本也会常驻在服务端，脚本功能得到了复用 加载脚本：script load命令可以将脚本内容加载到Redis内存中，例如下面将lua_get.lua加载到Redis中，得到SHA1 evalsha 脚本 SHA1 值 key 个数 key 列表 参数列表 新建脚本文件 vim lua_test.lua 脚本内容，保存 return “hello” ..KEYS[1]..ARGV[1] 加载脚本 [root@vmzq1l0l ~]# redis-cli script load “$(cat lua_test.lua)”“af54f206bd1c4e5de6b4a1edefa9b22622ea0805” 执行脚本 [root@vmzq1l0l ~]# redis-cli127.0.0.1:6379&gt; evalsha af54f206bd1c4e5de6b4a1edefa9b22622ea0805 1 redis world“helloredisworld” Lua的Redis API Lua可以使用redis.call函数实现对Redis的访问，例如下面代码是Lua使用redis.call调用了Redis的set和get操作 redis.call(“set”, “hello”, “world”)redis.call(“get”, “hello”) 放在Redis的执行效果如下 127.0.0.1:6379&gt; eval ‘return redis.call(“get”, KEYS[1])’ 1 hello“world” 除此之外Lua还可以使用redis.pcall函数实现对Redis的调用，redis.call和redis.pcall的不同在于，如果redis.call执行失败，那么脚本执行结束会直接返回错误，而redis.pcall会忽略错误继续执行脚本，所以在实际开发中要根据具体的应用场景进行函数的选择 举例使用：如果排名功能，用户的列表是存储在key为user:rank的列表中，各个键各自也作为string的键存储排名，现在要用lua来原子性的把每个用户的排名数+1 127.0.0.1:6379&gt; rpush user:rank Tom(integer) 1127.0.0.1:6379&gt; rpush user:rank Jerry(integer) 2127.0.0.1:6379&gt; rpush user:rank Spike(integer) 3127.0.0.1:6379&gt; lrange user:rank 0 -11) “Tom”2) “Jerry”3) “Spike”127.0.0.1:6379&gt; set Tom 254OK127.0.0.1:6379&gt; set Jerry 87OK127.0.0.1:6379&gt; set Spike 15OK127.0.0.1:6379&gt; mget Tom Jerry Spike1) “254”2) “87”3) “15” 编写脚本incr_user_rank.lua –定义局部变量list读取传入的redis list类型的数据local list = redis.call(“lrange”,keys[1],0,-1)–count是自增的次数local count = 0for i,key in ipairs(list)do redis.call(“incr”,key) count = count + 1endreturn count 执行脚本，增加了三条数据 [root@vmzq1l0l ~]# redis-cli –eval incr_user_rank.lua user:rank(integer) 3 执行结果每条数据都+1了 127.0.0.1:6379&gt; mget Tom Jerry Spike1) “255”2) “88”3) “16” Redis中的lua脚本命令 scripts flush 清除Redis内存已经加载的所有Lua脚本 scripts exists sha1 [sha1 … ] 返回SHA1 是否存在redis脚本缓存中 script load script 将Lua脚本加载到Redis内存中 scripts kill 杀掉正在执行的Lua脚本]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Lua的基本使用]]></title>
    <url>%2F2019%2F03%2F22%2Flua-e7-9a-84-e5-9f-ba-e6-9c-ac-e4-bd-bf-e7-94-a8%2F</url>
    <content type="text"><![CDATA[从打印hello world开始 输入命令 lua -i 或 lua 来启用交互式编程模式，使用print函数打印字符串”hello world”： [root@vmzq1l0l ~]# luaLua 5.1.4 Copyright (C) 1994-2008 Lua.org, PUC-Rio&gt; print(“hello world”)hello world&gt; 有交互是编程，也有脚本式编程 编写脚本指定解释器为 /bash/lua （本人为centos7的系统，自带lua，如果其他版本没有Lua的需要自行安装） 编写脚本 [root@vmzq1l0l ~]# vim hello.lua 脚本内容 #!/bin/luaprint(“hello world”) 执行脚本 [root@vmzq1l0l ~]# lua hello.luahello world 赋权后直接执行赋权操作只需执行一次 [root@vmzq1l0l ~]# chmod +x hello.lua[root@vmzq1l0l ~]# ./hello.luahello world 数据类型 Lua中有8个基本类型分别为：nil、boolean、number、string、userdata、function、thread和table 数据类型 描述 nil 这个最简单，只有值nil属于该类，表示一个无效值（在条件表达式中相当于false） boolean 包含两个值：false和true number 表示双精度类型的实浮点数 string 字符串由一对双引号或单引号来表示 function 由 C 或 Lua 编写的函数 userdata 表示任意存储在变量中的C数据结构 thread 表示执行的独立线路，用于执行协同程序 table Lua 中的表（table）其实是一个”关联数组”（associative arrays），数组的索引可以是数字或者是字符串。在 Lua 里，table 的创建是通过”构造表达式”来完成，最简单构造表达式是{}，用来创建一个空表 type函数可以查看数据类型 [root@vmzq1l0l ~]# luaLua 5.1.4 Copyright (C) 1994-2008 Lua.org, PUC-Rio&gt; print(type(nil))nil&gt; print(type(true))boolean&gt; print(type(“hello world”))string&gt; print(type(type))function&gt; print(type(23))number 通过例子快速了解其中几个数据类型和逻辑处理的语法 string 声明string变量“Tom”，此处是全局变量，只要不退出客户端界面，就可以一直使用这个变量local声明string变量“Jerry”，这是个局部变量，只能在当前作用域使用 &gt; Tom=”I’m a cat”&gt; print(Tom)I’m a cat&gt; local Jerry=”I’m a mouse” print(Jerry) –局部变量只能在当前作用域使用I’m a mouse&gt; print(Jerry)nil&gt; local Jerry=”I’m a mouse” print(type(Jerry)) –局部变量只能在当前作用域使用string&gt; print(type(Jerry))nil&gt; print(Tom)I’m a cat&gt; print(type(Tom))string string块用两个方括号括起来可以换行，换行之后前面会变成两个大于号&gt;&gt;，直到]]封尾 &gt; Tom=[[I’m a cat; &gt; I hate Spike]]&gt; print(Tom)I’m a cat;I hate Spike 拼接字符串，使用 .. 两个英文句号拼接字符串 &gt; Tom=”I’m a cat;”&gt; Jerry=”I’m a mouse;”&gt; print(Tom .. Jerry) –使用..来拼接字符串I’m a cat;I’m a mouse; 延伸：注释 -- 为lua单行注释 –[[这是多行注释–]] boolean &gt; print(type(true))boolean&gt; print(type(false))boolean&gt; if false or nil &gt; then&gt; print(“至少有一个是true”)&gt; else&gt; print(“false和nil都是false”)&gt; endfalse和nil都是false number lua可以对number类型的数据进行运算 &gt; a=12&gt; b=34&gt; c=a+b&gt; d=a-b&gt; e=a/b&gt; print(c)46&gt; print(d)-22&gt; print(e)0.35294117647059&gt; print(type(c))number&gt; print(type(d))number&gt; print(type(e))number table table有点像java中的map可以自定义key值，但是不定义key值的话key默认从1开始索引， &gt; table1={key=”val1”,key2=”val2”,key3=”val3”}&gt; print(type(table1))table&gt; print(table1[“key”])val1&gt; print(table1[“key3”])val3&gt; table2={“Tom”,”Jerry”,”Spike”}&gt; print(type(table2))table&gt; print(table2[0])nil&gt; print(table2[1])Tom&gt; print(table2[2])Jerry 写个脚本table1.lua，for循环中的k,v是自定义的，可以写任何名称a,b；key,val···目的就是代表q前一个是索引，后一个是值，pairs()里面放要循环的table名 #!/bin/luatable1={}table1[“key”]=”value”key = 5table1[key] = 15table1[key] = table1[key] + 10for k,v in pairs(table1) doprint(k .. “:” .. v)end 执行结果 [root@vmzq1l0l ~]# lua table1.luakey:value5:25 function 在 Lua 中，函数是被看作是”第一类值（First-Class Value）”，函数可以存在变量里:写一个脚本function_test.lua，这个脚本内部相当于递归调用自己的方法进行阶乘的运算 #!/bin/luafunction factorial1(n) if n == 0 then return 1 else return n*factorial1(n-1) endendprint(factorial1(5))factorial2=factorial1print(factorial2(5)) 执行结果 [root@vmzq1l0l ~]# lua function_test.lua120120 function 可以以匿名函数（anonymous function）的方式通过参数传递，写一个脚本function_test2.lua， #!/bin/luafunction testFun(tab,fun) for key,val in pairs(tab) do print(fun(key,val)) endend tab={key1=”val1”,key2=”val2”}testFun(tab, function(key,val) –匿名函数 return key..”=”..val end) 执行结果 [root@vmzq1l0l ~]# lua function_test2.luakey1=val1key2=val2 thread 在 Lua 里，最主要的线程是协同程序（coroutine）。它跟线程（thread）差不多，拥有自己独立的栈、局部变量和指令指针，可以跟其他协同程序共享全局变量和其他大部分东西 线程跟协程的区别：线程可以同时多个运行，而协程任意时刻只能运行一个，并且处于运行状态的协程只有被挂起（suspend）时才会暂停 userdata userdata 是一种用户自定义数据，用于表示一种由应用程序或 C/C++ 语言库所创建的类型，可以将任意 C/C++ 的任意数据类型的数据（通常是 struct 和 指针）存储到 Lua 变量中调用]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[linux修改Shell脚本编码格式]]></title>
    <url>%2F2019%2F03%2F21%2Flinux%E4%BF%AE%E6%94%B9Shell%E8%84%9A%E6%9C%AC%E7%BC%96%E7%A0%81%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[问题 从windows用ftp传到linux的脚本往往编码格式是dos，如果想要在linux执行，必须修改为unix编码格式 解决 vim打开要改变编码的文件，然后输入告诉我们该脚本是utf-8编码的，这种编码无法在linux运行 12:set fffileformat=dos 修改编码,保存退出 12:set ff=unix:wq]]></content>
      <categories>
        <category>Shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Git上别人不小心交了配置文件，和你本地不相同，如何在不revert的情况下拉到代码]]></title>
    <url>%2F2019%2F03%2F20%2FGit%E4%B8%8A%E5%88%AB%E4%BA%BA%E4%B8%8D%E5%B0%8F%E5%BF%83%E4%BA%A4%E4%BA%86%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%EF%BC%8C%E5%92%8C%E4%BD%A0%E6%9C%AC%E5%9C%B0%E4%B8%8D%E7%9B%B8%E5%90%8C%EF%BC%8C%E5%A6%82%E4%BD%95%E5%9C%A8%E4%B8%8Drevert%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E6%8B%89%E5%88%B0%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[起因 别人提交的东西不是我需要的，但是git pull拉取的时候会报冲突，不revert线上的版本就拉不到其他代码 解决办法 git stash 创建本地的暂存代码 这样就可以创建本地的暂存代码了 然后git pull再拉取代码就不会出现冲突的情况了 git unstash 还原之前的暂存代码 还原的代码是线上和本地stash的合并代码]]></content>
      <categories>
        <category>Git</category>
        <category>IDE</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[redis.conf配置文件（中文翻译）]]></title>
    <url>%2F2019%2F03%2F19%2Fredis.conf%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%EF%BC%88%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91%EF%BC%89%2F</url>
    <content type="text"><![CDATA[部分配置做了中文翻译，也可对照英文默认配置文档阅读 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894# Redis 配置文件样本## 注意：如果想要读取配置文件的参数，必须将配置文件以第一参数的形式启动，如下启动示例:## ./redis-server /path/to/redis.conf# 单位说明：当需要设置内存大小时，可以用1K 5GB 4M等常用格式指定：## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## 单位不区分大小写 所以 1GB 1Gb 1gB 都是一样的.################################## 多包含 #################################### 在此处包含一个或多个其他配置文件。# 如果您有一个标准模板，该模板可用于所有Redis服务器，# 但还需要自定义每个服务器的一些设置，则此模板非常有用。# include文件可以包括其他文件，因此请聪明地使用它。## 注意选项“include”不会被来自admin或redis sentinel的命令“config rewrite”重写。# 因为redis总是使用最后处理的行作为配置指令的值，所以最好将include放在该文件的开头，# 以避免在运行时覆盖配置更改。## 反之如果您对使用include覆盖配置选项感兴趣，最好使用include作为最后一行。## include /path/to/local.conf# include /path/to/other.conf################################## 多模块 ###################################### 启动时加载模块。如果服务器无法加载模块，它将中止。可以使用多个loadmodule指令。## loadmodule /path/to/my_module.so# loadmodule /path/to/other_module.so################################## 网络 ###################################### 默认情况下如果没有指定bind配置# Redis侦听服务器上所有可用网络接口的连接（指的是网卡）# 可以使用“bind”配置指令只侦听一个或多个选定的接口，后跟一个或多个IP地址。## Examples:## bind 192.168.1.100 10.0.0.1# bind 127.0.0.1 ::1## ~~~ 警告 ~~~ 如果运行Redis的计算机直接暴露在Internet上，# 绑定到所有接口是危险的，并且会将实例暴露给互联网上的每个人。# 因此，默认情况下，设置了bind 127.0.0.1，# 这意味着Redis只能接受运行在同一台计算机上的客户端的连接。## 如果您确定希望实例监听所有接口# 只需要注释下面这一行.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bind 127.0.0.1# 保护模式是一个安全保护层，为了避免在Internet上打开的Redis实例被访问和利用## 当保护模式打开时，如果:## 1) 服务器没有使用“bind”指令显式绑定到一组地址。# 2) 未配置密码。## 服务器只接受来自从IPv4和IPv6内网地址# 127.0.0.1和：：1连接的客户端以及来自Unix域socket的连接。## 默认情况下启用保护模式。 只有当您确定希望来自其他主机的客户机连接到Redis时，# 您才应该禁用它，即使没有配置身份验证，也没有使用“bind”指令显式列出特定的接口集。protected-mode yes# 指定端口上的连接，默认值为6379# 如果指定了端口0，Redis将不会侦听TCP连接。port 6379# TCP listen() backlog.## 在每秒高请求数的环境中，为了避免客户机连接速度慢的问题，您需要大量backlog。# 请注意，Linux内核将自动将其截断为/proc/sys/net/core/somaxconn的值，# 因此请确保同时提高somaxconn和tcp-max-syn-u backlog的值，以获得所需的效果。tcp-backlog 511# Unix socket.## 指定将用于侦听传入连接的Unix socket的路径。# 没有默认值，因此未指定时，redis不会在UNIX socket上侦听。## unixsocket /tmp/redis.sock# unixsocketperm 700# 客户端空闲n秒后关闭连接（0表示禁用）timeout 0# TCP keepalive.## 如果非零，则使用so-keepalive在没有通信的情况下向客户机发送TCP ACK。这有两个原因:## 1) 能够检测无响应的服务# 2) 让该连接中间的网络设备知道这个连接还存活## 在Linux上，这个指定的值(单位秒)就是发送ACK的时间间隔。# 注意：要关闭这个连接需要两倍的这个时间值。# 在其他内核上这个时间间隔由内核配置决定tcp-keepalive 300################################# 常用 ###################################### 默认情况下，redis不作为守护进程运行。如果需要，请使用“是”。请注意，当后台监控时，# redis将在/var/run/redis.pid中写入一个pid文件。daemonize no# 是否通过upstart或systemd管理守护进程。# 默认no没有服务监控，其它选项有upstart, systemd, auto。supervised no# 生成的pid文件位置pidfile /var/run/redis_6379.pid# 指定服务器日志级别# 可以配置为其中一个参数:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel notice# 指定日志文件名。空字符串还可以用于强制redis记录标准输出# 请注意，如果使用标准输出进行日志记录，但使用后台监控，则日志将发送到/dev/nulllogfile ""# 启用系统日志记录, 只需要设置 'syslog-enabled' 为 yes,# 还可以根据需要更新其他系统日志参数# syslog-enabled no# 指定系统日志标识# syslog-ident redis# 指定系统日志功能。必须是用户或介于 LOCAL0-LOCAL7.# syslog-facility local0# 设置数据库数。默认数据库为db0,# 您可以使用select&lt;dbid&gt;在每个连接上选择不同的连接databases 16# redis启动时是否显示Logoalways-show-logo yes################################ 快照 ################################## 保存数据到硬盘（持久化）:## save &lt;seconds&gt; &lt;changes&gt;## 如果在&lt;seconds&gt;秒后执行过&lt;changes&gt;次改变将会保存到硬盘## save 900 1 表示如果在900秒后至少发生了1次改变就会保存# save 300 10 表示如果在300秒后至少发生了10次改变就会保存# save save 60 10000 表示如果在60秒后至少发生了10000次改变就会保存## 注意：你可以注释掉所有的 save 行来停用保存功能。# 也可以直接一个空字符串来实现停用：# save ""save 900 1save 300 10save 60 10000# 默认情况下，如果 redis 最后一次的后台保存失败，redis 将停止接受写操作，# 这样以一种强硬的方式让用户知道数据不能正确的持久化到磁盘，# 否则就会没人注意到灾难的发生。## 如果后台保存进程重新启动工作了，redis 也将自动的允许写操作。## 然而你要是安装了靠谱的监控，你可能不希望 redis 这样做，那你就改成 no 好了。stop-writes-on-bgsave-error yes# 是否在 dump .rdb 数据库的时候使用 LZF 压缩字符串# 默认都设为 yes# 如果你希望保存子进程节省点 cpu ，你就设置它为 no ，# 不过这个数据集可能就会比较大rdbcompression yes# 是否校验rdb文件rdbchecksum yes# 工作目录# 例如上面的 dbfilename 只指定了文件名，# 但是它会写入到这个目录下。这个配置项一定是个目录，而不能是文件名。dir ./################################# 复制 ################################## 主从复制. 使用replicoaf将一个redis实例作为另一个redis服务器的副本，# 只需要在从服务器配置# A few things to understand ASAP about Redis replication.## +------------------+ +---------------+# | Master | ---&gt; | Replica |# | (receive writes) | | (exact copy) |# +------------------+ +---------------+## 1) Redis复制是异步的, but you can configure a master to# stop accepting writes if it appears to be not connected with at least# a given number of replicas.# 2) 如果复制链接丢失的时间相对较短，Redis副本可以执行与主服务器的部分重新同步。# 根据需要，您可能需要使用合理的值配置复制积压工作的大小（请参阅此文件的下一节）。# 3) 复制是自动的，不需要用户干预。# 在网络分区复制副本自动尝试重新连接到主服务器并与主服务器重新同步之后。## replicaof &lt;masterip&gt; &lt;masterport&gt;# 同步的主服务的密码# masterauth &lt;master-password&gt;# 当一个从服务失去和主服务的连接，或者同步正在进行中，副本可以以两种不同的方式进行操作：# 1) 如果 replica-serve-stale-data 设置为 "yes" (默认值)，# 从服务器会继续响应客户端请求，可能是正常数据，也可能是还没获得值的空数据。# 2) 如果 replica-serve-stale-data 设置为 "no"，# 从服务会回复"正在从主服务同步（SYNC with master in progress）"来处理各种请求，# 除了 INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG,# SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB,# COMMAND, POST, HOST: and LATENCY。replica-serve-stale-data yes# 你可以配置从服务实例是否接受写操作。可写的从服务实例可能对存储临时数据可能很有用(因为# 写入从服务的数据在同主服务同步之后将很容被删除)，# 但是如果客户端由于配置错误在写入时也可能导致问题。# 从Redis2.6默认所有的从服务为只读# 注意:只读的slave不是为了暴露给互联网上不可信的客户端而设计的。# 它只是一个防止实例误用的保护层。# 一个只读的slave支持所有的管理命令比如config,debug等。# 为了限制你可以用'rename-command'来隐藏所有的管理和危险命令来增强只读slave的安全性。replica-read-only yes# 复制同步策略：磁盘或socket。## -------------------------------------------------------# WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY# -------------------------------------------------------## 新的从服务和重新连接的从服务在复制的过程中将会无法接收差异数据，# 需要做完全同步，RDB文件从主服务传输到从服务# 传输有两种不同的方式：## 1) 磁盘备份: redis主机创建了一个新进程，该进程将RDB文件写入磁盘。# 随后，父进程将文件增量传输到副本。# 2) 无盘备份: redis master创建了一个新的进程，# 该进程直接将RDB文件写入从服务sockets，而根本不接触磁盘。## 使用磁盘备份复制，在生成RDB文件的同时，# 可以在当前生成子级时将更多从服务排队并与RDB文件一起提供服务。# RDB文件完成其工作. 在无盘复制中，一旦传输开始，# 新的从服务将会排队等到当前从服务终止才会开始传输## 当使用无盘复制时, 服务器在开始传输之前等待一段可配置的时间（以秒为单位），# 希望多个从服务能够到达，并且传输可以并行。## 使用低速磁盘和快速（大带宽）网络，无盘复制会更好。repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay the server waits in order to spawn the child that transfers the RDB via socket to the replicas.# 当启用无盘复制时，可以配置服务器等待的延迟，以便生成通过socket将RDB传输到副本的子级。## 这很重要，因为一旦转移开始，可能将不会为新的从服务提供服务,# 那将排队等待下一次RDB传输，因此，服务器会等待一段延迟，以便让更多的从服务到达## 延迟以秒为单位，默认为5秒。要完全禁用它，只需将其设置为0秒，传输将尽快开始。repl-diskless-sync-delay 5# 副本以预先定义的间隔向服务器发送ping。# 可以使用repl-ping-u replica-period选项更改此间隔。默认值为10秒。## repl-ping-replica-period 10# 以下选项设置复制超时：## 1) 从复制副本的角度看，同步期间的批量传输I/O。# 2) 从从服务的角度看主服务的超时 (data, pings)。# 3) 从主服务器的角度来看，复制超时 (REPLCONF ACK pings)。## 必须确保该值大于为repl-ping-replica-period指定的值，# 否则每次主服务器和副本之间的通信量低时都会检测到超时。## repl-timeout 60# 同步后在从服务socket上禁用TCP_NODELAY？## 如果选择“yes”，Redis将使用较少的TCP数据包和较少的带宽向从服务发送数据。# 但这可能会增加数据在从服务端出现的延迟，对于使用默认配置的Linux内核，延迟最长可达40毫秒。## 如果选择“no”，将减少数据出现在从服务端的延迟，但将使用更多带宽进行复制。## 默认情况下，我们会针对低延迟进行优化，但在非常高的流量条件下，# 或者当主服务器和从哪个服务器距离很多跃点时，将其设置为“yes”会更好repl-disable-tcp-nodelay no# 设置从服务积压量backlog大小。backlog是一个缓冲区，当从服务器断开连接一段时间后，# 它会累积从服务的数据，因此当从服务希望再次重新连接时，通常不需要完全重新同步，# 只部分重新同步就足够了，只需传递断开连接时从服务丢失的数据部分。## 从服务积压量backlog越大，从服务可断联时间可以越长，之后就可以执行部分重新同步。## 只有在至少连接了一个从服务后，才会分配积压空间backlog。## repl-backlog-size 1mb# 在主服务器一段时间内不再连接副本后，将释放backlog空间。# 以下选项配置从最后一个从服务断开连接开始，需要经过的秒数，以便释放backlog缓冲区。## 注意，从服务永远不会释放积压的超时时间，因为它们可能在主服务挂掉之后被提升为主服务，# 并且应该能够与从服务正确地“部分重新同步”：因此它们应该总是积累backlog。## 值为0表示永不释放backlog。## repl-backlog-ttl 3600# 从服务优先级是整数类型的参数# Redis Sentinel使用它来选择一个从服务，以便在主服务器不再正常工作时升级为主服务器## 设置值越小优先级越高## 但是，特殊优先级0将副本标记为无法执行master角色，# 因此redis sentinel将永远不会选择优先级为0的副本进行升级。## 默认配置为100replica-priority 100# 如果连接的从服务少于n个，且延迟小于或等于m秒，则主机可以停止接受写入。## n个副本需要处于“oneline”状态。## 延时是以秒为单位，并且必须小于等于指定值，# 是从最后一个从slave接收到的ping（通常每秒发送）开始计数。# 该选项不保证N个slave正确同步写操作，但是限制数据丢失的窗口期。## 例如至少需要3个延时小于等于10秒的从服务用下面的指令：## min-replicas-to-write 3# min-replicas-max-lag 10## 将其中一个设置为0将禁用该功能。# 默认min-replicas-to-write是0，min-replicas-max-lag是10.# redis主服务可以以不同的方式列出从服务的地址和端口。# 比如"INFO replication"命令就可以查看这些信息，# Redis Sentinel使用它和其他工具来发现副本实例。# 另一个可以获得此信息的地方是在主服务使用“role”命令查看这些信息## 从服务通常报告的列出的IP和地址是通过以下方式获得的：## IP: 通过检查从服务用于与主服务器连接的socket的对等地址，可以自动检测该地址。## Port: 在复制握手期间，该端口由从服务通信，通常是在从服务用来侦听连接的端口。## 但是，当使用端口转发或网络地址转换（NAT）时，从服务实际上可以通过不同的IP和端口对访问。# 下面两个参数就是为了解决这个问题的，可以自行设置，从节点上报给master的自己ip和端口## replica-announce-ip 5.5.5.5# replica-announce-port 1234################################## 安全 #################################### 要求客户端在处理任何其他命令之前发出auth&lt;password&gt;。# 在您不信任其他人访问运行redis服务器的主机的环境中，这可能很有用。## 为了向后兼容，并且因为大多数人不需要身份验证（例如，他们运行自己的服务器），# 所以应该将其注释掉。## 警告: 由于redis速度非常快，外部用户尝试破解密码速度会达到每秒多达150k次。# 这意味着您应该使用一个非常强的密码，否则很容易破解。## requirepass foobared# 命令重命名。## 在共享环境下，可以为危险命令改变名字。# 比如，你可以为 CONFIG 改个其他不太容易猜到的名字，这样内部的工具仍然可以使用## 比如:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## 也可以通过将命令重命名为空字符串来完全禁用它。## rename-command CONFIG ""## 请注意：改变命令名字被记录到AOF文件或被传送到从服务器可能产生问题################################### 客户端 ##################################### 同时设置最大连接客户端数。 默认情况下，此限制设置为10000个客户端，# 但是如果Redis服务器无法配置进程文件限制以允许指定的限制，# 则允许的最大客户端数设置为当前文件限制减去32#（因为Redis保留一些文件描述符供内部使用）。## 达到限制后，Redis将关闭所有发送“max number of clients reached”错误的新连接。## maxclients 10000############################## 内存管理 ################################# 设置使用内存上限，当达到上限，Redis会尝试根据maxmemory-policy的删除策略删除keys## 如果redis不能根据策略删除keys,或者如果策略设置为“noevicetion”, # Redis会回复需要更多内存的错误信息给命令。# 例如，SET,LPUSH等等，但是会继续响应像Get这样的只读命令。## 当将redis用作LRU或LFU缓存# 或者为实例设置了硬性内存限制的时候（使用“noevicetion”策略）时，此选项很有用。## 警告：当有多个slave连上达到内存上限时，# 主服务为同步从服务的输出缓冲区所需内存不计算在使用内存中。# 这样当移除key时，就不会因网络问题 / 重新同步事件触发移除key的循环，# 反过来从服务的输出缓冲区充满了key被移除的DEL命令，这将触发删除更多的key，# 直到这个数据库完全被清空为止。## 总之：如果您连接多个从服务，建议您为maxmemory设置一个下限，# 以便系统上有一些用于副本输出缓冲区的可用RAM（但如果策略为“noevection”，则不需要这样做）。## maxmemory &lt;bytes&gt;# MAXMEMORY POLICY:当达到最大内存时，Redis如何选择要删除的内容。你可以选择下面五种之一:## volatile-lru -&gt; 根据LRU算法删除设置过期时间的key# allkeys-lru -&gt; 根据LRU算法删除任何key# volatile-random -&gt; 随机删除设置了过期时间的key# allkeys-random -&gt; 随机删除任何key# volatile-ttl -&gt; 删除即将过期的key(minor TTL)# noeviction -&gt; 不删除任何key，只返回一个写错误信息## LRU means Least Recently Used# LFU means Least Frequently Used## Both LRU, LFU and volatile-ttl are implemented using approximated# randomized algorithms.## 注意: 以上任何一项政策，当没有腾出足够的空间执行写命令前，redis都会报一个写错误# 可能受影响的命令: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## 默认配置如下（不做删除操作）：## maxmemory-policy noeviction# LRU、LFU和最小TTL算法不是精确算法，而是近似算法（为了节省内存）,# 所以你可以调整它的速度或精度。对于默认的redis将检查五个键并选择最近使用较少的键，# 您可以使用以下配置指令更改样本大小。## 默认值5会产生足够好的结果。10非常接近真实的LRU，但消耗更多的CPU。3速度更快，但不太准确。## maxmemory-samples 5# 从redis 5开始，默认情况下从服务将忽略其maxmemory设置（除非在故障是Redis cluster转移# 主从或手动将从服务升级为主服务）这意味着删除策略只有主服务处理, 只是发送del命令给从服务## 这样可以保证主从数据的一致性# 但是，如果您的从服务是可写的，或者您希望从服务具有不同的内存设置，# 并且您确定对从服务执行的所有写入都是等幂的，则可以更改此默认值(但一定要明白你在做什么)。## replica-ignore-maxmemory yes############################# LAZY FREEING ##################################### Redis has two primitives to delete keys. One is called DEL and is a blocking# deletion of the object. It means that the server stops processing new commands# in order to reclaim all the memory associated with an object in a synchronous# way. If the key deleted is associated with a small object, the time needed# in order to execute the DEL command is very small and comparable to most other# O(1) or O(log_N) commands in Redis. However if the key is associated with an# aggregated value containing millions of elements, the server can block for# a long time (even seconds) in order to complete the operation.## For the above reasons Redis also offers non blocking deletion primitives# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and# FLUSHDB commands, in order to reclaim memory in background. Those commands# are executed in constant time. Another thread will incrementally free the# object in the background as fast as possible.## DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.# It's up to the design of the application to understand when it is a good# idea to use one or the other. However the Redis server sometimes has to# delete keys or flush the whole database as a side effect of other operations.# Specifically Redis deletes objects independently of a user call in the# following scenarios:## 1) On eviction, because of the maxmemory and maxmemory policy configurations,# in order to make room for new data, without going over the specified# memory limit.# 2) Because of expire: when a key with an associated time to live (see the# EXPIRE command) must be deleted from memory.# 3) Because of a side effect of a command that stores data on a key that may# already exist. For example the RENAME command may delete the old key# content when it is replaced with another one. Similarly SUNIONSTORE# or SORT with STORE option may delete existing keys. The SET command# itself removes any old content of the specified key in order to replace# it with the specified string.# 4) During replication, when a replica performs a full resynchronization with# its master, the content of the whole database is removed in order to# load the RDB file just transferred.## In all the above cases the default is to delete objects in a blocking way,# like if DEL was called. However you can configure each case specifically# in order to instead release memory in a non-blocking way like if UNLINK# was called, using the following configuration directives:lazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noreplica-lazy-flush no############################## APPEND ONLY MODE ################################ 默认情况下，redis异步转储磁盘上的数据集。在许多应用程序中，这种模式已经足够好了，# 但是Redis进程的问题或断电可能会导致几分钟的写入丢失（取决于配置的保存点）。## AOF是另一种持久性模式，可提供更好的耐久性。例如，如果使用RDB策略,# Redis挂掉（如服务器断电）时只会丢失一秒钟的写入时间## AOF和RDB持久性可以同时启用，不会出现问题。# Redis会优先使用AOF策略，因为更好地保证了数据持久化## 请访问 http://redis.io/topics/persistence 获取更多信息appendonly no# AOF文件名（默认："appendonly.aof"）appendfilename "appendonly.aof"# fsync() 系统调用告诉操作系统把数据写到磁盘上，而不是等更多的数据进入输出缓冲区。# 有些操作系统会真的把数据马上刷到磁盘上；有些则会尽快去尝试这么做。# Redis支持三种不同的模式：# no：不要立刻刷，只有在操作系统需要刷的时候再刷。比较快。# always：每次写操作都立刻写入到aof文件。慢，但是最安全。# everysec：每秒写一次。折中方案。# 默认的 "everysec" 通常来说能在速度和数据安全性之间取得比较好的平衡。## 更多细节信息访问下面链接：# http://antirez.com/post/redis-persistence-demystified.html## 如果不确定就使用everysec# appendfsync alwaysappendfsync everysec# appendfsync no# 当AOF同步策略设置为always或everysec时，后台保存进程(后台保存或写入AOF日志)正在对# 磁盘执行大量I/O，在某些Linux配置中，redis可能会在fsync（）调用上阻塞太长时间。# 注意，目前对这个情况还没有完美修正，甚至不同线程的 fsync() 会阻塞我们同步的write(2)调用。## 为了缓解这个问题，可以用下面这个选项。它可以在 BGSAVE 或 BGREWRITEAOF# 处理时阻止fsync()。## 这意味着当另一个子进程在保存时, 那么Redis就处于"不可同步"的状态。# 这实际上是说，在最差的情况下可能会丢掉30秒钟的日志数据。（默认Linux设定）## 如果把这个设置成"yes"带来了延迟问题，就保持"no"，这是保存持久数据的最安全的方式。no-appendfsync-on-rewrite no# 自动重写AOF文件# 当AOF日志大小增加指定的百分比时，Redis能够调用BGREWRITEAOF自动重写AOF的日志文件。## 工作原理:Redis记住上次重写AOF文件的大小(如果重启后还没有写操作，就直接用启动时的AOF大小)## 这个基准大小和当前大小做比较。如果当前大小超过指定比例，就会触发重写操作。# 你还需要指定被重写日志的最小尺寸，这样避免了达到指定百分比但尺寸仍然很小的情况还要重写。## 指定百分比为0会禁用AOF自动重写特性auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# 如果设置为yes，如果一个因异常被截断的AOF文件被redis启动时加载进内存，# redis将会发送日志通知用户。如果设置为no，erdis将会拒绝启动。# 此时需要用"redis-check-aof"工具修复文件。aof-load-truncated yes# 在重写AOF文件时，Redis可以使用AOF文件中的RDB前导码来更快地重写和恢复。# 启用此选项时，重写的AOF文件由两个不同的参数组成。## [RDB file][AOF tail]## 当加载redis时，识别出AOF文件以“redis”字符串开头并加载前缀RDB文件，然后继续加载AOF尾部。aof-use-rdb-preamble yes################################ LUA 脚本 ################################ Lua脚本的最大执行时间（毫秒）lua-time-limit 5000################################ REDIS 集群 ################################# 如果想让Redis实例作为集群的一部分，需要去掉下方配置的注释# cluster-enabled yes# 配置redis自动生成的集群配置文件名。确保同一系统中运行的各redis实例该配置文件不要重名。# cluster-config-file nodes-6379.conf# 集群节点超时毫秒数。超时的节点将被视为不可用状态。# cluster-node-timeout 15000# 如果发生故障的主机从服务的数据太旧了，这个从服务会避免升级为主服务，# 如果主从失联时间超过：# (node-timeout * replica-validity-factor) + repl-ping-replica-period# 则不会被提升为master## 如node-timeout为30秒，slave-validity-factor为10,slave-validity-factor为10,# 默认default repl-ping-slave-period为10秒,失联时间超过310秒slave就不会成为master。## 较大的slave-validity-factor值可能允许包含过旧数据的从服务器成为主服务器，# 同时较小的值可能会阻止集群选举出新主服务。# 为了达到最大限度的高可用性，可以设置为0，即从服务不管和主服务失联多久都可以提升为主服务## cluster-replica-validity-factor 10# 只有在之前master有其它指定数量的工作状态下的slave节点时，slave节点才能提升为master。# 默认为1（即该集群至少有3个节点，1 master＋2 slaves，master宕机，# 仍有另外1个slave的情况下其中1个slave可以提升）# 测试环境可设置为0，生成环境中至少设置为1# cluster-migration-barrier 1# 默认情况下如果redis集群如果检测到至少有1个hash slot不可用，集群将停止查询数据。# 如果所有slot恢复则集群自动恢复。# 如果需要集群部分可用情况下仍可提供查询服务，设置为no。# cluster-require-full-coverage yes# 选项设置为yes时，会阻止replicas尝试对其主服务在主故障期间进行故障转移# 然而，主服务仍然可以执行手动故障转移,如果强制这样做的话。# cluster-replica-no-failover no# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.########################## CLUSTER DOCKER/NAT support ######################### In certain deployments, Redis Cluster nodes address discovery fails, because# addresses are NAT-ted or because ports are forwarded (the typical case is# Docker and other containers).## In order to make Redis Cluster working in such environments, a static# configuration where each node knows its public address is needed. The# following two options are used for this scope, and are:## * cluster-announce-ip# * cluster-announce-port# * cluster-announce-bus-port## Each instruct the node about its address, client port, and cluster message# bus port. The information is then published in the header of the bus packets# so that other nodes will be able to correctly map the address of the node# publishing the information.## If the above options are not used, the normal Redis Cluster auto-detection# will be used instead.## Note that when remapped, the bus port may not be at the fixed offset of# clients port + 10000, so you can specify any port and bus-port depending# on how they get remapped. If the bus-port is not set, a fixed offset of# 10000 will be used as usually.## Example:## cluster-announce-ip 10.1.1.5# cluster-announce-port 6379# cluster-announce-bus-port 6380################################## 慢日志 #################################### 慢查询日志，记录超过多少微秒的查询命令。# 查询的执行时间不包括客户端的IO执行和网络通信时间，只是查询命令执行时间。# 1000000等于1秒，设置为0则记录所有命令slowlog-log-slower-than 10000# 记录大小，可通过SLOWLOG RESET命令重置slowlog-max-len 128################################ 延时监控 ############################### redis延时监控系统在运行时会采样一些操作，以便收集可能导致延时的数据根源。# 通过 LATENCY命令 可以打印一些图样和获取一些报告，方便监控# 这个系统仅仅记录那个执行时间大于或等于预定时间（毫秒）的操作,# 这个预定时间是通过latency-monitor-threshold配置来指定的，# 当设置为0时，这个监控系统处于停止状态latency-monitor-threshold 0############################# 事件通知 ############################### K 键空间通知，所有通知以 __keyspace@&lt;db&gt;__ 为前缀# E 键事件通知，所有通知以 __keyevent@&lt;db&gt;__ 为前缀# g DEL 、 EXPIRE 、 RENAME 等类型无关的通用命令的通知# $ 字符串命令的通知# l 列表命令的通知# s 集合命令的通知# h 哈希命令的通知# z 有序集合命令的通知# x 过期事件：每当有过期键被删除时发送# e 驱逐(evict)事件：每当有键因为 maxmemory 政策而被删除时发送# A 参数 g$lshzxe 的别名# Example: to enable list and generic events, from the point of view of the# event name, use:## notify-keyspace-events Elg## Example 2: to get the stream of the expired keys subscribing to channel# name __keyevent@0__:expired use:## notify-keyspace-events Ex# Redis能通知 Pub/Sub 客户端关于键空间发生的事件，默认关闭notify-keyspace-events ""############################### 高级配置 ################################ 当hash只有少量的entry时，并且最大的entry所占空间没有超过指定的限制时，会用一种节省内存的# 数据结构来编码。可以通过下面的指令来设定限制hash-max-ziplist-entries 512hash-max-ziplist-value 64# 当取正值的时候，表示按照数据项个数来限定每个quicklist节点上的ziplist长度。# 比如，当这个参数配置成5的时候，表示每个quicklist节点的ziplist最多包含5个数据项。# 当取负值的时候，表示按照占用字节数来限定每个quicklist节点上的ziplist长度。# 这时，它只能取-1到-5# 这五个值，每个值含义如下：# -5: 每个quicklist节点上的ziplist大小不能超过64 Kb。（注：1kb =&gt; 1024 bytes）# -4: 每个quicklist节点上的ziplist大小不能超过32 Kb。# -3: 每个quicklist节点上的ziplist大小不能超过16 Kb。# -2: 每个quicklist节点上的ziplist大小不能超过8 Kb。（-2是Redis给出的默认值）# -1: 每个quicklist节点上的ziplist大小不能超过4 Kb。list-max-ziplist-size -2# 这个参数表示一个quicklist两端不被压缩的节点个数。# 注：这里的节点个数是指quicklist双向链表的节点个数，而不是指ziplist里面的数据项个数。# 实际上，一个quicklist节点上的ziplist，如果被压缩，就是整体被压缩的。# 参数list-compress-depth的取值含义如下：# 0: 是个特殊值，表示都不压缩。这是Redis的默认值。# 1: 表示quicklist两端各有1个节点不压缩，中间的节点压缩。# 2: 表示quicklist两端各有2个节点不压缩，中间的节点压缩。# 3: 表示quicklist两端各有3个节点不压缩，中间的节点压缩。# 依此类推…# 由于0是个特殊值，很容易看出quicklist的头节点和尾节点总是不被压缩的，# 以便于在表的两端进行快速存取。list-compress-depth 0# set有一种特殊编码的情况：当set数据全是十进制64位有符号整型数字构成的字符串时。# 下面这个配置项就是用来设置set使用这种编码来节省内存的最大长度。set-max-intset-entries 512# 与hash和list相似，有序集合也可以用一种特别的编码方式来节省大量空间。# 这种编码只适合长度和元素都小于下面限制的有序集合zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog稀疏结构表示字节的限制。该限制包括# 16个字节的头。当HyperLogLog使用稀疏结构表示# 这些限制，它会被转换成密度表示。# 值大于16000是完全没用的，因为在该点# 密集的表示是更多的内存效率。# 建议值是3000左右，以便具有的内存好处, 减少内存的消耗hll-sparse-max-bytes 3000# Streams宏节点最大大小/项目。 流数据结构是基数编码内部多个项目的大节点树。# 使用此配置可以配置单个节点的字节数，以及切换到新节点之前可能包含的最大项目数# 追加新的流条目。 如果以下任何设置设置为0，忽略限制，因此例如可以设置一个# 大入口限制将max-bytes设置为0，将max-entries设置为所需的值stream-node-max-bytes 4096stream-node-max-entries 100# 启用哈希刷新，每100个CPU毫秒会拿出1个毫秒来刷新Redis的主哈希表（顶级键值映射表）activerehashing yes# 客户端的输出缓冲区的限制，可用于强制断开那些因为某种原因从服务器# 读取数据的速度不够快的客户端client-output-buffer-limit normal 0 0 0client-output-buffer-limit replica 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# 客户端查询缓冲区累积新命令。 它们仅限于固定的默认情况下，# 多数情况下为了避免协议不同步导致客户端查询缓冲区中未绑定的内存使用量的错误# 但是，如果你有使用的话，你可以在这里配置它，比如我们有很多执行请求或类似的。# client-query-buffer-limit 1gb# 在Redis协议中，批量请求，即表示单个的元素strings，通常限制为512 MB。# 但是，您可以z更改此限制# proto-max-bulk-len 512mb# 默认情况下，“hz”的被设定为10。提高该值将在Redis空闲时使用更多的CPU时，但同时当有多个key# 同时到期会使Redis的反应更灵敏，以及超时可以更精确地处理hz 10# Normally it is useful to have an HZ value which is proportional to the# number of clients connected. This is useful in order, for instance, to# avoid too many clients are processed for each background task invocation# in order to avoid latency spikes.## Since the default HZ value by default is conservatively set to 10, Redis# offers, and enables by default, the ability to use an adaptive HZ value# which will temporary raise when there are many connected clients.## When dynamic HZ is enabled, the actual configured HZ will be used as# as a baseline, but multiples of the configured HZ value will be actually# used as needed once more clients are connected. In this way an idle# instance will use very little CPU time while a busy instance will be# more responsive.dynamic-hz yes# 当一个子进程重写AOF文件时，如果启用下面的选项，则文件每生成32M数据会被同步aof-rewrite-incremental-fsync yes# 当redis保存RDB文件时，如果启用了以下选项，每生成32 MB数据，文件将被fsync-ed。# 这很有用，以便以递增方式将文件提交到磁盘并避免大延迟峰值。rdb-save-incremental-fsync yes# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good# idea to start with the default settings and only change them after investigating# how to improve the performances and how the keys LFU change over time, which# is possible to inspect via the OBJECT FREQ command.## There are two tunable parameters in the Redis LFU implementation: the# counter logarithm factor and the counter decay time. It is important to# understand what the two parameters mean before changing them.## The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis# uses a probabilistic increment with logarithmic behavior. Given the value# of the old counter, when a key is accessed, the counter is incremented in# this way:## 1. A random number R between 0 and 1 is extracted.# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).# 3. The counter is incremented only if R &lt; P.## The default lfu-log-factor is 10. This is a table of how the frequency# counter changes with a different number of accesses with different# logarithmic factors:## +--------+------------+------------+------------+------------+------------+# | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits |# +--------+------------+------------+------------+------------+------------+# | 0 | 104 | 255 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 1 | 18 | 49 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 10 | 10 | 18 | 142 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 100 | 8 | 11 | 49 | 143 | 255 |# +--------+------------+------------+------------+------------+------------+## NOTE: The above table was obtained by running the following commands:## redis-benchmark -n 1000000 incr foo# redis-cli object freq foo## NOTE 2: The counter initial value is 5 in order to give new objects a chance# to accumulate hits.## The counter decay time is the time, in minutes, that must elapse in order# for the key counter to be divided by two (or decremented if it has a value# less &lt;= 10).## The default value for the lfu-decay-time is 1. A Special value of 0 means to# decay the counter every time it happens to be scanned.## lfu-log-factor 10# lfu-decay-time 1########################### ACTIVE DEFRAGMENTATION ######################## 启用主动碎片整理# activedefrag yes# 启动活动碎片整理的最小碎片浪费量# active-defrag-ignore-bytes 100mb# 启动碎片整理的最小碎片百分比# active-defrag-threshold-lower 10# 使用最大消耗时的最大碎片百分比# active-defrag-threshold-upper 100# 在CPU百分比中进行碎片整理的最小消耗# active-defrag-cycle-min 5# 磁盘碎片整理的最大消耗# active-defrag-cycle-max 75# 将从主字典扫描处理的最大set / hash / zset / list字段数# active-defrag-max-scan-fields 1000]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[redis.conf配置文件（英文原文）]]></title>
    <url>%2F2019%2F03%2F19%2Fredis.conf%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%EF%BC%88%E8%8B%B1%E6%96%87%E5%8E%9F%E6%96%87%EF%BC%89%2F</url>
    <content type="text"><![CDATA[配置文件中有很详细的注释，告诉各个配置参数的用途，可以对照部分中文翻译的文档阅读 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997998999100010011002100310041005100610071008100910101011101210131014101510161017101810191020102110221023102410251026102710281029103010311032103310341035103610371038103910401041104210431044104510461047104810491050105110521053105410551056105710581059106010611062106310641065106610671068106910701071107210731074107510761077107810791080108110821083108410851086108710881089109010911092109310941095109610971098109911001101110211031104110511061107110811091110111111121113111411151116111711181119112011211122112311241125112611271128112911301131113211331134113511361137113811391140114111421143114411451146114711481149115011511152115311541155115611571158115911601161116211631164116511661167116811691170117111721173117411751176117711781179118011811182118311841185118611871188118911901191119211931194119511961197119811991200120112021203120412051206120712081209121012111212121312141215121612171218121912201221122212231224122512261227122812291230123112321233123412351236123712381239124012411242124312441245124612471248124912501251125212531254125512561257125812591260126112621263126412651266126712681269127012711272127312741275127612771278127912801281128212831284128512861287128812891290129112921293129412951296129712981299130013011302130313041305130613071308130913101311131213131314131513161317131813191320132113221323132413251326132713281329133013311332133313341335133613371338133913401341134213431344134513461347134813491350135113521353135413551356135713581359136013611362136313641365136613671368136913701371137213731374137513761377# Redis configuration file example.## Note that in order to read the configuration file, Redis must be# started with the file path as first argument:## ./redis-server /path/to/redis.conf# Note on units: when memory size is needed, it is possible to specify# it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.################################## INCLUDES #################################### Include one or more other config files here. This is useful if you# have a standard template that goes to all Redis servers but also need# to customize a few per-server settings. Include files can include# other files, so use this wisely.## Notice option "include" won't be rewritten by command "CONFIG REWRITE"# from admin or Redis Sentinel. Since Redis always uses the last processed# line as value of a configuration directive, you'd better put includes# at the beginning of this file to avoid overwriting config change at runtime.## If instead you are interested in using includes to override configuration# options, it is better to use include as the last line.## include /path/to/local.conf# include /path/to/other.conf################################## MODULES ###################################### Load modules at startup. If the server is not able to load modules# it will abort. It is possible to use multiple loadmodule directives.## loadmodule /path/to/my_module.so# loadmodule /path/to/other_module.so################################## NETWORK ###################################### By default, if no "bind" configuration directive is specified, Redis listens# for connections from all the network interfaces available on the server.# It is possible to listen to just one or multiple selected interfaces using# the "bind" configuration directive, followed by one or more IP addresses.## Examples:## bind 192.168.1.100 10.0.0.1# bind 127.0.0.1 ::1## ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the# internet, binding to all the interfaces is dangerous and will expose the# instance to everybody on the internet. So by default we uncomment the# following bind directive, that will force Redis to listen only into# the IPv4 loopback interface address (this means Redis will be able to# accept connections only from clients running into the same computer it# is running).## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES# JUST COMMENT THE FOLLOWING LINE.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bind 127.0.0.1# Protected mode is a layer of security protection, in order to avoid that# Redis instances left open on the internet are accessed and exploited.## When protected mode is on and if:## 1) The server is not binding explicitly to a set of addresses using the# "bind" directive.# 2) No password is configured.## The server only accepts connections from clients connecting from the# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain# sockets.## By default protected mode is enabled. You should disable it only if# you are sure you want clients from other hosts to connect to Redis# even if no authentication is configured, nor a specific set of interfaces# are explicitly listed using the "bind" directive.protected-mode yes# Accept connections on the specified port, default is 6379 (IANA #815344).# If port 0 is specified Redis will not listen on a TCP socket.port 6379# TCP listen() backlog.## In high requests-per-second environments you need an high backlog in order# to avoid slow clients connections issues. Note that the Linux kernel# will silently truncate it to the value of /proc/sys/net/core/somaxconn so# make sure to raise both the value of somaxconn and tcp_max_syn_backlog# in order to get the desired effect.tcp-backlog 511# Unix socket.## Specify the path for the Unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 700# Close the connection after a client is idle for N seconds (0 to disable)timeout 0# TCP keepalive.## If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence# of communication. This is useful for two reasons:## 1) Detect dead peers.# 2) Take the connection alive from the point of view of network# equipment in the middle.## On Linux, the specified value (in seconds) is the period used to send ACKs.# Note that to close the connection the double of the time is needed.# On other kernels the period depends on the kernel configuration.## A reasonable value for this option is 300 seconds, which is the new# Redis default starting with Redis 3.2.1.tcp-keepalive 300################################# GENERAL ###################################### By default Redis does not run as a daemon. Use 'yes' if you need it.# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.daemonize no# If you run Redis from upstart or systemd, Redis can interact with your# supervision tree. Options:# supervised no - no supervision interaction# supervised upstart - signal upstart by putting Redis into SIGSTOP mode# supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET# supervised auto - detect upstart or systemd method based on# UPSTART_JOB or NOTIFY_SOCKET environment variables# Note: these supervision methods only signal "process is ready."# They do not enable continuous liveness pings back to your supervisor.supervised no# If a pid file is specified, Redis writes it where specified at startup# and removes it at exit.## When the server runs non daemonized, no pid file is created if none is# specified in the configuration. When the server is daemonized, the pid file# is used even if not specified, defaulting to "/var/run/redis.pid".## Creating a pid file is best effort: if Redis is not able to create it# nothing bad happens, the server will start and run normally.pidfile /var/run/redis_6379.pid# Specify the server verbosity level.# This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile ""# To enable logging to the system logger, just set 'syslog-enabled' to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and 'databases'-1databases 16# By default Redis shows an ASCII art logo only when started to log to the# standard output and if the standard output is a TTY. Basically this means# that normally a logo is displayed only in interactive sessions.## However it is possible to force the pre-4.0 behavior and always show a# ASCII art logo in startup logs by setting the following option to yes.always-show-logo yes################################ SNAPSHOTTING ################################## Save the DB on disk:## save &lt;seconds&gt; &lt;changes&gt;## Will save the DB if both the given number of seconds and the given# number of write operations against the DB occurred.## In the example below the behaviour will be to save:# after 900 sec (15 min) if at least 1 key changed# after 300 sec (5 min) if at least 10 keys changed# after 60 sec if at least 10000 keys changed## Note: you can disable saving completely by commenting out all "save" lines.## It is also possible to remove all the previously configured save# points by adding a save directive with a single empty string argument# like in the following example:## save ""save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# For default that's set to 'yes' as it's almost always a win.# If you want to save some CPU in the saving child set it to 'no' but# the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.rdbchecksum yes# The filename where to dump the DBdbfilename dump.rdb# The working directory.## The DB will be written inside this directory, with the filename specified# above using the 'dbfilename' configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir ./################################# REPLICATION ################################## Master-Replica replication. Use replicaof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.## +------------------+ +---------------+# | Master | ---&gt; | Replica |# | (receive writes) | | (exact copy) |# +------------------+ +---------------+## 1) Redis replication is asynchronous, but you can configure a master to# stop accepting writes if it appears to be not connected with at least# a given number of replicas.# 2) Redis replicas are able to perform a partial resynchronization with the# master if the replication link is lost for a relatively small amount of# time. You may want to configure the replication backlog size (see the next# sections of this file) with a sensible value depending on your needs.# 3) Replication is automatic and does not need user intervention. After a# network partition replicas automatically try to reconnect to masters# and resynchronize with them.## replicaof &lt;masterip&gt; &lt;masterport&gt;# If the master is password protected (using the "requirepass" configuration# directive below) it is possible to tell the replica to authenticate before# starting the replication synchronization process, otherwise the master will# refuse the replica request.## masterauth &lt;master-password&gt;# When a replica loses its connection with the master, or when the replication# is still in progress, the replica can act in two different ways:## 1) if replica-serve-stale-data is set to 'yes' (the default) the replica will# still reply to client requests, possibly with out of date data, or the# data set may just be empty if this is the first synchronization.## 2) if replica-serve-stale-data is set to 'no' the replica will reply with# an error "SYNC with master in progress" to all the kind of commands# but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG,# SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB,# COMMAND, POST, HOST: and LATENCY.#replica-serve-stale-data yes# You can configure a replica instance to accept writes or not. Writing against# a replica instance may be useful to store some ephemeral data (because data# written on a replica will be easily deleted after resync with the master) but# may also cause problems if clients are writing to it because of a# misconfiguration.## Since Redis 2.6 by default replicas are read-only.## Note: read only replicas are not designed to be exposed to untrusted clients# on the internet. It's just a protection layer against misuse of the instance.# Still a read only replica exports by default all the administrative commands# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve# security of read only replicas using 'rename-command' to shadow all the# administrative / dangerous commands.replica-read-only yes# Replication SYNC strategy: disk or socket.## -------------------------------------------------------# WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY# -------------------------------------------------------## New replicas and reconnecting replicas that are not able to continue the replication# process just receiving differences, need to do what is called a "full# synchronization". An RDB file is transmitted from the master to the replicas.# The transmission can happen in two different ways:## 1) Disk-backed: The Redis master creates a new process that writes the RDB# file on disk. Later the file is transferred by the parent# process to the replicas incrementally.# 2) Diskless: The Redis master creates a new process that directly writes the# RDB file to replica sockets, without touching the disk at all.## With disk-backed replication, while the RDB file is generated, more replicas# can be queued and served with the RDB file as soon as the current child producing# the RDB file finishes its work. With diskless replication instead once# the transfer starts, new replicas arriving will be queued and a new transfer# will start when the current one terminates.## When diskless replication is used, the master waits a configurable amount of# time (in seconds) before starting the transfer in the hope that multiple replicas# will arrive and the transfer can be parallelized.## With slow disks and fast (large bandwidth) networks, diskless replication# works better.repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay# the server waits in order to spawn the child that transfers the RDB via socket# to the replicas.## This is important since once the transfer starts, it is not possible to serve# new replicas arriving, that will be queued for the next RDB transfer, so the server# waits a delay in order to let more replicas arrive.## The delay is specified in seconds, and by default is 5 seconds. To disable# it entirely just set it to 0 seconds and the transfer will start ASAP.repl-diskless-sync-delay 5# Replicas send PINGs to server in a predefined interval. It's possible to change# this interval with the repl_ping_replica_period option. The default value is 10# seconds.## repl-ping-replica-period 10# The following option sets the replication timeout for:## 1) Bulk transfer I/O during SYNC, from the point of view of replica.# 2) Master timeout from the point of view of replicas (data, pings).# 3) Replica timeout from the point of view of masters (REPLCONF ACK pings).## It is important to make sure that this value is greater than the value# specified for repl-ping-replica-period otherwise a timeout will be detected# every time there is low traffic between the master and the replica.## repl-timeout 60# Disable TCP_NODELAY on the replica socket after SYNC?## If you select "yes" Redis will use a smaller number of TCP packets and# less bandwidth to send data to replicas. But this can add a delay for# the data to appear on the replica side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select "no" the delay for data to appear on the replica side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and replicas are many hops away, turning this to "yes" may# be a good idea.repl-disable-tcp-nodelay no# Set the replication backlog size. The backlog is a buffer that accumulates# replica data when replicas are disconnected for some time, so that when a replica# wants to reconnect again, often a full resync is not needed, but a partial# resync is enough, just passing the portion of data the replica missed while# disconnected.## The bigger the replication backlog, the longer the time the replica can be# disconnected and later be able to perform a partial resynchronization.## The backlog is only allocated once there is at least a replica connected.## repl-backlog-size 1mb# After a master has no longer connected replicas for some time, the backlog# will be freed. The following option configures the amount of seconds that# need to elapse, starting from the time the last replica disconnected, for# the backlog buffer to be freed.## Note that replicas never free the backlog for timeout, since they may be# promoted to masters later, and should be able to correctly "partially# resynchronize" with the replicas: hence they should always accumulate backlog.## A value of 0 means to never release the backlog.## repl-backlog-ttl 3600# The replica priority is an integer number published by Redis in the INFO output.# It is used by Redis Sentinel in order to select a replica to promote into a# master if the master is no longer working correctly.## A replica with a low priority number is considered better for promotion, so# for instance if there are three replicas with priority 10, 100, 25 Sentinel will# pick the one with priority 10, that is the lowest.## However a special priority of 0 marks the replica as not able to perform the# role of master, so a replica with priority of 0 will never be selected by# Redis Sentinel for promotion.## By default the priority is 100.replica-priority 100# It is possible for a master to stop accepting writes if there are less than# N replicas connected, having a lag less or equal than M seconds.## The N replicas need to be in "online" state.## The lag in seconds, that must be &lt;= the specified value, is calculated from# the last ping received from the replica, that is usually sent every second.## This option does not GUARANTEE that N replicas will accept the write, but# will limit the window of exposure for lost writes in case not enough replicas# are available, to the specified number of seconds.## For example to require at least 3 replicas with a lag &lt;= 10 seconds use:## min-replicas-to-write 3# min-replicas-max-lag 10## Setting one or the other to 0 disables the feature.## By default min-replicas-to-write is set to 0 (feature disabled) and# min-replicas-max-lag is set to 10.# A Redis master is able to list the address and port of the attached# replicas in different ways. For example the "INFO replication" section# offers this information, which is used, among other tools, by# Redis Sentinel in order to discover replica instances.# Another place where this info is available is in the output of the# "ROLE" command of a master.## The listed IP and address normally reported by a replica is obtained# in the following way:## IP: The address is auto detected by checking the peer address# of the socket used by the replica to connect with the master.## Port: The port is communicated by the replica during the replication# handshake, and is normally the port that the replica is using to# listen for connections.## However when port forwarding or Network Address Translation (NAT) is# used, the replica may be actually reachable via different IP and port# pairs. The following two options can be used by a replica in order to# report to its master a specific set of IP and port, so that both INFO# and ROLE will report those values.## There is no need to use both the options if you need to override just# the port or the IP address.## replica-announce-ip 5.5.5.5# replica-announce-port 1234################################## SECURITY #################################### Require clients to issue AUTH &lt;PASSWORD&gt; before processing any other# commands. This might be useful in environments in which you do not trust# others with access to the host running redis-server.## This should stay commented out for backward compatibility and because most# people do not need auth (e.g. they run their own servers).## Warning: since Redis is pretty fast an outside user can try up to# 150k passwords per second against a good box. This means that you should# use a very strong password otherwise it will be very easy to break.## requirepass foobared# Command renaming.## It is possible to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# hard to guess so that it will still be available for internal-use tools# but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possible to completely kill a command by renaming it into# an empty string:## rename-command CONFIG ""## Please note that changing the name of commands that are logged into the# AOF file or transmitted to replicas may cause problems.################################### CLIENTS ##################################### Set the max number of connected clients at the same time. By default# this limit is set to 10000 clients, however if the Redis server is not# able to configure the process file limit to allow for the specified limit# the max number of allowed clients is set to the current file limit# minus 32 (as Redis reserves a few file descriptors for internal uses).## Once the limit is reached Redis will close all the new connections sending# an error 'max number of clients reached'.## maxclients 10000############################## MEMORY MANAGEMENT ################################# Set a memory usage limit to the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys# according to the eviction policy selected (see maxmemory-policy).## If Redis can't remove keys according to the policy, or if the policy is# set to 'noeviction', Redis will start to reply with errors to commands# that would use more memory, like SET, LPUSH, and so on, and will continue# to reply to read-only commands like GET.## This option is usually useful when using Redis as an LRU or LFU cache, or to# set a hard memory limit for an instance (using the 'noeviction' policy).## WARNING: If you have replicas attached to an instance with maxmemory on,# the size of the output buffers needed to feed the replicas are subtracted# from the used memory count, so that network problems / resyncs will# not trigger a loop where keys are evicted, and in turn the output# buffer of replicas is full with DELs of keys evicted triggering the deletion# of more keys, and so forth until the database is completely emptied.## In short... if you have replicas attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for replica# output buffers (but this is not needed if the policy is 'noeviction').## maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached. You can select among five behaviors:## volatile-lru -&gt; Evict using approximated LRU among the keys with an expire set.# allkeys-lru -&gt; Evict any key using approximated LRU.# volatile-lfu -&gt; Evict using approximated LFU among the keys with an expire set.# allkeys-lfu -&gt; Evict any key using approximated LFU.# volatile-random -&gt; Remove a random key among the ones with an expire set.# allkeys-random -&gt; Remove a random key, any key.# volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)# noeviction -&gt; Don't evict anything, just return an error on write operations.## LRU means Least Recently Used# LFU means Least Frequently Used## Both LRU, LFU and volatile-ttl are implemented using approximated# randomized algorithms.## Note: with any of the above policies, Redis will return an error on write# operations, when there are no suitable keys for eviction.## At the date of writing these commands are: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## The default is:## maxmemory-policy noeviction# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can tune it for speed or# accuracy. For default Redis will check five keys and pick the one that was# used less recently, you can change the sample size using the following# configuration directive.## The default of 5 produces good enough results. 10 Approximates very closely# true LRU but costs more CPU. 3 is faster but not very accurate.## maxmemory-samples 5# Starting from Redis 5, by default a replica will ignore its maxmemory setting# (unless it is promoted to master after a failover or manually). It means# that the eviction of keys will be just handled by the master, sending the# DEL commands to the replica as keys evict in the master side.## This behavior ensures that masters and replicas stay consistent, and is usually# what you want, however if your replica is writable, or you want the replica to have# a different memory setting, and you are sure all the writes performed to the# replica are idempotent, then you may change this default (but be sure to understand# what you are doing).## Note that since the replica by default does not evict, it may end using more# memory than the one set via maxmemory (there are certain buffers that may# be larger on the replica, or data structures may sometimes take more memory and so# forth). So make sure you monitor your replicas and make sure they have enough# memory to never hit a real out-of-memory condition before the master hits# the configured maxmemory setting.## replica-ignore-maxmemory yes############################# LAZY FREEING ##################################### Redis has two primitives to delete keys. One is called DEL and is a blocking# deletion of the object. It means that the server stops processing new commands# in order to reclaim all the memory associated with an object in a synchronous# way. If the key deleted is associated with a small object, the time needed# in order to execute the DEL command is very small and comparable to most other# O(1) or O(log_N) commands in Redis. However if the key is associated with an# aggregated value containing millions of elements, the server can block for# a long time (even seconds) in order to complete the operation.## For the above reasons Redis also offers non blocking deletion primitives# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and# FLUSHDB commands, in order to reclaim memory in background. Those commands# are executed in constant time. Another thread will incrementally free the# object in the background as fast as possible.## DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.# It's up to the design of the application to understand when it is a good# idea to use one or the other. However the Redis server sometimes has to# delete keys or flush the whole database as a side effect of other operations.# Specifically Redis deletes objects independently of a user call in the# following scenarios:## 1) On eviction, because of the maxmemory and maxmemory policy configurations,# in order to make room for new data, without going over the specified# memory limit.# 2) Because of expire: when a key with an associated time to live (see the# EXPIRE command) must be deleted from memory.# 3) Because of a side effect of a command that stores data on a key that may# already exist. For example the RENAME command may delete the old key# content when it is replaced with another one. Similarly SUNIONSTORE# or SORT with STORE option may delete existing keys. The SET command# itself removes any old content of the specified key in order to replace# it with the specified string.# 4) During replication, when a replica performs a full resynchronization with# its master, the content of the whole database is removed in order to# load the RDB file just transferred.## In all the above cases the default is to delete objects in a blocking way,# like if DEL was called. However you can configure each case specifically# in order to instead release memory in a non-blocking way like if UNLINK# was called, using the following configuration directives:lazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noreplica-lazy-flush no############################## APPEND ONLY MODE ################################ By default Redis asynchronously dumps the dataset on disk. This mode is# good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on# the configured save points).## The Append Only File is an alternative persistence mode that provides# much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a# dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is# still running correctly.## AOF and RDB persistence can be enabled at the same time without problems.# If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.## Please check http://redis.io/topics/persistence for more information.appendonly no# The name of the append only file (default: "appendonly.aof")appendfilename "appendonly.aof"# The fsync() call tells the Operating System to actually write data on disk# instead of waiting for more data in the output buffer. Some OS will really flush# data on disk, some other OS will just try to do it ASAP.## Redis supports three different modes:## no: don't fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.## The default is "everysec", as that's usually the right compromise between# speed and data safety. It's up to you to understand if you can relax this to# "no" that will let the operating system flush the output buffer when# it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that's snapshotting),# or on the contrary, use "always" that's very slow but a bit safer than# everysec.## More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use "everysec".# appendfsync alwaysappendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it's possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving, the durability of Redis is# the same as "appendfsync none". In practical terms, this means that it is# possible to lose up to 30 seconds of log in the worst scenario (with the# default Linux settings).## If you have latency problems turn this to "yes". Otherwise leave it as# "no" that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size grows by the specified percentage.## This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a percentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# An AOF file may be found to be truncated at the end during the Redis# startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running# crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can't happen when Redis itself# crashes or aborts but the operating system still works correctly).## Redis can either exit with an error when this happens, or load as much# data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.## If aof-load-truncated is set to yes, a truncated AOF file is loaded and# the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error# and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the "redis-check-aof" utility before to restart# the server.## Note that if the AOF file will be found to be corrupted in the middle# the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes# will be found.aof-load-truncated yes# When rewriting the AOF file, Redis is able to use an RDB preamble in the# AOF file for faster rewrites and recoveries. When this option is turned# on the rewritten AOF file is composed of two different stanzas:## [RDB file][AOF tail]## When loading Redis recognizes that the AOF file starts with the "REDIS"# string and loads the prefixed RDB file, and continues loading the AOF# tail.aof-use-rdb-preamble yes################################ LUA SCRIPTING ################################ Max execution time of a Lua script in milliseconds.## If the maximum execution time is reached Redis will log that a script is# still in execution after the maximum allowed time and will start to# reply to queries with an error.## When a long running script exceeds the maximum execution time only the# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be# used to stop a script that did not yet called write commands. The second# is the only way to shut down the server in the case a write command was# already issued by the script but the user doesn't want to wait for the natural# termination of the script.## Set it to 0 or a negative value for unlimited execution without warnings.lua-time-limit 5000################################ REDIS CLUSTER ################################# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++# WARNING EXPERIMENTAL: Redis Cluster is considered to be stable code, however# in order to mark it as "mature" we need to wait for a non trivial percentage# of users to deploy it in production.# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++## Normal Redis instances can't be part of a Redis Cluster; only nodes that are# started as cluster nodes can. In order to start a Redis instance as a# cluster node enable the cluster support uncommenting the following:## cluster-enabled yes# Every cluster node has a cluster configuration file. This file is not# intended to be edited by hand. It is created and updated by Redis nodes.# Every Redis Cluster node requires a different cluster configuration file.# Make sure that instances running in the same system do not have# overlapping cluster configuration file names.## cluster-config-file nodes-6379.conf# Cluster node timeout is the amount of milliseconds a node must be unreachable# for it to be considered in failure state.# Most other internal time limits are multiple of the node timeout.## cluster-node-timeout 15000# A replica of a failing master will avoid to start a failover if its data# looks too old.## There is no simple way for a replica to actually have an exact measure of# its "data age", so the following two checks are performed:## 1) If there are multiple replicas able to failover, they exchange messages# in order to try to give an advantage to the replica with the best# replication offset (more data from the master processed).# Replicas will try to get their rank by offset, and apply to the start# of the failover a delay proportional to their rank.## 2) Every single replica computes the time of the last interaction with# its master. This can be the last ping or command received (if the master# is still in the "connected" state), or the time that elapsed since the# disconnection with the master (if the replication link is currently down).# If the last interaction is too old, the replica will not try to failover# at all.## The point "2" can be tuned by user. Specifically a replica will not perform# the failover if, since the last interaction with the master, the time# elapsed is greater than:## (node-timeout * replica-validity-factor) + repl-ping-replica-period## So for example if node-timeout is 30 seconds, and the replica-validity-factor# is 10, and assuming a default repl-ping-replica-period of 10 seconds, the# replica will not try to failover if it was not able to talk with the master# for longer than 310 seconds.## A large replica-validity-factor may allow replicas with too old data to failover# a master, while a too small value may prevent the cluster from being able to# elect a replica at all.## For maximum availability, it is possible to set the replica-validity-factor# to a value of 0, which means, that replicas will always try to failover the# master regardless of the last time they interacted with the master.# (However they'll always try to apply a delay proportional to their# offset rank).## Zero is the only value able to guarantee that when all the partitions heal# the cluster will always be able to continue.## cluster-replica-validity-factor 10# Cluster replicas are able to migrate to orphaned masters, that are masters# that are left without working replicas. This improves the cluster ability# to resist to failures as otherwise an orphaned master can't be failed over# in case of failure if it has no working replicas.## Replicas migrate to orphaned masters only if there are still at least a# given number of other working replicas for their old master. This number# is the "migration barrier". A migration barrier of 1 means that a replica# will migrate only if there is at least 1 other working replica for its master# and so forth. It usually reflects the number of replicas you want for every# master in your cluster.## Default is 1 (replicas migrate only if their masters remain with at least# one replica). To disable migration just set it to a very large value.# A value of 0 can be set but is useful only for debugging and dangerous# in production.## cluster-migration-barrier 1# By default Redis Cluster nodes stop accepting queries if they detect there# is at least an hash slot uncovered (no available node is serving it).# This way if the cluster is partially down (for example a range of hash slots# are no longer covered) all the cluster becomes, eventually, unavailable.# It automatically returns available as soon as all the slots are covered again.## However sometimes you want the subset of the cluster which is working,# to continue to accept queries for the part of the key space that is still# covered. In order to do so, just set the cluster-require-full-coverage# option to no.## cluster-require-full-coverage yes# This option, when set to yes, prevents replicas from trying to failover its# master during master failures. However the master can still perform a# manual failover, if forced to do so.## This is useful in different scenarios, especially in the case of multiple# data center operations, where we want one side to never be promoted if not# in the case of a total DC failure.## cluster-replica-no-failover no# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.########################## CLUSTER DOCKER/NAT support ######################### In certain deployments, Redis Cluster nodes address discovery fails, because# addresses are NAT-ted or because ports are forwarded (the typical case is# Docker and other containers).## In order to make Redis Cluster working in such environments, a static# configuration where each node knows its public address is needed. The# following two options are used for this scope, and are:## * cluster-announce-ip# * cluster-announce-port# * cluster-announce-bus-port## Each instruct the node about its address, client port, and cluster message# bus port. The information is then published in the header of the bus packets# so that other nodes will be able to correctly map the address of the node# publishing the information.## If the above options are not used, the normal Redis Cluster auto-detection# will be used instead.## Note that when remapped, the bus port may not be at the fixed offset of# clients port + 10000, so you can specify any port and bus-port depending# on how they get remapped. If the bus-port is not set, a fixed offset of# 10000 will be used as usually.## Example:## cluster-announce-ip 10.1.1.5# cluster-announce-port 6379# cluster-announce-bus-port 6380################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).## You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 128################################ LATENCY MONITOR ############################### The Redis latency monitoring subsystem samples different operations# at runtime in order to collect data related to possible sources of# latency of a Redis instance.## Via the LATENCY command this information is available to the user that can# print graphs and obtain reports.## The system only logs operations that were performed in a time equal or# greater than the amount of milliseconds specified via the# latency-monitor-threshold configuration directive. When its value is set# to zero, the latency monitor is turned off.## By default latency monitoring is disabled since it is mostly not needed# if you don't have latency issues, and collecting data has a performance# impact, that while very small, can be measured under big load. Latency# monitoring can easily be enabled at runtime using the command# "CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;" if needed.latency-monitor-threshold 0############################# EVENT NOTIFICATION ############################### Redis can notify Pub/Sub clients about events happening in the key space.# This feature is documented at http://redis.io/topics/notifications## For instance if keyspace events notification is enabled, and a client# performs a DEL operation on key "foo" stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo## It is possible to select the events that Redis will notify among a set# of classes. Every class is identified by a single character:## K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.# E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.# g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...# $ String commands# l List commands# s Set commands# h Hash commands# z Sorted set commands# x Expired events (events generated every time a key expires)# e Evicted events (events generated when a key is evicted for maxmemory)# A Alias for g$lshzxe, so that the "AKE" string means all the events.## The "notify-keyspace-events" takes as argument a string that is composed# of zero or multiple characters. The empty string means that notifications# are disabled.## Example: to enable list and generic events, from the point of view of the# event name, use:## notify-keyspace-events Elg## Example 2: to get the stream of the expired keys subscribing to channel# name __keyevent@0__:expired use:## notify-keyspace-events Ex## By default all notifications are disabled because most users don't need# this feature and the feature has some overhead. Note that if you don't# specify at least one of K or E, no events will be delivered.notify-keyspace-events ""############################### ADVANCED CONFIG ################################ Hashes are encoded using a memory efficient data structure when they have a# small number of entries, and the biggest entry does not exceed a given# threshold. These thresholds can be configured using the following directives.hash-max-ziplist-entries 512hash-max-ziplist-value 64# Lists are also encoded in a special way to save a lot of space.# The number of entries allowed per internal list node can be specified# as a fixed maximum size or a maximum number of elements.# For a fixed maximum size, use -5 through -1, meaning:# -5: max size: 64 Kb &lt;-- not recommended for normal workloads# -4: max size: 32 Kb &lt;-- not recommended# -3: max size: 16 Kb &lt;-- probably not recommended# -2: max size: 8 Kb &lt;-- good# -1: max size: 4 Kb &lt;-- good# Positive numbers mean store up to _exactly_ that number of elements# per list node.# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),# but if your use case is unique, adjust the settings as necessary.list-max-ziplist-size -2# Lists may also be compressed.# Compress depth is the number of quicklist ziplist nodes from *each* side of# the list to *exclude* from compression. The head and tail of the list# are always uncompressed for fast push/pop operations. Settings are:# 0: disable all list compression# 1: depth 1 means "don't start compressing until after 1 node into the list,# going from either the head or tail"# So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]# [head], [tail] will always be uncompressed; inner nodes will compress.# 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]# 2 here means: don't compress head or head-&gt;next or tail-&gt;prev or tail,# but compress all nodes between them.# 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]# etc.list-compress-depth 0# Sets have a special encoding in just one case: when a set is composed# of just strings that happen to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog sparse representation bytes limit. The limit includes the# 16 bytes header. When an HyperLogLog using the sparse representation crosses# this limit, it is converted into the dense representation.## A value greater than 16000 is totally useless, since at that point the# dense representation is more memory efficient.## The suggested value is ~ 3000 in order to have the benefits of# the space efficient encoding without slowing down too much PFADD,# which is O(N) with the sparse encoding. The value can be raised to# ~ 10000 when CPU is not a concern, but space is, and the data set is# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.hll-sparse-max-bytes 3000# Streams macro node max size / items. The stream data structure is a radix# tree of big nodes that encode multiple items inside. Using this configuration# it is possible to configure how big a single node can be in bytes, and the# maximum number of items it may contain before switching to a new node when# appending new stream entries. If any of the following settings are set to# zero, the limit is ignored, so for instance it is possible to set just a# max entires limit by setting max-bytes to 0 and max-entries to the desired# value.stream-node-max-bytes 4096stream-node-max-entries 100# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation Redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into a hash table# that is rehashing, the more rehashing "steps" are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.## The default is to use this millisecond 10 times every second in order to# actively rehash the main dictionaries, freeing memory when possible.## If unsure:# use "activerehashing no" if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply from time to time# to queries with 2 milliseconds delay.## use "activerehashing yes" if you don't have such hard requirements but# want to free memory asap when possible.activerehashing yes# The client output buffer limits can be used to force disconnection of clients# that are not reading data from the server fast enough for some reason (a# common reason is that a Pub/Sub client can't consume messages as fast as the# publisher can produce them).## The limit can be set differently for the three different classes of clients:## normal -&gt; normal clients including MONITOR clients# replica -&gt; replica clients# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern## The syntax of every client-output-buffer-limit directive is the following:## client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;## A client is immediately disconnected once the hard limit is reached, or if# the soft limit is reached and remains reached for the specified number of# seconds (continuously).# So for instance if the hard limit is 32 megabytes and the soft limit is# 16 megabytes / 10 seconds, the client will get disconnected immediately# if the size of the output buffers reach 32 megabytes, but will also get# disconnected if the client reaches 16 megabytes and continuously overcomes# the limit for 10 seconds.## By default normal clients are not limited because they don't receive data# without asking (in a push way), but just after a request, so only# asynchronous clients may create a scenario where data is requested faster# than it can read.## Instead there is a default limit for pubsub and replica clients, since# subscribers and replicas receive data in a push fashion.## Both the hard or the soft limit can be disabled by setting them to zero.client-output-buffer-limit normal 0 0 0client-output-buffer-limit replica 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# Client query buffers accumulate new commands. They are limited to a fixed# amount by default in order to avoid that a protocol desynchronization (for# instance due to a bug in the client) will lead to unbound memory usage in# the query buffer. However you can configure it here if you have very special# needs, such us huge multi/exec requests or alike.## client-query-buffer-limit 1gb# In the Redis protocol, bulk requests, that are, elements representing single# strings, are normally limited ot 512 mb. However you can change this limit# here.## proto-max-bulk-len 512mb# Redis calls an internal function to perform many background tasks, like# closing connections of clients in timeout, purging expired keys that are# never requested, and so forth.## Not all tasks are performed with the same frequency, but Redis checks for# tasks to perform according to the specified "hz" value.## By default "hz" is set to 10. Raising the value will use more CPU when# Redis is idle, but at the same time will make Redis more responsive when# there are many keys expiring at the same time, and timeouts may be# handled with more precision.## The range is between 1 and 500, however a value over 100 is usually not# a good idea. Most users should use the default of 10 and raise this up to# 100 only in environments where very low latency is required.hz 10# Normally it is useful to have an HZ value which is proportional to the# number of clients connected. This is useful in order, for instance, to# avoid too many clients are processed for each background task invocation# in order to avoid latency spikes.## Since the default HZ value by default is conservatively set to 10, Redis# offers, and enables by default, the ability to use an adaptive HZ value# which will temporary raise when there are many connected clients.## When dynamic HZ is enabled, the actual configured HZ will be used as# as a baseline, but multiples of the configured HZ value will be actually# used as needed once more clients are connected. In this way an idle# instance will use very little CPU time while a busy instance will be# more responsive.dynamic-hz yes# When a child rewrites the AOF file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.aof-rewrite-incremental-fsync yes# When redis saves RDB file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.rdb-save-incremental-fsync yes# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good# idea to start with the default settings and only change them after investigating# how to improve the performances and how the keys LFU change over time, which# is possible to inspect via the OBJECT FREQ command.## There are two tunable parameters in the Redis LFU implementation: the# counter logarithm factor and the counter decay time. It is important to# understand what the two parameters mean before changing them.## The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis# uses a probabilistic increment with logarithmic behavior. Given the value# of the old counter, when a key is accessed, the counter is incremented in# this way:## 1. A random number R between 0 and 1 is extracted.# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).# 3. The counter is incremented only if R &lt; P.## The default lfu-log-factor is 10. This is a table of how the frequency# counter changes with a different number of accesses with different# logarithmic factors:## +--------+------------+------------+------------+------------+------------+# | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits |# +--------+------------+------------+------------+------------+------------+# | 0 | 104 | 255 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 1 | 18 | 49 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 10 | 10 | 18 | 142 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 100 | 8 | 11 | 49 | 143 | 255 |# +--------+------------+------------+------------+------------+------------+## NOTE: The above table was obtained by running the following commands:## redis-benchmark -n 1000000 incr foo# redis-cli object freq foo## NOTE 2: The counter initial value is 5 in order to give new objects a chance# to accumulate hits.## The counter decay time is the time, in minutes, that must elapse in order# for the key counter to be divided by two (or decremented if it has a value# less &lt;= 10).## The default value for the lfu-decay-time is 1. A Special value of 0 means to# decay the counter every time it happens to be scanned.## lfu-log-factor 10# lfu-decay-time 1########################### ACTIVE DEFRAGMENTATION ######################### WARNING THIS FEATURE IS EXPERIMENTAL. However it was stress tested# even in production and manually tested by multiple engineers for some# time.## What is active defragmentation?# -------------------------------## Active (online) defragmentation allows a Redis server to compact the# spaces left between small allocations and deallocations of data in memory,# thus allowing to reclaim back memory.## Fragmentation is a natural process that happens with every allocator (but# less so with Jemalloc, fortunately) and certain workloads. Normally a server# restart is needed in order to lower the fragmentation, or at least to flush# away all the data and create it again. However thanks to this feature# implemented by Oran Agra for Redis 4.0 this process can happen at runtime# in an "hot" way, while the server is running.## Basically when the fragmentation is over a certain level (see the# configuration options below) Redis will start to create new copies of the# values in contiguous memory regions by exploiting certain specific Jemalloc# features (in order to understand if an allocation is causing fragmentation# and to allocate it in a better place), and at the same time, will release the# old copies of the data. This process, repeated incrementally for all the keys# will cause the fragmentation to drop back to normal values.## Important things to understand:## 1. This feature is disabled by default, and only works if you compiled Redis# to use the copy of Jemalloc we ship with the source code of Redis.# This is the default with Linux builds.## 2. You never need to enable this feature if you don't have fragmentation# issues.## 3. Once you experience fragmentation, you can enable this feature when# needed with the command "CONFIG SET activedefrag yes".## The configuration parameters are able to fine tune the behavior of the# defragmentation process. If you are not sure about what they mean it is# a good idea to leave the defaults untouched.# Enabled active defragmentation# activedefrag yes# Minimum amount of fragmentation waste to start active defrag# active-defrag-ignore-bytes 100mb# Minimum percentage of fragmentation to start active defrag# active-defrag-threshold-lower 10# Maximum percentage of fragmentation at which we use maximum effort# active-defrag-threshold-upper 100# Minimal effort for defrag in CPU percentage# active-defrag-cycle-min 5# Maximal effort for defrag in CPU percentage# active-defrag-cycle-max 75# Maximum number of set/hash/zset/list fields that will be processed from# the main dictionary scan# active-defrag-max-scan-fields 1000]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—事务]]></title>
    <url>%2F2019%2F03%2F19%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[事务简单理解就是一组命令要么执行，要么全部不执行，否则执行一部分数据会造成数据不一致的问题 Redis提供了简单的事务功能，不支持回滚操作，将一组需要批量执行的命令放到multi和exec两个命令之间就可以实现简单地事务功能。 multi命令代表事务开始 exec命令代表事务结束 discard命令代表中途中断 下面添加一个名字为Charlie的成绩信息，要求一气呵成的添加，如果没有输入exec之前，数据的返回值是“QUEUED”排队的状态，输入exec之后数据就被写入到Redis里面了（如果中途想要中断就输入discard） 123456789101112127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set Charlie:english:score 88QUEUED127.0.0.1:6379&gt; set Charlie:math:score 75QUEUED127.0.0.1:6379&gt; set Charlie:totalscore 163QUEUED127.0.0.1:6379&gt; exec1) OK2) OK3) OK 如果整个执行过程中命令输错，比如set写成了sett写错了，这样整个事务就无法执行了 如果整个执行过程中输错命令，把set写成功了zset，因为zset原来就存在这个命令所以不会报错，需要开发人员自行处理 watch 有些应用场景需要在事务之前，确保事务中的key没有被其他客户端修改过，才执行事务，否则不执行（类似乐观锁）。Redis提供了watch命令来解决这类问题 客户端1中使用watch命令标记key为java的键 12127.0.0.1:6379&gt; watch javaOK 客户端2操作设置了key为java的值 12127.0.0.1:6379&gt; set java jedisOK 客户端1进行事务操作，会发现exec之后返回（nil）没有执行成功，watch起到了效果 123456127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set java JedisQUEUED127.0.0.1:6379&gt; exec(nil)]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—Pipeline]]></title>
    <url>%2F2019%2F03%2F19%2Fredis-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-pipeline%2F</url>
    <content type="text"><![CDATA[Redis客户端执行一条命令分为如下四个过程 发送命令 命令排队 执行命令 返回结果 无论网络延如何延时，数据包总是能从客户端到达服务器，并从服务器返回数据回复客户端，这样的从1~4所需要的时间称为Round Trip Time（RTT，往返时间） Redis提供了批量操作命令（例如mget、mset等），有效地节约RTT，但是大部分命令不支持批量操作，这样每次去请求在网络上消耗时间是非常不明智的，Pipeline（流水线）机制能改善上面这类问题，它能将一组Redis命令进行组装，通过一次RTT传输给Redis，再将这组Redis命令的执行结果按顺序返回给客户端 下图是不使用Pipeline的情况，请求多少次就会有多少RTT，会在网络传输上耗费大量的时间 Pipeline执行速度一般比逐条执行要快；客户端和服务端的网络延时越大，Pipeline的效果越明显，下图是使用pipeline执行的情况 可以使用Pipeline模拟出批量操作的效果，但是在使用时要注意它与原生批量命令的区别，具体包含以下几点： 原生批量命令是原子的，Pipeline是非原子的 原生批量命令是一个命令对应多个key，Pipeline支持多个命令 原生批量命令是Redis服务端支持实现的，而Pipeline需要服务端和客户端的共同实现 Pipeline虽然好用，但是每次Pipeline组装的命令个数不能没有节制，否则一次组装Pipeline数据量过大，一方面会增加客户端的等待时间，另一方面会造成一定的网络阻塞，可以将一次包含大量命令的Pipeline拆分成多次较小的Pipeline来完成。Pipeline只能操作一个Redis实例，但是即使在分布式Redis场景中，也可以作为批量操作的重要优化手段]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—redis-benchmark详解]]></title>
    <url>%2F2019%2F03%2F19%2Fredis-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-redis-benchmark-e8-af-a6-e8-a7-a3%2F</url>
    <content type="text"><![CDATA[redis-benchmark可以为Redis做基准性能测试，它提供了很多选项帮助开发和运维人员测试Redis的相关性能 -c -c（clients）选项代表客户端的并发量（默认50） -n -n（num）选项代表客户端请求数量（默认100000） 例如测试100个客户端一共请求20000次 [root@vmzq1l0l ~]# redis-benchmark -c 100 -n 20000 redis-benchmark会对各类数据结构的命令进行测试，并给出性能指标 ====== MSET (10 keys) ====== 20000 requests completed in 0.28 seconds 100 parallel clients 3 bytes payload keep alive: 1 51.86% &lt;= 1 milliseconds93.78% &lt;= 2 milliseconds97.22% &lt;= 3 milliseconds98.67% &lt;= 4 milliseconds99.65% &lt;= 5 milliseconds99.87% &lt;= 7 milliseconds99.92% &lt;= 8 milliseconds100.00% &lt;= 8 milliseconds70921.98 requests per second 上面一共执行了20000次get操作，在0.28秒完成，每个请求数据量是3个字节，51.89%的命令执行时间小于1毫秒，···，99.92%的命令执行时间小于8毫秒，Redis每秒可以处理70921.98次get请求 -q -q选项仅仅显示redis-benchmark的requests per second信息 -r 在一个空的Redis上执行了redis-benchmark会发现只有3个键 127.0.0.1:6379&gt; dbsize(integer) 3127.0.0.1:6379&gt; keys *1) “counter:__rand_int__”2) “mylist”3) “key:__rand_int__” 如果想向Redis插入更多的键，可以执行使用-r（random）选项，可以向Redis插入更多随机的键 [root@vmzq1l0l ~]# redis-benchmark -c 100 -n 20000 -r 10000 -r选项会在key、counter键上加一个12位的后缀，-r10000代表只对后四位做随机处理（-r不是随机数的个数）。例如上面操作后，key的数量和结果结构如下 127.0.0.1:6379&gt; dbsize(integer) 18641127.0.0.1:6379&gt; scan 01) “14336”2) 1) “key:000000004580”2) “key:000000004519”…10) “key:000000002113” -P -P选项代表每个请求pipeline的数据量（默认为1） -k -k选项代表客户端是否使用keepalive，1为使用，0为不使用，默认值为1 -t -t选项可以对指定命令进行基准测试 [root@vmzq1l0l ~]# redis-benchmark -t get,set -qSET: 59453.03 requests per secondGET: 60901.34 requests per second –csv –csv选项会将结果按照csv格式输出，便于后续处理，如导出到Excel等 [root@vmzq1l0l ~]# redis-benchmark -t get,set -q –csv“SET”,”63532.40”“GET”,”81967.21”]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—redis-server详解]]></title>
    <url>%2F2019%2F03%2F19%2Fredis-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-redis-server-e8-af-a6-e8-a7-a3%2F</url>
    <content type="text"><![CDATA[redis-server除了启动Redis外，还有一个–test-memory选项。redis-server –test-memory可以用来检测当前操作系统能否稳定地分配指定容量的内存给Redis，通过这种检测可以有效避免因为内存问题造成Redis崩溃，例如下面操作检测当前操作系统能否提供1G的内存给Redis： [root@vmzq1l0l ~]# redis-server –test-memory 1024 整个内存检测的时间比较长。当输出passed this test时说明内存检测完毕，最后会提示–test-memory只是简单检测，如果有质疑可以使用更加专业的内存检测工具： ...======================================================================================================================================================================………………………………………………………………………………………..Please keep the test running several minutes per GB of memory.Also check http://www.memtest86.com/ and http://pyropus.ca/software/memtester/ Your memory passed this test.Please if you are still in doubt use the following two tools:1) memtest86: http://www.memtest86.com/2) memtester: http://pyropus.ca/software/memtester/ 通常无需每次开启Redis实例时都执行–test-memory选项，该功能更偏向于调试和测试，例如，想快速占满机器内存做一些极端条件的测试，这个功能是一个不错的选择。]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—redis-cli详解]]></title>
    <url>%2F2019%2F03%2F19%2Fredis-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-redis-cli-e8-af-a6-e8-a7-a3%2F</url>
    <content type="text"><![CDATA[输入 redis-cli 连接到命令行客户端（默认host=127.0.0.1，port=6379） [root@vmzq1l0l ~]# redis-cli127.0.0.1:6379&gt; -h -h（host：主机）选项代表主机IP，不输入默认为127.0.0.1 [root@vmzq1l0l ~]# redis-cli -h 127.0.0.1127.0.0.1:6379&gt; -p -p（port：端口）选项代表端口号，不输入默认为6379 [root@vmzq1l0l ~]# redis-cli -p 6379127.0.0.1:6379&gt; ** -r** -r（repeat：重复）选项表示将命令执行多次，例如执行三次ping操作 [root@vmzq1l0l ~]# redis-cli -r 3 pingPONGPONGPONG -i -i（interval：间隔）选项表示间隔多少秒执行一次命令，但是 -i 必须要和 -r 一起使用，例如每隔1秒执行一次ping命令，总共执行3次 [root@vmzq1l0l ~]# redis-cli -r 3 -i 1 pingPONGPONGPONG -x -x选项表示从标准输入（stdin）读取数据作为redis-cli的最后一个参数，例如下面的操作会将字符串”It’s a dog”作为键Goofy的值（echo的-e表示开启转义，\c表示不换行） [root@vmzq1l0l ~]# echo -e “It’s a dog\c” | redis-cli -x set GoofyOK[root@vmzq1l0l ~]# redis-cli127.0.0.1:6379&gt; get Goofy“It’s a dog” ** -c** -c（cluster：集群）选项是连接Redis Cluster节点时需要使用的，-c选项可以防止moved和ask异常 -a -a（authority：权限）如果Redis配置了密码，可以用-a选项，有了这个选项就不需要手动输入auth命令。 ** –scan和–pattern** –scan选项和–pattern选项用于扫描指定模式的键，相当于使用scan命令 –slave –slave项是把当前客户端模拟成当前Redis节点的从节点，可以用来获取当前Redis节点的更新操作 第一个客户端开启slave [root@vmzq1l0l ~]# redis-cli –slaveSYNC with master, discarding 884 bytes of bulk transfer…SYNC done. Logging commands from master.“PING”“PING”“PING”“PING” 第二个客户端做set字符串操作 [root@vmzq1l0l ~]# redis-cli127.0.0.1:6379&gt; set hello worldOK 此时第一个客户端 [root@vmzq1l0l ~]# redis-cli –slaveSYNC with master, discarding 884 bytes of bulk transfer…SYNC done. Logging commands from master.“PING”“PING”“PING”“PING”“SELECT”,”0”“set”,”hello”,”world” –rdb –rdb选项会请求Redis实例生成并发送RDB持久化文件，保存在本地。可使用它做持久化文件的定期备份 –pipe –pipe选项用于将命令封装成Redis通信协议定义的数据格式，批量发送给Redis执行；例如下面操作同时执行了set hello world和incr counter两条命令 $ echo -en &apos;*3\r\n$3\r\nSET\r\n$5\r\nhello\r\n$5\r\nworld\r\n*2\r\n$4\r\nincr\r\n$7\r\ncounter\r\n&apos; | redis-cli --pipe–bigkeys –bigkeys选项使用scan命令对Redis的键进行采样，从中找到内存占用比较大的键值，这些键可能是系统的瓶颈 –eval –eval选项用于执行指定Lua脚本 –latency latency有三个选项，分别是–latency、–latency-history、–latency-dist。它们都可以检测网络延迟，对于Redis的开发和运维非常有帮助 –stat –stat（statistics：统计）选项可以实时获取Redis的重要统计信息，虽然info命令中的统计信息更全，但是能实时看到一些增量的数据（例如requests）对于Redis的运维还是有一定帮助的 [root@vmzq1l0l ~]# redis-cli –stat------- data —— ——————— load ——————– - child -keys mem clients blocked requests connections27 2.06M 12 0 7696 (+0) 6627 2.06M 12 0 7697 (+1) 6627 2.06M 12 0 7698 (+1) 66... –raw和–no-raw –no-raw选项是要求命令的返回结果必须是原始的格式，–raw恰恰相反，返回格式化后的结果，设置一个键为hello的value为中文“你好”，正常get会返回一个二进制格式的结果，加了–raw就会返回正常的中文字符 [root@vmzq1l0l ~]# redis-cli set hello “你好”OK[root@vmzq1l0l ~]# redis-cli get hello“\xe4\xbd\xa0\xe5\xa5\xbd”[root@vmzq1l0l ~]# redis-cli –raw get hello你好]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—慢查询分析]]></title>
    <url>%2F2019%2F03%2F19%2Fredis-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-e6-85-a2-e6-9f-a5-e8-af-a2-e5-88-86-e6-9e-90%2F</url>
    <content type="text"><![CDATA[慢查询日志帮助开发人员定位系统存在的慢操作，慢查询就是系统在命令执行前后计算的每条命令的执行时间，当超过预设阈值，就将这条命令的相关信息（例如：发生时间，耗时，命令的详细信息）记录下来，Redis提供类似的功能 客户端请求的四个执行步骤 1.发送命令——2.排队等待执行——3.执行命令——4.返回结果 慢查询只统计步骤3执行命令的这一步，所以没有慢查询并不等于客户端没有超时的情况 慢查询的配置参数 在redis.conf配置文件中有两个配置项：slowlog-log-slower-than和slowlog-max-len slowlog-log-slower-than表示预设的阈值，默认为10000微秒（1秒=1000毫秒=1000000微秒），如果一条命令执行时间超过了10000微秒就会被记录放在慢查询日志中 如果slowlog-log-slower-than设置为0就会记录所有的命令，设置为小于0就不会记录任何的命令 slowlog-max-len：Redis使用了一个列表来存储慢查询日志，此配置就是这个列表的最大长度，默认为128条，如果记录多于128条会删除最先插入的数据 如果想要修改配置可以直接在redis客户端进行修改，以slowlog-max-len为例 127.0.0.1:6379&gt; config set slowlog-max-len 512OK127.0.0.1:6379&gt; config rewriteOK 在查看配置文件，发现slowlog-max-len配置已经修改为512了 获取慢查询日志 虽然慢查询日志是存放在Redis内存列表中的，但是Redis并没有暴露这个列表的键，而是通过一组命令来实现对慢查询日志的访问和管理。 slowlog get [n] 查询两条慢查询记录，此处的[n]为选填项，如果不填查询所有，如果填就是查询指定的条数 127.0.0.1:6379&gt; slowlog get “2”1) 1) (integer) 12 2) (integer) 1552899968 3) (integer) 1000502 4) 1) “migrate” 2) “127.0.0.1” 3) “6379” 4) “a” 5) “1” 6) “1000” 7) “copy” 5) “127.0.0.1:35730” 6) “”2) 1) (integer) 11 2) (integer) 1552899943 3) (integer) 1000909 4) 1) “migrate” 2) “127.0.0.1” 3) “6379” 4) “a” 5) “0” 6) “1000” 7) “copy” 5) “127.0.0.1:35730” 6) “” 四个主要参数 1）慢查询日志的标识id、 2）发生时间戳、 3）命令耗时、 4）执行命令和参数 获取慢查询列表长度 slowlog len 当前有13条慢查询 127.0.0.1:6379&gt; slowlog len(integer) 13 清除慢查询列表 slowlog reset 清除当前慢查询列表 127.0.0.1:6379&gt; slowlog len(integer) 13127.0.0.1:6379&gt; slowlog resetOK127.0.0.1:6379&gt; slowlog len(integer) 0]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—数据库管理]]></title>
    <url>%2F2019%2F03%2F18%2Fredis-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-e6-95-b0-e6-8d-ae-e5-ba-93-e7-ae-a1-e7-90-86%2F</url>
    <content type="text"><![CDATA[切换收据库 select dbindex MySQL支持在一个实例下有多个数据库存在，但是关系型数据库是用字符来区分不同的数据库名，Redis只用数字来区分不同数据库，redis默认是16个数据库，在redis.conf配置文件有个默认配置databases 16。一般情况下不建议使用多数据库管理缓存，比如Redis的分布式实现Redis Cluster只允许使用0号数据库，如果想用多个数据库最好是布置多个redis实例用端口区分，用所有的实例的0号数据库，尽量不要做切换数据库的操作，以免难以维护 在14号数据库设置key为Charlie的数据，在其他数据库数据不互通 127.0.0.1:6379&gt; get Charlie(nil)127.0.0.1:6379&gt; select 14OK127.0.0.1:6379[14]&gt; set Charlie “I’m tired”OK127.0.0.1:6379[14]&gt; select 0OK127.0.0.1:6379&gt; get Charlie(nil) 清除数据 flushdb flushall flushdb/flushall 清库操作谨慎使用]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—遍历键]]></title>
    <url>%2F2019%2F03%2F18%2Fredis-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-e9-81-8d-e5-8e-86-e9-94-ae%2F</url>
    <content type="text"><![CDATA[全量遍历键 keys pattern 查看所有的key、“J”开头的key、“T”或者“J”开头的key 127.0.0.1:6379&gt; keys *1) “Tom”2) “Jerry”3) “hello”4) “Java”127.0.0.1:6379&gt; keys J*1) “Jerry”2) “Java”127.0.0.1:6379&gt; keys [T,J]*1) “Tom”2) “Jerry”3) “Java” 如果redis数据量数据量有大量键， keys命令有可能造成redis阻塞，如果不清楚有多少键的情况下避免在生产环境使用keys命令 渐进式遍历 Redis从2.8版本后，提供了一个新的命令scan，它能有效的解决keys命令存在的问题。和keys命令执行时会遍历所有键不同，scan采用渐进式遍历的方式来解决keys命令可能带来的阻塞问题，每次scan命令的时间复杂度是O（1），但是要真正实现keys的功能，需要执行多次scan。 Redis存储键值对实际使用的是hashtable的数据结构，那么每次执行scan，可以想象成只扫描一个字典中的一部分键，直到将字典中的所有键遍历完毕 scan cursor [MATCH pattern] [COUNT count] cursor是必需参数，实际上cursor是一个游标，第一次遍历从0开始，每次scan遍历完都会返回当前游标的值，直到游标值为0，表示遍历结束 match pattern是可选参数，它的作用的是做模式的匹配，这点和keys的模式匹配很像 count number是可选参数，它的作用是表明每次要遍历的键个数，默认值是10，此参数可以适当增大 把刚刚6666端口实例清空（flushdb命令），然后添加英文字母26个key用作测试 127.0.0.1:6666&gt; flushdbOK127.0.0.1:6666&gt; keys *(empty list or set)127.0.0.1:6666&gt; mset a a b b c c d d e e f f g g h h i i j j k k l l m m n n o o p p q q r r s s t t u u v v w w x x y y z zOK 第一次执行scan0，返回结果分为两个部分：第一个部分1就是下次scan需要的cursor，第二个部分是10个键： 127.0.0.1:6666&gt; scan 01) “1”2) 1) “u” 2) “w” 3) “g” 4) “a” 5) “b” 6) “m” 7) “z” 8) “q” 9) “i” 10) “y” 使用新的cursor=“1”，scan 1 返回下一次的cursor=“29”是个键 127.0.0.1:6666&gt; scan 11) “29”2) 1) “n” 2) “e” 3) “t” 4) “f” 5) “c” 6) “s” 7) “h” 8) “x” 9) “o” 10) “j” 使用得到的新的cursor=“29”，scan 29 返回结果cursor变成0，说明所有的键都已经遍历过了 127.0.0.1:6666&gt; scan 291) “0”2) 1) “p” 2) “v” 3) “r” 4) “l” 5) “k” 6) “d” 除了scan以外，Redis提供了面向哈希类型、集合类型、有序集合的扫描遍历命令，解决诸如hgetall、smembers、zrange可能产生的阻塞问题，对应的命令分别是hscan、sscan、zscan，它们的用法和scan基本类似，下面以sscan为例子进行说明，当前集合有两种类型的元素，例如分别以old：user和new：user开头，先需要将old：user开头的元素全部删除，可以参考如下伪代码： String key = “myset”;// 定义 pattern String pattern = “old:user*”;// 游标每次从 0 开始 String cursor = “0”;while (true) { // 获取扫描结果 ScanResult scanResult = redis.sscan(key, cursor, pattern); List elements = scanResult.getResult(); if (elements != null &amp;&amp; elements.size() &gt; 0) { // 批量删除 redis.srem(key, elements); } // 获取新的游标 cursor = scanResult.getStringCursor(); // 如果游标为 0 表示遍历结束 if (“0”.equals(cursor)) { break; } }]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—单个键管理]]></title>
    <url>%2F2019%2F03%2F11%2Fredis-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-e9-94-ae-e7-ae-a1-e7-90-86%2F</url>
    <content type="text"><![CDATA[键重命名 rename key newkey 重命名键名“python”为“java” 127.0.0.1:6379&gt; set python jedisOK127.0.0.1:6379&gt; rename python javaOK127.0.0.1:6379&gt; get python(nil)127.0.0.1:6379&gt; get java“jedis” 如果原来的键存在，rename会覆盖原来的键的值 127.0.0.1:6379&gt; set a bOK127.0.0.1:6379&gt; set c dOK127.0.0.1:6379&gt; rename a cOK127.0.0.1:6379&gt; get a(nil)127.0.0.1:6379&gt; get c“b” 为了防止强行覆盖Redis提供了renamenx命令 renamenx key newkey 举例说明 127.0.0.1:6379&gt; set a bOK127.0.0.1:6379&gt; set c dOK127.0.0.1:6379&gt; renamenx a c(integer) 0127.0.0.1:6379&gt; get a“b”127.0.0.1:6379&gt; get c“d”127.0.0.1:6379&gt; renamenx a e(integer) 1 *随机返回一个键* randomkey 随机返回键 127.0.0.1:6379&gt; dbsize(integer) 21127.0.0.1:6379&gt; randomkey“user:ranking:2”127.0.0.1:6379&gt; randomkey“java”127.0.0.1:6379&gt; randomkey“user:2” 键过期 1.设置键过期 秒级： expire key seconds 毫秒级： pexpire key milliseconds 2.查看键的剩余过期时间 秒级别 ttl 毫秒级 pttl 大于等于0的整数：键剩余的过期时间（ttl是秒，pttl是毫秒） -1：键没有设置过期时间 -2：键不存在。 设置键为“java”过期时间100秒， 127.0.0.1:6379&gt; expire java 100(integer) 1127.0.0.1:6379&gt; pttl java(integer) 95390127.0.0.1:6379&gt; ttl java(integer) 89 3.键在时间戳timestamp后过期 秒级时间戳： expireat key timestamp 毫秒级时间戳： pexpireat key milliseconds-timestamp 设置键“hello”在2019-03-11 15:35:42过期（秒时间戳是1552289742，毫秒为1552289742000） expireat hello 1552289742 4.清除键的过期时间 1）persist 命令 persist key 2）字符串类型的key，直接set会清除key的过期时间 5.设置键值的同时添加过期时间 setex key seconds value setex 不但是原子执行，同时减少了一次网络通讯的时间 迁移键 迁移键功能非常重要，因为有时候我们只想把部分数据由一个Redis迁移到另一个Redis（例如从生产环境迁移到测试环境），Redis发展历程中提供了move、dump+restore、migrate三组迁移键的方法，它们的实现方式以及使用的场景不太相同 1.move move key db move命令用于在Redis内部进行数据迁移，Redis内部可以有多个数据库，彼此数据是相互隔离的，move就是从一个数据库移动到另外一个数据库 2.dump+restore dump keyrestore key ttl value dump+restore实现的是在不同Redis实例间进行数据迁移： 在源Redis实例上，dump命令会将键值序列化，格式采用的是RDB格式 在目标Redis上，restore命令将上面序列化的值进行复原，其中ttl为过期时间，如果ttl设置为0则代表没有过期时间 在主机1中对key为listkey的数据进行序列化，返回RDB格式的序列化value 127.0.0.1:6379&gt; dump java“\x00\x05jedis\t\x00b\xfc\b/\xf1’&lt;)” 复制这个序列化的value，在主机2中restore迁移数据 127.0.0.1:6379&gt; get java(nil)127.0.0.1:6379&gt; restore java 0 “\x00\x05jedis\t\x00b\xfc\b/\xf1’&lt;)”OK127.0.0.1:6379&gt; get java“jedis” 3.migrate migrate host port key|”” destination-db timeout [COPY] [REPLACE] [KEYS key] host：目标Redis的IP地址 port：目标Redis的端口 key：如果单个键就写键名，如果转移多个键此处就写双引号””空字符串（Redis3.0.6版本之后支持迁移多个键） destination-db：目标数据库的索引，如果是0号数据库就写0 timeout：迁移的超时时间（毫秒） [copy]：如果添加此选项，迁移后并不删除源键 [replace]：如果添加此选项，migrate不管目标Redis是否存在该键都会正常迁移进行数据覆盖 [KEYS key]：迁移多个键，例如要迁移key1、key2、key3，此处填写“keys key1 key2 key3 mingrate实现的是在不同Redis实例间进行数据迁移，实际上migrate命令就是将dump、restore、del三个命令进行组合，从而简化了操作流程。这个命令是一个原子操作，它在执行的时候会阻塞进行迁移的两个实例，直到以下任意结果发生：迁移成功，迁移失败，等到超时。 开另一个端口为6666的Redis实例 # ./redis-server –port 6666 &amp;# redis-cli -h 127.0.0.1 -p 6666127.0.0.1:6666&gt; keys *(empty list or set) 6379端口实例进行migrate的copy操作 127.0.0.1:6379&gt; migrate 127.0.0.1 6666 java 0 1000 copyOK127.0.0.1:6379&gt; get java“jedis” 6666端口实例 127.0.0.1:6666&gt; keys *1) “java” 6379端口实例进行migrate的默认操作（dump——restore——del） 127.0.0.1:6379&gt; set hello worldOK127.0.0.1:6379&gt; get hello“world”127.0.0.1:6379&gt; migrate 127.0.0.1 6666 hello 0 1000OK127.0.0.1:6379&gt; get hello(nil) 6666端口实例 127.0.0.1:6666&gt; keys *1) “java”2) “hello” 从端口6379端口实例批量迁移到6666端口实例 127.0.0.1:6379&gt; mset Tom “It’s a cat” Jerry “It’s a mouse”OK127.0.0.1:6379&gt; mget Tom Jerry1) “It’s a cat”2) “It’s a mouse”127.0.0.1:6379&gt; migrate 127.0.0.1 6666 “” 0 1000 keys Tom JerryOK127.0.0.1:6379&gt; mget Tom Jerry1) (nil)2) (nil) 6666端口实例 127.0.0.1:6666&gt; keys *1) “Tom”2) “Jerry”3) “java”4) “hello”]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—数据类型：有序集合（zset）]]></title>
    <url>%2F2019%2F03%2F11%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%EF%BC%9A%E6%9C%89%E5%BA%8F%E9%9B%86%E5%90%88%EF%BC%88zset%EF%BC%89%2F</url>
    <content type="text"><![CDATA[有序集合相对于哈希、列表、集合来说会有一点点陌生，但既然叫有序集合，那么它和集合必然有着联系，它保留了集合不能有重复成员的特性，但不同的是，有序集合中的元素可以排序。但是它和列表使用索引下标作为排序依据不同的是，它给每个元素设置一个分数（score）作为排序的依据。 集合内操作 1.添加成员 zadd key [nx|xx] [ch] [incr] score member [score member …] nx：member必须不存在，才可以设置成功，用于添加 xx：member必须存在，才可以设置成功，用于更新 ch：返回此次操作后，有序集合元素和分数发生变化的个数 incr：对score做增加，相当于后面介绍的zincrby 有序集合相比集合提供了排序字段，但是也产生了代价，zadd的时间复杂度为O（log（n）），sadd的时间复杂度为O（1）。 添加key为“user:ranking”的集合 127.0.0.1:6379&gt; zadd user:ranking 150 Tom(integer) 1127.0.0.1:6379&gt; zadd user:ranking 1 Charlie 14 Day 66 Ling 124 Zhang(integer) 4 2.计算成员个数 zcard key 计算key为“user:ranking”的集合成员个数 127.0.0.1:6379&gt; zcard user:ranking(integer) 5 3.计算某个成员的分数 zscore key member 计算key为“user:ranking”的集合成员“Charlie”的分数 127.0.0.1:6379&gt; zscore user:ranking Charlie“1” 4.计算成员的排名 从低到高 zrank key member 从高到低 zrevrank key member 计算key为“user:ranking”的集合成员的排名 127.0.0.1:6379&gt; zrank user:ranking Day(integer) 1127.0.0.1:6379&gt; zrank user:ranking Charlie(integer) 0127.0.0.1:6379&gt; zrank user:ranking Zhang(integer) 3 5.删除成员 zrem key member [member …] 删除key为“user:ranking”的集合成员“Zhang” 127.0.0.1:6379&gt; zrem user:ranking Zhang(integer) 1 6.增加成员的分数 zincrby key increment member 增加key为“user:ranking”的集合成员“Ling”5分 127.0.0.1:6379&gt; zincrby user:ranking 5 Ling“71” 7.返回指定排名范围的成员 从低到高 zrange key start end [withscores] 从高到低 zrevrange key start end [withscores] 返回key为“user:ranking”的集合指定范围和分数 127.0.0.1:6379&gt; zrange user:ranking 0 -11) “Charlie”2) “Day”3) “Ling”4) “Tom”127.0.0.1:6379&gt; zrange user:ranking 0 -1 withscores1) “Charlie”2) “1”3) “Day”4) “14”5) “Ling”6) “71”7) “Tom”8) “150” 8.返回指定分数范围的成员 从低到高 zrangebyscore key min max [withscores] [limit offset count] 从高到低 zrevrangebyscore key max min [withscores] [limit offset count] 返回key为“user:ranking”的集合指定分数范围的成员 127.0.0.1:6379&gt; zrangebyscore user:ranking 10 80 withscores1) “Day”2) “14”3) “Ling”4) “71”127.0.0.1:6379&gt; zrevrangebyscore user:ranking 80 10 withscores1) “Ling”2) “71”3) “Day”4) “14” min和max默认闭区间，支持开区间（小括号），-inf和+inf分别代表无限小和无限大 127.0.0.1:6379&gt; zrangebyscore user:ranking 10 +inf withscores1) “Day”2) “14”3) “Ling”4) “71”5) “Tom”6) “150”127.0.0.1:6379&gt; zrevrangebyscore user:ranking 80 -inf withscores1) “Ling”2) “71”3) “Day”4) “14”5) “Charlie”6) “1”127.0.0.1:6379&gt; zrevrangebyscore user:ranking 71 -inf withscores1) “Ling”2) “71”3) “Day”4) “14”5) “Charlie”6) “1”127.0.0.1:6379&gt; zrevrangebyscore user:ranking (71 -inf withscores1) “Day”2) “14”3) “Charlie”4) “1” 9.返回指定分数范围的成员个数 zcount key min max 返回key为“user:ranking”的集合指定分数范围的成员个数 127.0.0.1:6379&gt; zcount user:ranking 10 80(integer) 2 10.删除指定排名内的升序元素 zremrangebyrank key start stop 11.删除指定分数范围的成员 zremrangebyscore key min max 集合间的操作 添加测试数据 127.0.0.1:6379&gt; zadd user:ranking:1 1 Carlie 15 Day 67 Lin 124 Zhang(integer) 4127.0.0.1:6379&gt; zadd user:ranking:2 5 Tom 35 Jerry 564 Zhang(integer) 3 1.交集 zinterstore destination numkeys key [key …] [weights weight [weight …]] [aggregate sum|min|max] destination：交集计算结果保存到这个键 numkeys：需要做交集计算键的个数 key[key…]：需要做交集计算的键 weights weight[weight…]：每个键的权重，在做交集计算时，每个键中的每个member会将自己分数乘以这个权重，每个键的权重默认是1 aggregate sum|min|max：计算成员交集后，分值可以按照sum（和）、min（最小值）、max（最大值）做汇总，默认值是sum 下面操作对user：ranking：1和user：ranking：2做交集，weights和aggregate使用了默认配置，可以看到目标键user：ranking：1_inter_2对分值做了sum操作 127.0.0.1:6379&gt; zinterstore user:ranking:1_inter_2 2 user:ranking:1 user:ranking:2(integer) 1127.0.0.1:6379&gt; zrange user:ranking:1_inter_2 0 -1 withscores1) “Zhang”2) “688” 2.并集 zunionstore destination numkeys key [key …] [weights weight [weight …]] [aggregate sum|min|max] 生成key为“user:ranking:1_union_2”的并集 127.0.0.1:6379&gt; zunionstore user:ranking:1_union_2 2 user:ranking:1 user:ranking:2(integer) 6127.0.0.1:6379&gt; zrange user:ranking:1_union_2 0 -1 withscores 1) “Carlie” 2) “1” 3) “Tom” 4) “5” 5) “Day” 6) “15” 7) “Jerry” 8) “35” 9) “Lin”10) “67”11) “Zhang”12) “688” 内部编码 ziplist（压缩列表）：当有序集合的元素个数小于zset-max-ziplist-entries配置（默认128个），同时每个元素的值都小于zset-max-ziplist-value配置（默认64字节）时，Redis会用ziplist来作为有序集合的内部实现，ziplist可以有效减少内存的使用 skiplist（跳跃表）：当ziplist条件不满足时，有序集合会使用skiplist作为内部实现，因为此时ziplist的读写效率会下降。 使用场景 排行榜系统 1.添加用户赞数可以使用zadd和zincrby功能 2.取消用户赞数，用户作弊或者注销，可以用zrem 3.展示排行榜使用zrevrange命令 4.展示用户信息或者分数可以使用zscore和zrank两个功能]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—数据类型：集合（set）]]></title>
    <url>%2F2019%2F03%2F08%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%EF%BC%9A%E9%9B%86%E5%90%88%EF%BC%88set%EF%BC%89%2F</url>
    <content type="text"><![CDATA[集合（set）类型也是用来保存多个的字符串元素，但和列表类型不一样的是，集合中不允许有重复元素，并且集合中的元素是无序的，不能通过索引下标获取元素。 ** 集合内操作** 1.添加元素 sadd key element [element …] 添加key为“myset”的集合，返回的是添加的个数 127.0.0.1:6379&gt; sadd myset a b c(integer) 3127.0.0.1:6379&gt; sadd myset a b(integer) 0 2.获取所有元素 smembers key 获取key为“myset”的集合所有元素 127.0.0.1:6379&gt; smembers myset1) “c”2) “a”3) “b” 3.删除元素 srem key element [element …] 删除key为“myset”集合的“a”元素 127.0.0.1:6379&gt; srem myset a(integer) 1 4.计算元素个数 scard key 计算key为“myset”集合的元素个数 127.0.0.1:6379&gt; scard myset(integer) 2 5.判断元素是否在集合中 sismember key element 判断key为“myset”集合中元素 127.0.0.1:6379&gt; sismember myset b(integer) 1127.0.0.1:6379&gt; sismember myset a(integer) 0 6.随机从集合返回指定个数元素 srandmember key count 随机从key为“myset”集合返回元素 127.0.0.1:6379&gt; srandmember myset 11) “b”127.0.0.1:6379&gt; srandmember myset 11) “b”127.0.0.1:6379&gt; srandmember myset 11) “c” 7.从集合随机弹出（删除）元素（可指定个数） spop key [count] 随机删除key为“myset”集合中的元素 127.0.0.1:6379&gt; smembers myset1) “c”2) “a”3) “b”127.0.0.1:6379&gt; spop myset 11) “b”127.0.0.1:6379&gt; spop myset“a”127.0.0.1:6379&gt; smembers myset1) “c” *集合间操作* 添加测试用数据 127.0.0.1:6379&gt; sadd act:1 My name is Tom(integer) 4127.0.0.1:6379&gt; sadd act:2 It is Jerry(integer) 3 1.求多个集合的交集 sinter key [key…] 求key为“act:1”和“act:2”的交集 127.0.0.1:6379&gt; sinter act:1 act:21) “is” 2.求多个集合的并集 sunion key [key …] 求key为“act:1”和“act:2”的交集 127.0.0.1:6379&gt; sunion act:1 act:21) “name”2) “is”3) “Tom”4) “Jerry”5) “My”6) “It” 3.求多个集合的差集 sdiff key [key …] 求key为“act:1”和“act:2”的差集 127.0.0.1:6379&gt; sdiff act:1 act:21) “Tom”2) “My”3) “name” 4.将交集、并集、差集的结果保存 sinterstore destination key [key …]sunionstore destination key [key …]sdiffstore destination key [key …] 保存key为“act:1”和“act:2”的交集为key名“act:1_2:inter” 127.0.0.1:6379&gt; sinterstore act:1_2:inter act:1 act:2(integer) 1127.0.0.1:6379&gt; smembers act:1_2:inter1) “is” ** 内部编码** intset（整数集合）：当集合中的元素都是整数且元素个数小于set-max-intset-entries配置（默认512个）时，Redis会选用intset来作为集合的内部实现，从而减少内存的使用。 hashtable（哈希表）：当集合类型无法满足intset的条件时，Redis会使用hashtable作为集合的内部实现。 使用场景 sadd=Tagging（标签） spop/srandmember=Random item（生成随机数，比如抽奖） sadd+sinter=Social Graph（社交需求）]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—数据类型：列表（list）]]></title>
    <url>%2F2019%2F03%2F08%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%EF%BC%9A%E5%88%97%E8%A1%A8%EF%BC%88list%EF%BC%89%2F</url>
    <content type="text"><![CDATA[列表（list）类型是用来存储多个有序的字符串。列表是一种比较灵活的数据结构，它可以充当栈和队列的角色，在实际开发上有很多应用场景。 添加操作 1.从右边插入元素 rpush key value [value …] 从右边插入a,b,c 127.0.0.1:6379&gt; rpush mylist a b c(integer) 3 从左到右获取列表所有元素 127.0.0.1:6379&gt; lrange mylist 0 -11) “a”2) “b”3) “c” 2.从左边插入元素 lpush key value [value …] 从左边插入d,e 127.0.0.1:6379&gt; lpush mylist d e(integer) 5127.0.0.1:6379&gt; lrange mylist 0 -11) “e”2) “d”3) “a”4) “b”5) “c” 3.向某个元素前或者后插入元素 linsert key BEFORE|AFTER pivot value 在key为mylist的数组元素“e”之后添加一个“test”元素，在元素“b”之前添加一个“test2”元素 127.0.0.1:6379&gt; linsert mylist after e test(integer) 6127.0.0.1:6379&gt; lrange mylist 0 -11) “e”2) “test”3) “d”4) “a”5) “b”6) “c”127.0.0.1:6379&gt; linsert mylist before b test2(integer) 7127.0.0.1:6379&gt; lrange mylist 0 -11) “e”2) “test”3) “d”4) “a”5) “test2”6) “b”7) “c” 查找操作 1.获取指定范围内的元素列表 lrange key start stop 第一，索引下标从左到右分别是0到N-1，但是从右到左分别是-1到-N。第二，lrange中的end选项包含了自身，这个和很多编程语言不包含end不太相同 获取key为mylist的索引位置3-5（即第四个到第六个）的列表数据；索引位置倒数第四个-4到最后一个-1的列表数据 127.0.0.1:6379&gt; lrange mylist 3 51) “a”2) “test2”3) “b”127.0.0.1:6379&gt; lrange mylist -4 -11) “a”2) “test2”3) “b”4) “c” 2.获取指定索引下标的元素 lindex key index 获取key为mylist索引位置为3（第四个）的元素 127.0.0.1:6379&gt; lrange mylist 0 -11) “e”2) “test”3) “d”4) “a”5) “test2”6) “b”7) “c”127.0.0.1:6379&gt; lindex mylist 3“a” 3.获取列表长度 llen key 获取key为mylist的列表长度 127.0.0.1:6379&gt; llen mylist(integer) 7 删除操作 1.从列表左边弹出元素 lpop key 删除key为mylist的左边第一个元素 127.0.0.1:6379&gt; lrange mylist 0 -11) “e”2) “test”3) “d”4) “a”5) “test2”6) “b”7) “c”127.0.0.1:6379&gt; lpop mylist“e”127.0.0.1:6379&gt; lrange mylist 0 -11) “test”2) “d”3) “a”4) “test2”5) “b”6) “c” 2.从列表右边弹出元素 rpop key 3.删除指定元素 lrem key count value lrem命令会从列表中找到等于value的元素进行删除，根据count的不同分为三种情况： count&gt;0，从左到右，删除最多count个元素。 count&lt;0，从右到左，删除最多count绝对值个元素。 count=0，删除所有。 先给key为mylist的列表左边添加5个“a”元素，然后删除五个“a”元素 127.0.0.1:6379&gt; lpush mylist a a a a a(integer) 11127.0.0.1:6379&gt; lrange mylist 0 -1 1) “a” 2) “a” 3) “a” 4) “a” 5) “a” 6) “e” 7) “test” 8) “d” 9) “test2”10) “b”11) “c”127.0.0.1:6379&gt; lrem mylist 5 a(integer) 5127.0.0.1:6379&gt; lrange mylist 0 -11) “e”2) “test”3) “d”4) “test2”5) “b”6) “c” 4.按照索引范围修剪列表 ltirm key start end 保留key为mylist的列表索引位置2~4（第三个到第五个）的元素 127.0.0.1:6379&gt; lrange mylist 0 -11) “e”2) “test”3) “d”4) “test2”5) “b”6) “c”127.0.0.1:6379&gt; ltrim mylist 2 4OK127.0.0.1:6379&gt; lrange mylist 0 -11) “d”2) “test2”3) “b” 修改操作 lset key index value 修改key为mylist索引位置1（第二个）的元素值 127.0.0.1:6379&gt; lrange mylist 0 -11) “d”2) “test2”3) “b”127.0.0.1:6379&gt; lset mylist 1 testOK127.0.0.1:6379&gt; lrange mylist 0 -11) “d”2) “test”3) “b” 阻塞删除操作 blpop key [key …] timeoutbrpop key [key …] timeout 1.列表为空 如果查询列表为空，则等待timeout设置的时间返回，比如timeout=5，就是等待5秒后返回；如果timeout=0就会一直阻塞下去，直到给此未设置的列表key添加元素 127.0.0.1:6379&gt; blpop listkey 5(nil)(5.03s)127.0.0.1:6379&gt; blpop listkey 0处于阻塞状态 打开另外的客户端，给空key为listkey的添加列表元素 127.0.0.1:6379&gt; lpush listkey a(integer) 1 此时一直处于阻塞的客户端返回删除的列表key和删除的元素 127.0.0.1:6379&gt; blpop listkey 01) “listkey”2) “a”(68.04s) 2.列表不为空 列表不为空如果timeout=0会立即返回删除的列表key和删除的元素，如果设置了时间会等到时间到了返回删除结果 127.0.0.1:6379&gt; blpop mylist 01) “mylist”2) “a” 第一点，如果是多个键，那么brpop会从左至右遍历键，一旦有一个键能删除元素，客户端立即返回第二点，如果多个客户端对同一个键执行brpop，那么最先执行brpop命令的客户端可以获取到弹出的值 内部编码 列表类型的内部编码有两种。 ziplist（压缩列表）：当列表的元素个数小于list-max-ziplist-entries配置（默认512个），同时列表中每个元素的值都小于list-max-ziplist-value配置时（默认64字节），Redis会选用ziplist来作为列表的内部实现来减少内存的使用。 linkedlist（链表）：当列表类型无法满足ziplist的条件时，Redis会使用linkedlist作为列表的内部实现。 quicklist内部编码，简单地说它是以一个ziplist为节点的linkedlist，它结合了ziplist和linkedlist两者的优势，为列表类型提供了一种更为优秀的内部编码实 使用场景 lpush+lpop=Stack（栈） lpush+rpop=Queue（队列） lpsh+ltrim=Capped Collection（有限集合） lpush+brpop=Message Queue（消息队列） 1.消息队列 Redis的lpush+brpop命令组合即可实现阻塞队列，生产者客户端使用lrpush从列表左侧插入元素，多个消费者客户端使用brpop命令阻塞式的“抢”列表尾部的元素，多个客户端保证了消费的负载均衡和高可用性。 2.文章列表 每个用户有属于自己的文章列表，现需要分页展示文章列表。此时可以考虑使用列表，因为列表不但是有序的，同时支持按照索引范围获取元素。]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—数据类型：哈希（hash）]]></title>
    <url>%2F2019%2F03%2F08%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%EF%BC%9A%E5%93%88%E5%B8%8C%EF%BC%88hash%EF%BC%89%2F</url>
    <content type="text"><![CDATA[几乎所有的编程语言都提供了哈希（hash）类型，它们的叫法可能是哈希、字典、关联数组。在Redis中，哈希类型是指键值本身又是一个键值对结构，形如value={ {field1，value1}，…{fieldN，valueN}} 哈希类型中的映射关系叫作field-value，注意这里的value是指field对应的值，不是键对应的值，请注意value在不同上下文的作用。 设置值 1hset key filed value 为key名为user:1设置field-value 1234127.0.0.1:6379&gt; hset user:1 name Tom(integer) 1127.0.0.1:6379&gt; hset user:1 age 26(integer) 1 Redis额外提供了hsetnx命令；hset和hsetnx的关系与set和setnx的关系类似，如果没有则添加，如果有则不作操作，只不过hsetnx是对于field操作的，setnx是相对于key来做操作 获取值 1hget key field 获取key为user:1，分别获取field为name和age的值 1234127.0.0.1:6379&gt; hget user:1 name"Tom"127.0.0.1:6379&gt; hget user:1 age"26" 删除field 1hdel key field [field ...] 删除key为user:1，field为sex的值；返回的是删除的个数 1234127.0.0.1:6379&gt; hset user:1 sex male(integer) 1127.0.0.1:6379&gt; hdel user:1 sex(integer) 1 计算field个数 1hlen key 获取key为user:1的field个数 12127.0.0.1:6379&gt; hlen user:1(integer) 2 批量设置field-value 1hmset key field value [field value ...] 批量设置key为user:2的field-value 12127.0.0.1:6379&gt; hmset user:2 name Jerry age 15OK 批量获取field-value 1hmget key field [field ...] 批量获取key为user:2的field为name和age的value 123127.0.0.1:6379&gt; hmget user:2 name age1) "Jerry"2) "15" 判断field是否存在 1hexists key field 返回个数，没有就返回0 1234127.0.0.1:6379&gt; hexists user:1 name(integer) 1127.0.0.1:6379&gt; hexists user:1 job(integer) 0 获取所有field 1hkeys key 获取key为user:1的所有field 123127.0.0.1:6379&gt; hkeys user:11) "name"2) "age" 获取所有value 1hvals key 获取key为user:1的所有value 123127.0.0.1:6379&gt; hkeys user:11) "name"2) "age" 获取所有的filed-value 1hgetall key 获取key为user:1的所有filed-value 12345127.0.0.1:6379&gt; hgetall user:11) "name"2) "Tom"3) "age"4) "26" 使用hgetall时如果哈希元素过多，有可能造成Redis阻塞，尽量避免使用，可以用hscan命令渐进式遍历哈希类型 计数 增加指定数字 1hincrby key field increment 给key为user:1，filed为age的value加5 1234127.0.0.1:6379&gt; hget user:1 age"26"127.0.0.1:6379&gt; hincrby user:1 age 5(integer) 31 增加指定浮点数 1hincrbyfloat key field increment 计算value的字符串长度 1hstrlen key field 获取key为user:1，field为name的value长度 12127.0.0.1:6379&gt; hstrlen user:1 name(integer) 3 内部编码 哈希类型的内部编码有两种： ·ziplist（压缩列表）：当哈希类型元素个数小于hash-max-ziplist-entries配置（默认512个）、同时所有值都小于hash-max-ziplist-value配置（默认64字节）时，Redis会使用ziplist作为哈希的内部实现，ziplist使用更加紧凑的结构实现多个元素的连续存储，所以在节省内存方面比hashtable更加优秀。 ·hashtable（哈希表）：当哈希类型无法满足ziplist的条件时，Redis会使用hashtable作为哈希的内部实现，因为此时ziplist的读写效率会下降，而hashtable的读写时间复杂度为O（1）]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—数据类型：字符串（string）]]></title>
    <url>%2F2019%2F03%2F08%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%EF%BC%9A%E5%AD%97%E7%AC%A6%E4%B8%B2%EF%BC%88string%EF%BC%89%2F</url>
    <content type="text"><![CDATA[字符串是Redis最基础的数据结构。首先键是字符串类型，而且其他几种类型都是在字符串类型的基础上构建的。字符串类型的值实际可以是字符串（简单的字符串、复杂的字符串（例如JSON、XML））、数字（整数、浮点数），甚至是二进制（图片、音频、视频），但是值最大不能超过512MB。 设置值set命令 1set key value [ex seconds] [px milliseconds] [nx|xx] ex seconds：为键设置秒级过期时间。 12127.0.0.1:6379&gt; set hello world ex 50OK px milliseconds：为键设置毫秒级过期时间。 12127.0.0.1:6379&gt; set hello world px 50000OK nx：键必须不存在，才可以设置成功，用于添加。 1234127.0.0.1:6379&gt; set hello world nxOK127.0.0.1:6379&gt; set hello world nx(nil) xx：与nx相反，键必须存在，才可以设置成功，用于更新。 12345678127.0.0.1:6379&gt; del hello(integer) 1127.0.0.1:6379&gt; set hello world xx(nil)127.0.0.1:6379&gt; set hello worldOK127.0.0.1:6379&gt; set hello world1 xxOK setex命令（设置过期时间） 1setex key seconds value 下面四个例子都是设置key为hello的缓存过期时间50秒 12345678127.0.0.1:6379&gt; setex hello 50 worldOK127.0.0.1:6379&gt; set hello world ex 50OK127.0.0.1:6379&gt; set hello world px 50000OK127.0.0.1:6379&gt; expire java 50(integer) 1 setnx命令（如果key存在则不做操作，如果没有则创建） 1setnx key value 由于Redis的单线程命令处理机制，如果有多个客户端同时执行setnx key value，根据setnx的特性只有一个客户端能设置成功，setnx可以作为分布式锁的一种实现方案，Redis官方给出了使用setnx实现分布式锁的方法：http://redis.io/topics/distlock。 获取值1get key 批量设置值1mset key value [key value ...] 批量设置a,b,c三个key 12127.0.0.1:6379&gt; mset a 1 b 2 c 3OK 批量获取值按顺序返回1mget key [key ...] 批量获取a,b,c三个key值 1234127.0.0.1:6379&gt; mget a b c1) "1"2) "2"3) "3" 批量操作可以减少项目和redis数据库的网络请求，如果循环获取每条，那每条都要走网络请求，跟内存速度比，网络绝对是性能的瓶颈，所以批量操作是交给redis内部处理，一次请求，一次返回，有利于提高业务处理效率；但是要注意的是每次批量操作所发送的命令数不是无节制的，如果数量过多可能会造成Redis的阻塞或者网络拥塞。 计数自增 1incr key 自减 1decr key 自增指定数 1incr key increment 自减指定数 1decrby key decrement 自增浮点数 1incrbyfloat key increment 很多存储系统和编程语言内部使用CAS机制实现计数功能，会有一定的CPU开销，但在Redis中完全不存在这个问题，因为Redis是单线程架构，任何命令到了Redis服务端都要顺序执行。 追加值1append key value 举例说明： 123456127.0.0.1:6379&gt; get hello"world"127.0.0.1:6379&gt; append hello is(integer) 7127.0.0.1:6379&gt; get hello"worldis" 获取字符串长度1strlen key 举例说明： 1234127.0.0.1:6379&gt; set hello worldOK127.0.0.1:6379&gt; strlen hello(integer) 5 设置并返回值（如果原来有值则返回原来的值）1getset key value 举例说明： 123456127.0.0.1:6379&gt; get hello"world"127.0.0.1:6379&gt; getset hello world1"world"127.0.0.1:6379&gt; get hello"world1" 设置指定位置的字符1setrange key offeset value 举例说明： 123456127.0.0.1:6379&gt; set java redisOK127.0.0.1:6379&gt; setrange java 0 j(integer) 5127.0.0.1:6379&gt; get java"jedis" 获取部分字符串1getrange key start end 举例说明： 1234127.0.0.1:6379&gt; get java"jedis"127.0.0.1:6379&gt; getrange java 1 3"edi" 查看内部编码12127.0.0.1:6379&gt; object encoding hello"embstr" 字符串类型的内部编码有3种： int：8个字节的长整型。 embstr：小于等于39个字节的字符串。 raw：大于39个字节的字符串。 Redis会根据当前值的类型和长度决定使用哪种内部编码实现。 典型使用场景 （三个例子）1）做存储层不常改变的，但是又经常需要访问的数据的一个缓存层，减轻数据库压力 2）做用户session的存储器，解决分布式服务负载均衡而出现的session不同步问题 3）计数，例如网页的浏览量，视频播放量，如果每次都调用接口存数据库会造成大量请求的出现，极大地影响性能，可以先存在redis，之后再进行持久化]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—单线程架构]]></title>
    <url>%2F2019%2F03%2F08%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E5%8D%95%E7%BA%BF%E7%A8%8B%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Redis使用单线程架构和I/O多路复用模型来实现高性能的内存数据库服务Redis是单线程来处理命令，所以一条命令从客户端到达服务端不会立即执行，所有命令都会进入一个队列中，然后逐个被执行，不会出现两条命令同时执行的情况，不会产生并发的问题，这就是Redis单线程的基本模型。但是发送命令、返回结果、命令排队肯定不像描述的这么简单，Redis使用I/O多路复用技术来解决I/O的问题。 为何单线程还可以这么快 纯内存访问，Redis将所有的数据存放在内存中，内存的响应时间大约为100纳秒，这是Redis达到每秒万级别访问的重要基础 非阻塞I/O，Redis使用epoll作为I/O多路复用技术的实现，再加上Redis自身的事件处理模型将epoll中的连接、读写、关闭都转为了事件，不在网络I/O上浪费过多的时间 单线程避免了线程切换和竞态产生的消耗 第一，单线程可以简化数据结构和算法的实现。如果对高级编程语言熟悉的读者应该了解并发数据结构实现不但困难而且开发测试比较麻烦。 第二，单线程避免了线程切换和竞态产生的消耗，对于服务端开发来说，锁和线程切换通常是性能杀手。 但是单线程会有一个问题：对于每个命令的执行时间是有要求的。如果某个命令执行过长，会造成其他命令的阻塞，对于Redis这种高性能的服务来说是致命的，所以Redis是面向快速执行场景的数据库]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—初识Redis]]></title>
    <url>%2F2019%2F03%2F08%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E5%88%9D%E8%AF%86Redis%2F</url>
    <content type="text"><![CDATA[连接命令行客户端（redis-cli）1#redis-cli 插入字符串类型的键值对（set key）1234127.0.0.1:6379&gt; set hello worldOK127.0.0.1:6379&gt; set java jedisOK 插入列表类型（list）的键值对（set listkey val1 val2 … valN ）1127.0.0.1:6379&gt; set mylist a b c d e 查看某一个键的值（get key）12127.0.0.1:6379&gt; get hello"world" 查看所有的键（keys *)123412.0.0.1:6379&gt; keys *1) "hello"2) "java"3) "mylist" 键总数（dbsize 显示数量）12127.0.0.1:6379&gt; dbsize(integer) 3 dbsize命令在计算键总数时不会遍历所有键，而是直接获取Redis内置的键总数变量，所以dbsize命令的时间复杂度是O（1）。而keys命令会遍历所有键，所以它的时间复杂度是O（n），当Redis保存了大量键时，线上环境禁止使用。检查键是否存在（exists key 下方显示的数字是个数）1234127.0.0.1:6379&gt; exists hello(integer) 1127.0.0.1:6379&gt; exists not_exist_key(integer) 0 删除键（删除单个：del key；批量删除：del key1 key2 key3）12345678910111213141516127.0.0.1:6379&gt; set delkey testdelOK127.0.0.1:6379&gt; exists delkey(integer) 1127.0.0.1:6379&gt; del delkey(integer) 1127.0.0.1:6379&gt; exists delkey(integer) 0127.0.0.1:6379&gt; set a 1OK127.0.0.1:6379&gt; set b 2OK127.0.0.1:6379&gt; set c 3OK127.0.0.1:6379&gt; del a b c(integer) 3 键过期（expire key seconds）12127.0.0.1:6379&gt; expire hello 15(integer) 1 查看设置键过期时间的剩余时间（ttl key）12345678910127.0.0.1:6379&gt; expire hello 15(integer) 1127.0.0.1:6379&gt; ttl hello(integer) 10127.0.0.1:6379&gt; ttl hello(integer) 7127.0.0.1:6379&gt; ttl hello(integer) 1127.0.0.1:6379&gt; ttl hello(integer) -2 ttl命令会返回键的剩余过期时间，它有3种返回值： 大于等于0的整数：键剩余的过期时间。 -1：键没设置过期时间。 -2：键不存在 查看键的数据结构类型（type key）1234127.0.0.1:6379&gt; type javastring127.0.0.1:6379&gt; type mylistlist]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—Redis安装、配置、启动、关闭]]></title>
    <url>%2F2019%2F03%2F08%2Fredis-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-redis-e5-ae-89-e8-a3-85%2F</url>
    <content type="text"><![CDATA[安装六个步骤（redis-5.0.3版本为例） #wget http://download.redis.io/releases/redis-5.0.3.tar.gz #tar xzf redis-5.0.3.tar.gz #ln -s redis-5.0.3 redis #cd redis #make #make install 下载指定版本的redis 解压压缩包 建立一个redis目录的软连接，指向redis-5.0.3 进入redis目录 编译（编译之前确保操作系统已经安装gcc） 安装 注意两点：第3步建立名为redis目录的软连接，这样做是为了不把redis目录固定在指定版本上，有利于Redis未来版本升级，算是安装软件的一种好习惯；第6步中的安装是将Redis的相关运行文件放到/usr/local/bin/下，这样就可以在任意目录下执行Redis的命令 配置、启动、操作、关闭RedisRedis安装之后，src和/usr/local/bin目录下多了几个以redis开头可执行文件，我们称之为Redis Shell，这些可执行文件可以做很多事情，例如可以启动和停止Redis、可以检测和修复Redis的持久化文件，还可以检测Redis的性能。 Redis Shell 功能 redis-server 启动redis redis-cli Redis命令行客户端 redis-benchmark Redis基准测试工具 redis-check-aof Redis AOF持久化文件检测和修复工具 redis-check-dump Redis RDB持久化文件检测和修复工具 redis-sentinel 启动Redis Sentinel 启动配置文件启动方式 # redis-server redis.conf端口启动方式（&amp;表示在后台运行） # redis-server --port 6666 &amp;配置文件redis.conf里面几个基础配置 参数 含义 port 端口号 logfile 日志文件 dir Redis工作目录（存放持久化文件和日志文件） daemonize 是否以守护进程的方式启动Redis，默认关闭 连接命令行客户端#redis-cli -h 127.0.0.1 -p 6379-h如果不输入默认是127.0.0.1，-p不输入默认是6379，所以直接输入redis-cli代表输入的是redis-cli -h 127.0.0.1 -p 6379；这里如果显示连接不上就设置redis.conf中daemonize为yes就可以后台连接命令行客户端了 关闭Redis服务# redis-cli shutdowm这时候再去输入命令redis-cli连接命令行客户端就会显示无连接，再启动redis服务即可连接]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记—Redis特性]]></title>
    <url>%2F2019%2F03%2F08%2Fredis-e5-ad-a6-e4-b9-a0-e7-ac-94-e8-ae-b0-redis-e7-89-b9-e6-80-a7%2F</url>
    <content type="text"><![CDATA[速度快redis数据存放在内存中； C语言实现的redis，C语言是距离操作系统最近的语言； 采用单线程架构，预防多线程可能产生的竞争问题。 基于键值对的数据结构服务器Redis支持五种数据类型：字符串类型（string），散列类型（hash），列表类型（list），集合类型（set），有序集合类型（zset）。 丰富的功能提供了键过期功能，可以用来实现缓存； 提供了发布订阅功能，可以用来实现消息系统； 支持Lua脚本功能，可以利用Lua创造出新的Redis命令； 提供了简单的事务功能，能在一定程度上保证事务特性； 提供了流水线（Pipeline）功能，这样客户端能将一批命令一次性传到Redis，减少了网络的开销。 简单稳定源码量少；其次Redis使用单线程模型，这样不仅使得Redis服务端处理模型变得简单，而且也使得客户端开发变得简单。最后，Redis不需要依赖于操作系统中的类库（例如Memcache需要依赖libevent这样的系统类库），Redis自己实现了事件处理的相关功能。 客户端语言多支持几乎所有主流语言使用，例如Java、PHP、Python、C、C++、Nodejs等。 持久化Redis提供了两种持久化方式：RDB和AOF。 主从复制Redis提供了复制功能，实现了多个相同数据的Redis副本，复制功能是分布式Redis的基础。 高可用和分布式Redis从2.8版本正式提供了高可用实现Redis Sentinel，它能够保证Redis节点的故障发现和故障自动转移。Redis从3.0版本正式提供了分布式实现Redis Cluster，它是Redis真正的分布式实现，提供了高可用、读写和容量的扩展性。]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[以下哪些方法可以取到http请求中的cookie值（）]]></title>
    <url>%2F2019%2F03%2F07%2F%E4%BB%A5%E4%B8%8B%E5%93%AA%E4%BA%9B%E6%96%B9%E6%B3%95%E5%8F%AF%E4%BB%A5%E5%8F%96%E5%88%B0http%E8%AF%B7%E6%B1%82%E4%B8%AD%E7%9A%84cookie%E5%80%BC%EF%BC%88%EF%BC%89%2F</url>
    <content type="text"><![CDATA[链接：https://www.nowcoder.com/questionTerminal/695903546ba84d388fd80b1461a6141c?orderByHotValue=1&amp;page=1&amp;onlyReference=false 来源：牛客网 以下哪些方法可以取到http请求中的cookie值（）?A:request.getAttributeB:request.getHeaderC:request.getParameterD:request.getCookies答案：B DA. request.getAttribute:getAttribute是在服务器端的操作。 比如说 request.setAttribute(k,v),其行为动作在服务器端。 而在服务端放入cookies是通过response.addCookie(cookie)。因此，A错了 B. Accept 浏览器可接受的MIME类型 Accept-Charset 浏览器支持的字符编码 Accept-Encoding 浏览器知道如何解码的数据编码类型(如 gzip)。Servlets 可以预先检查浏览器是否支持gzip并可以对支持gzip的浏览器返回gzipped的HTML页面，并设置Content-Encoding回应头(response header)来指出发送的内容是已经gzipped的。在大多数情况下，这样做可以加快网页下载的速度。 Accept-Language 浏览器指定的语言，当Server支持多语种时起作用。 Authorization 认证信息，一般是对服务器发出的WWW-Authenticate头的回应。 Connection 是否使用持续连接。如果servlet发现这个字段的值是Keep-Alive，或者由发出请求的命令行发现浏览器支持 HTTP 1.1 (持续连接是它的默认选项)，使用持续连接可以使保护很多小文件的页面的下载时间减少。 Content-Length (使用POST方法提交时，传递数据的字节数) Cookie (很重要的一个Header，用来进行和Cookie有关的操作，详细的信息将在后面的教程中介绍) Host (主机和端口) If-Modified-Since (只返回比指定日期新的文档，如果没有，将会反回304 “Not Modified”) Referer (URL) User-Agent (客户端的类型，一般用来区分不同的浏览器) C.request.getParameter()方法获取从客户端中通过get 或者post方式传送到服务器端的参数。行为操作在服务器端。所以cookies明显不是通过url或者form表单提交过来的。C错 D.看方法名字就行了。]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[一次无语的调试经历]]></title>
    <url>%2F2019%2F03%2F01%2F%E4%B8%80%E6%AC%A1%E6%97%A0%E8%AF%AD%E7%9A%84%E8%B0%83%E8%AF%95%E7%BB%8F%E5%8E%86%2F</url>
    <content type="text"><![CDATA[前景概要 别人的项目；本人调试断断续续耗时两天， 因为不紧急所以心态没有爆炸，茫然大于忧虑，知道原因后只有苦笑要feign调用此服务的接口，死活都无法调用成功, resTemplate调用也是无济于事，瞬间想到没注册到eureka，或者可能服务名写错了，转头去注册中心的页面查看，结果此子项目是存在的，然后试着直接调用这个子项目的接口看是否有返回值，结果是有的，但是狗血的是返回值是XML格式的，倍感诧异，此服务的配置文件跟其他正常服务的配置也差不多，整体结构也基本一样，除了业务代码的差异基本也没有差别 原因 对方的 org.springframework.cloudgroupId> spring-cloud-starter-eureka-serverartifactId> dependency> 我方的 org.springframework.cloudgroupId> spring-cloud-starter-eurekaartifactId> dependency> 总结 拿到别人的模块后不要怀疑自己后写的代码，找找原先代码里面那些傻X操作。从中也算能总结出一点道理：依赖有的copy就copy，有公共模块就用公共的]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[线程池创建规范（摘自阿里java开发手册）]]></title>
    <url>%2F2019%2F02%2F25%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%88%9B%E5%BB%BA%E8%A7%84%E8%8C%83%EF%BC%88%E6%91%98%E8%87%AA%E9%98%BF%E9%87%8Cjava%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[线程池不允许使用Executors去创建，而是通过ThreadPoolExecutor的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。 说明：Executors各个方法的弊端： 1）newFixedThreadPool和newSingleThreadExecutor: 主要问题是堆积的请求处理队列可能会耗费非常大的内存，甚至OOM。 2）newCachedThreadPool和newScheduledThreadPool: 主要问题是线程数最大数是Integer.MAX_VALUE，可能会创建数量非常多的线程，甚至OOM。 Positive example 1： //org.apache.commons.lang3.concurrent.BasicThreadFactory ScheduledExecutorService executorService = new ScheduledThreadPoolExecutor(1, new BasicThreadFactory.Builder().namingPattern(&quot;example-schedule-pool-%d&quot;).daemon(true).build());Positive example 2： ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build(); //Common Thread Pool ExecutorService pool = new ThreadPoolExecutor(5, 200,0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy()); pool.execute(()-&gt; System.out.println(Thread.currentThread().getName())); pool.shutdown();//gracefully shutdownPositive example 3： &lt;bean id=&quot;userThreadPool&quot; class=&quot;org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor&quot;&gt; &lt;property name=&quot;corePoolSize&quot; value=&quot;10&quot; /&gt; &lt;property name=&quot;maxPoolSize&quot; value=&quot;100&quot; /&gt; &lt;property name=&quot;queueCapacity&quot; value=&quot;2000&quot; /&gt; &lt;property name=&quot;threadFactory&quot; value= threadFactory /&gt; &lt;property name=&quot;rejectedExecutionHandler&quot;&gt; &lt;ref local=&quot;rejectedExecutionHandler&quot; /&gt; &lt;/property&gt; &lt;/bean&gt; //in code userThreadPool.execute(thread);]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell教程目录]]></title>
    <url>%2F2019%2F02%2F14%2Fshell-e5-9f-ba-e7-a1-80-e5-ad-a6-e4-b9-a0-e7-9b-ae-e5-bd-95%2F</url>
    <content type="text"><![CDATA[学习本系列需要具有一定的Linux基础和vi编辑器基础，Shell学习难度不高，有其他语言学习经验的朋友学习起来会觉得语法简单易懂，注意一些语法的差异就能写出健壮的脚本。本系列学习笔记大量参考菜鸟教程 的目录结构和广大网友的智慧结晶，并对本系列学习笔记中的所有脚本进行测试，均为可运行实例。 Shell介绍和运行 Shell变量 Shell传递参数 Shell注释 Shell字符串 Shell数组 Shell运算符 Shell echo命令 Shell printf命令 Shell流程控制 Shell函数 Shell输入/输出重定向 Shell文件包含]]></content>
      <categories>
        <category>Shell教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell文件包含]]></title>
    <url>%2F2019%2F02%2F14%2Fshell-e6-96-87-e4-bb-b6-e5-8c-85-e5-90-ab%2F</url>
    <content type="text"><![CDATA[比如两个脚本：a脚本引用b脚本，a脚本可以使用b脚本中的变量 格式 . fileName #注意：.号和fileName中间有空格或者source fileName 举例说明 创建脚本test1.sh，写入如下内容 url=”https://blog.doeat.cn&quot; 创建脚本test2.sh，写入如下内容 #第一种格式 . ./test1.sh #第二种格式 #source ./test1.sh echo “我的博客地址：$url” 注：引用的文件不需要执行权限]]></content>
      <categories>
        <category>Shell教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell输入/输出重定向]]></title>
    <url>%2F2019%2F02%2F14%2FShell%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%87%8D%E5%AE%9A%E5%90%91%2F</url>
    <content type="text"><![CDATA[格式 command &gt; file 将输出重定向到 file command &lt; file 将输入重定向到 file command &gt;&gt; file 将输入追加的方式重定向到 file（不删除原先文件内容） n &gt; file 将文件描述为n的文件重定向到file n &gt;&gt; file 将文件描述为n的文件追加重定向到file n &gt;&amp; m 将输出文件m和n合并 文件描述符 0 通常是标准输入（STDIN） 1 是标准输出（STDOUT） 2 是标准错误输出（STDERR） 输出重定向 将一句话保存大file1文件中 echo “我要保存这句话到file1文件里面” &gt; file1 在保存另一句覆盖file1文件中的内容，发现之前保存的内容被覆盖了 echo “我要保存这第二句话到file1文件里面” &gt; file1 保存第三句追加在file1文件的末尾 echo “我要保存这第三句话到file1文件里面” &gt;&gt; file1 输入重定向 运行 ll 命令输入重定向到 ll.txt 文件 ll &gt; ll.txt 用wc -l命令统计行数 wc -l &lt; ll.txt 重定向深入学习 一般情况下，每个 Unix/Linux 命令运行时都会打开三个文件： 标准输入文件(stdin)：stdin的文件描述符为0，Unix程序默认从stdin读取数据。 标准输出文件(stdout)：stdout 的文件描述符为1，Unix程序默认向stdout输出数据。 标准错误文件(stderr)：stderr的文件描述符为2，Unix程序会向stderr流中写入错误信息。 默认情况下，command &gt; file 将 stdout 重定向到 file，command &lt; file 将stdin 重定向到 file。 如果希望 stderr 重定向到 file，如下： command 2 &gt; file 如果希望 stderr 追加到 file 文件末尾， 如下 ： command 2 &gt;&gt; file 2 表示标准错误文件(stderr) 如果希望将 stdout 和 stderr 合并后重定向到 file，如下： command &gt; file 2&gt;&amp;1或者command &gt;&gt; file 2&gt;&amp;1 如果希望对 stdin 和 stdout 都重定向，可以这样写： command &lt; file1 &gt;file2 /dev/null 文件 如果希望执行某个命令，但又不希望在屏幕上显示输出结果，那么可以将输出重定向到 /dev/null： command &gt; /dev/null /dev/null 是一个特殊的文件，写入到它的内容都会被丢弃；如果尝试从该文件读取内容，那么什么也读不到。但是 /dev/null 文件非常有用，将命令的输出重定向到它，会起到”禁止输出”的效果。 command &gt; /dev/null 2&gt;&amp;1]]></content>
      <categories>
        <category>Shell教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell函数]]></title>
    <url>%2F2019%2F02%2F14%2Fshell-e5-87-bd-e6-95-b0%2F</url>
    <content type="text"><![CDATA[格式直接举例说明#!/bin/bash #自定义函数；前面的function可以去掉 function doSomething(){ echo &quot;这是名为doSomething的函数&quot; } doSomething2(){ echo &quot;这是名为doSomething2的函数 &quot; } #调用函数直接写函数名即可 echo &quot;-----函数开始执行-----&quot; doSomething doSomething2 echo &quot;-----函数执行完毕-----&quot;执行结果： -----函数开始执行----- 这是名为doSomething的函数 这是名为doSomething2的函数 -----函数执行完毕-----带返回值的函数#!/bin/bash returnFunction(){ echo &quot;此函数计算两个数字之和&quot; echo &quot;请输入第一个数字:&quot; read num1 echo &quot;请输入第二个数字:&quot; read num2 echo &quot;两个数字分别是$num1 和 $num2&quot; return $(($num1+$num2)) } #调用函数通过 $? 获取返回值 returnFunction echo &quot;两数之和为 $? &quot;执行结果： 此函数计算两个数字之和 请输入第一个数字: 23 请输入第二个数字: 34 两个数字分别是23 和 34 两数字之和为 57函数接收参数#!/bin/bash getParamFunction(){ echo &quot;第一个参数为 $1&quot; echo &quot;第二个参数为 $2&quot; #$10这样无法获取第十个和大于第十个的参数，需要加上花括号 ${10} echo &quot;第十个参数为 $10&quot; echo &quot;第十个参数为 ${10}&quot; echo &quot;第十一个参数为 ${11}&quot; echo &quot;参数个数为 $#&quot; echo &quot;作为字符串输出所有参数 $*&quot; } getParamFunction 1 2 3 4 5 6 7 8 9 54 76执行结果： 第一个参数为 1 第二个参数为 2 第十个参数为 10 第十个参数为 54 第十一个参数为 76 参数个数为 11 作为字符串输出所有参数 1 2 3 4 5 6 7 8 9 54 76参数 作用 $# 传递到脚本的参数个数 $* 以一个单字符串显示所有向脚本传递的参数 $! 后台运行的最后一个进程的ID号 $@ 与$*相同，但是使用时加引号，并在引号中返回每个参数 $- 显示Shell使用的当前选项，与set命令功能相同 $? 显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误]]></content>
      <categories>
        <category>Shell教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[防止暴力破解服务器密码的脚本]]></title>
    <url>%2F2019%2F02%2F14%2F%E9%98%B2%E6%AD%A2%E6%9A%B4%E5%8A%9B%E7%A0%B4%E8%A7%A3%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AF%86%E7%A0%81%E7%9A%84%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[思路分析登录日志/var/log/secure中登录失败的记录，组成“ ip号=失败次数 ”这样的日志（如下图），然后记录登录失败超过10次的ip放入hosts.deny做黑名单处理，最后根据自身情况添加定时任务 脚本#!/bin/bash #防暴力破解密码 cat /var/log/secure | awk &apos;/Failed/{print $(NF-3)}&apos; | sort | uniq -c | awk &apos;{print $2&quot;=&quot;$1;}&apos; &gt; /tmp/denyhosts.txt DEFINE=10 for i in cat /tmp/denyhosts.txt do IP=echo $i | awk -F= &apos;{print $1}&apos; NUM=echo $i | awk -F= &apos;{print $2}&apos; if [ $NUM -gt $DEFINE ] then ipExists=grep $IP /etc/hosts.deny | grep -v grep | wc -l if [ $ipExists -lt 1 ] then echo &quot;sshd:$IP&quot; &gt;&gt; /etc/hosts.deny fi fi done添加定时任务编辑定时任务（crontab命令学习点击此处） #crontab -e每过半小时执行一次脚本（在每个小时的0分和30分执行，例如15:00,15:30,16:00,16:30……） 解释器/bin/bash,我的脚本在/root/denyHosts.sh。 0,30 * * * * /bin/bash /root/denyHosts.sh]]></content>
      <categories>
        <category>Linux安全</category>
        <category>Shell实例</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[crontab命令]]></title>
    <url>%2F2019%2F02%2F14%2Fcrontab%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[定时任务的管理 service crond status #查看定时任务状态 service crond start #开启定时任务 service crond stop #停止定时任务 service crond restart #重启定时任务 service crond reload #重新载入定时任务配置 查看已有定时任务列表 crontab -l 编辑定时任务 crontab -e 或者(下面方式编辑可以清楚看到每个位置参数代表含义) vim /etc/crontab 举例说明（只要写十来个例子就学会了，只是看和复制粘贴是永远学不会的） 每分钟执行(三种方式都行) * * * * * command*/1 * * * * command0-59 * * * * command 每天21点30执行 30 21 * * * command 每半小时执行 0,30 * * * * command 每小时的10分、45分执行 10,45 * * * * command 每天8点到10点的每10分和45分执行（ 8点10分、 8点45分、 9点10分、 9点45分、 10点10分、 10点45分 ） 10,45 8-10 * * * command 每隔5天的8点到10点的第10和第45分钟执行 10,45 8-10 */5 * * command 每个星期一的上午8点到10点的第10和第45分钟执行（两种方式，星期数字可用英文缩写代替） 10,45 8-11 * * 1 command10,45 8-11 * * mon command 每月7号、17号、27号的3点25分执行 25 3 7,17,27 * * command 每年3月、6月、9月、12月 的 3号到10号 的 2点45分执行 45 2 3-10 3,6,9,12 * command]]></content>
      <categories>
        <category>Linux命令</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[线程卡死无报错信息的调试]]></title>
    <url>%2F2019%2F02%2F14%2F%E7%BA%BF%E7%A8%8B%E5%8D%A1%E6%AD%BB%E6%97%A0%E6%8A%A5%E9%94%99%E4%BF%A1%E6%81%AF%E7%9A%84%E8%B0%83%E8%AF%95%2F</url>
    <content type="text"><![CDATA[问题 项目启动之后调用接口一段时间后，接口会报超时，所有接口都是等待状态，项目没挂，接口访问直到超时时间报错，此时项目日志没有报错信息， 网关只有超时报错，无法定位出问题的代码位置。 这种情况非常像是线程阻塞在卡死状态 。 定位 首先获取项目的PID ps -ef|grep java执行java的jstask工具导出日志 jstack -l 9902 &gt;&gt; dumpLog.txt查看日志 more dumpLog.txt截取的导出日志信息 &quot;http-nio-12026-exec-34&quot; #102 daemon prio=5 os_prio=0 tid=0x00007f50b4014000 nid=0x13147 waiting on condition [0x00007f51abdf9000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for &lt;0x00000005d03e6420&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at com.alibaba.druid.pool.DruidDataSource.takeLast(DruidDataSource.java:1617) at com.alibaba.druid.pool.DruidDataSource.getConnectionInternal(DruidDataSource.java:1227) at com.alibaba.druid.pool.DruidDataSource.getConnectionDirect(DruidDataSource.java:1095) at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:1075) at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:1065) at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:104) at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) at org.hibernate.internal.AbstractSessionImpl$NonContextualJdbcConnectionAccess.obtainConnection(AbstractSessionImpl.java:386) at org.hibernate.resource.jdbc.internal.LogicalConnectionManagedImpl.acquireConnectionIfNeeded(LogicalConnectionManagedImpl.java:87) at org.hibernate.resource.jdbc.internal.LogicalConnectionManagedImpl.getPhysicalConnection(LogicalConnectionManagedImpl.java:112) at org.hibernate.resource.jdbc.internal.LogicalConnectionManagedImpl.getConnectionForTransactionManagement(LogicalConnectionManagedImpl.java:230) at org.hibernate.resource.jdbc.internal.LogicalConnectionManagedImpl.begin(LogicalConnectionManagedImpl.java:237) at org.hibernate.resource.transaction.backend.jdbc.internal.JdbcResourceLocalTransactionCoordinatorImpl$TransactionDriverControlImpl.begin(JdbcResourceLocalTransactionCoordinatorImpl.java:214) at org.hibernate.engine.transaction.internal.TransactionImpl.begin(TransactionImpl.java:52) at org.hibernate.internal.SessionImpl.beginTransaction(SessionImpl.java:1512) at org.hibernate.jpa.internal.TransactionImpl.begin(TransactionImpl.java:45) at org.springframework.orm.jpa.vendor.HibernateJpaDialect.beginTransaction(HibernateJpaDialect.java:189) at org.springframework.orm.jpa.JpaTransactionManager.doBegin(JpaTransactionManager.java:380) at org.springframework.transaction.support.AbstractPlatformTransactionManager.getTransaction(AbstractPlatformTransactionManager.java:373) at org.springframework.transaction.interceptor.TransactionAspectSupport.createTransactionIfNecessary(TransactionAspectSupport.java:447) at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:277) at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:96) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:673) at com.dfmz.city.brainservicesensor.service.SensorService$$EnhancerBySpringCGLIB$$60f2fb6b.getSensorListNotFilter() at com.dfmz.city.brainservicesensor.controller.GisHomeController.getSensorListNotFilter(GisHomeController.java:89) at sun.reflect.GeneratedMethodAccessor284.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205) at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133) at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97) at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827) at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738) at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:963) at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:897) at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) at javax.servlet.http.HttpServlet.service(HttpServlet.java:635) at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) at javax.servlet.http.HttpServlet.service(HttpServlet.java:742) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:105) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:81) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:198) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96) at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:478) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:140) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:80) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:87) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342) at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:799) at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66) at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:861) at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1455) at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) - locked &lt;0x000000071d20a240&gt; (a org.apache.tomcat.util.net.NioEndpoint$NioSocketWrapper) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:748) Locked ownable synchronizers: - &lt;0x000000071eb90ea0&gt; (a java.util.concurrent.ThreadPoolExecutor$Worker)解决 根据导出的日志能看到出问题的地方在GisHomeController.java的89行，修改问题代码即可]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell流程控制]]></title>
    <url>%2F2019%2F02%2F12%2Fshell-e6-b5-81-e7-a8-8b-e6-8e-a7-e5-88-b6%2F</url>
    <content type="text"><![CDATA[if else 语句 如果没有判断条件不要写，比如else后面不需要执行任何命令，在其他语言例如java、php中不写不会报错，但是shell中不允许 单分支 if 条件 then 执行代码 fi可以写做一行只要每个语句后面加上分号“ ; ”就可以，如下 if 条件;then 执行代码;fi双分支 if 条件 then 执行代码 else 执行代码 fi多分支 if 条件 then 执行代码 elif 条件2 then 执行代码 else 执行代码 fi例子：判断两个变量是否相等 a=1 b=2 if [ $a == $b ] then echo &quot;a 等于 b&quot; elif [ $a -gt $b ] then echo &quot;a 大于 b&quot; elif [ $a -lt $b ] then echo &quot;a 小于 b&quot; else echo &quot;没有符合的条件&quot; fi 输出结果： a 小于 bcase语句 case 变量引用 in 条件1) 分支1 ;; 条件2) 分支2 ;; esac例子： echo 请输入1，2，3任一数字： echo 您输入的数字为： read num case $num in 1) echo 您输入了1 ;; 2) echo 您输入了2 ;; 3) echo 您输入了3 ;; *) echo 请输入1,2,3中的任一数字 ;; esacfor循环 #第一种格式 for i in 1 2 3 4 5 do echo &quot;第一种格式:$i&quot; done #第二种格式 for((i=1;i&lt;=5;i++)) do echo &quot;第二种格式:$i&quot; done执行结果： 第一种格式：1 第一种格式：2 第一种格式：3 第一种格式：4 第一种格式：5 第二种格式：1 第二种格式：2 第二种格式：3 第二种格式：4 第二种格式：5while语句 var=1 while(($var&lt;=5)) do echo $var let var++ done执行结果： 1 2 3 4 5无限循环 #第一种 while : do 执行代码 done #第二种 while true do 执行代码 done #第三种 for((;;)) do 执行代码 doneuntil循环 循环判断直到条件满足时停止，与while类似 a=0 echo &quot;以下为until执行结果&quot; until [ ! $a -lt 5 ] do echo $a a=`expr $a + 1` done #下面用while方式的代码和until执行结果完全相同 echo &quot;以下为while执行结果&quot; b=0 while(($b&lt;5)) do echo $b b=`expr $b + 1` done执行结果： 以下为until执行结果 0 1 2 3 4 以下为while执行结果 0 1 2 3 4循环控制break和continue break退出循环 continue继续当前轮的循环]]></content>
      <categories>
        <category>Shell教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SS连不上，能ping通，IP没封]]></title>
    <url>%2F2019%2F02%2F06%2FSS%E8%BF%9E%E4%B8%8D%E4%B8%8A%EF%BC%8C%E8%83%BDping%E9%80%9A%EF%BC%8CIP%E6%B2%A1%E5%B0%81%2F</url>
    <content type="text"><![CDATA[状况： 大年初一，ssr连不上，本以为喜闻乐见的封 IP，但是ssh能连上，也ping得通，一时间也没有搜到解决办法，感觉身陷囹圄，无法理解此般状况，便重装了系统，继而又按照之前的配置重装并设置了ssr，都无济于事···· 解决： 换一个ssr的连接端口号····]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java代码SSH连接服务器执行脚本找不到java命令]]></title>
    <url>%2F2019%2F01%2F29%2FJava%E4%BB%A3%E7%A0%81SSH%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC%E6%89%BE%E4%B8%8D%E5%88%B0java%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[出现的问题用的JSchUtil工具类，执行服务器中已存在的脚本（脚本主要是执行java -jar运行jar包的）；会报找不到java命令 解决方法只需要运行脚本之前刷新一下环境变量就行 source /etc/profile]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java启动项目自定义JVM占用内存大小]]></title>
    <url>%2F2019%2F01%2F26%2FJava%E5%90%AF%E5%8A%A8%E9%A1%B9%E7%9B%AE%E8%87%AA%E5%AE%9A%E4%B9%89JVM%E5%8D%A0%E7%94%A8%E5%86%85%E5%AD%98%E5%A4%A7%E5%B0%8F%2F</url>
    <content type="text"><![CDATA[待整理 参数的含义 -vmargs -Xms128M -Xmx512M -XX:PermSize=64M -XX:MaxPermSize=128M -vmargs 说明后面是VM的参数，所以后面的都是JVM的参数了 -Xms128m JVM初始分配的堆内存 -Xmx512m JVM最大允许分配的堆内存，按需分配 -XX:PermSize=64M JVM初始分配的非堆内存 -XX:MaxPermSize=128M JVM最大允许分配的非堆内存，按需分配 -Xms128m JVM初始分配的堆内存 -Xmx256m JVM最大允许分配的堆内存，按需分配 -Xmn256m 年轻代大小 -XX:MaxNewSize 年轻代最大值 -Xss256k 每个线程的堆栈大小 -XX:ReservedCodeCacheSize=64m -XX:+UseParallelGC -XX:+UseParallelOldGC -XX:ParallelGCThreads=4 查看进程的总线程数，正常情况下线程数为平均值的75%。所以在偶发高峰期是很有可能达到极限的值。 ps hH p &lt;pid&gt; | wc -l]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell获取执行的脚本的所在路径]]></title>
    <url>%2F2019%2F01%2F24%2FShell%E8%8E%B7%E5%8F%96%E6%89%A7%E8%A1%8C%E7%9A%84%E8%84%9A%E6%9C%AC%E7%9A%84%E6%89%80%E5%9C%A8%E8%B7%AF%E5%BE%84%2F</url>
    <content type="text"><![CDATA[1basepath=$(cd `dirname $0`; pwd)]]></content>
      <categories>
        <category>Shell实例</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell printf命令]]></title>
    <url>%2F2019%2F01%2F23%2Fshell-printf-e5-91-bd-e4-bb-a4%2F</url>
    <content type="text"><![CDATA[语法 printf format-string [arguments…] 参数说明 format-string: 为格式控制字符串arguments: 为参数列表。 printf “%-10s %-8s %-4s\n” 姓名 性别 体重kg printf “%-10s %-8s %-4.2f\n” 郭靖 男 66.1234 printf “%-10s %-8s %-4.2f\n” 杨过 男 48.6543 printf “%-10s %-8s %-4.2f\n” 郭芙 女 47.9876 执行结果： 姓名 性别 体重kg 郭靖 男 66.12 杨过 男 48.65 郭芙 女 47.99 每个%号代表后面一个一个参数%s %c %d %f都是格式替代符%-10s 指一个宽度为10个字符（-表示左对齐，没有则表示右对齐），任何字符都会被显示在10个字符宽的字符内，如果不足则自动以空格填充，超过也会将内容全部显示出来。%-4.2f 指格式化为小数，其中.2指保留2位小数。 printf转义序列 \a 警告字符，通常为ASCII的BEL字符\b 后退\c 抑制（不显示）输出结果中任何结尾的换行字符（只在%b格式指示符控制下的参数字符串中有效），而且，任何留在参数里的字符、任何接下来的参数以及任何留在格式字符串中的字符，都被忽略\f 换页（formfeed）\n 换行\r 回车（Carriage return）\t 水平制表符\v 垂直制表符\\ 一个字面上的反斜杠字符\ddd 表示1到3位数八进制值的字符。仅在格式字符串中有效\0ddd 表示1到3位的八进制值字符]]></content>
      <categories>
        <category>Shell教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell echo命令]]></title>
    <url>%2F2019%2F01%2F23%2Fshell-echo-e5-91-bd-e4-bb-a4%2F</url>
    <content type="text"><![CDATA[功能说明 显示文字。 语 法 echo [-ne][字符串] / echo [–help][–version] 补充说明 echo会将输入的字符串送往标准输出。输出的字符串间以空白字符隔开, 并在最后加上换行号。 参 数 -n 不要在最后自动换行-e 打开反斜杠ESC转义。若字符串中出现以下字符，则特别加以处理，而不会将它当成一般文字输出： \a 发出警告声； \b 删除前一个字符； \c 最后不加上换行符号； \f 换行但光标仍旧停留在原来的位置； \n 换行且光标移至行首； \r 光标移至行首，但不换行； \t 插入tab； \v 与\f相同； \\ 插入\字符； \nnn 插入nnn（八进制）所代表的ASCII字符；-E 取消反斜杠ESC转义 (默认)-help 显示帮助-version 显示版本信息 举例说明 显示普通字符串和转义 echo “It\‘s a test” #反斜杠转义echo It\‘s a test #不加双引号同上 执行结果：It’s a testIt’s a test 显示变量 read name #read命令从标准输入读取一行，很有意思一定要尝试一下echo $name It is a test 执行命令：$./test.shOK #输入OK 执行结果：OK It is a test #输出 显示换行 echo -e “OK! \n” # -e 开启转义echo “It is a test” 执行结果：OK! It is a test 显示不换行 echo -e “OK! \c” # -e 开启转义 \c 不换行echo “It is a test” 执行结果：OK! It is a test 显示结果定向到文件 echo “It is a test” &gt; myfile 显示命令执行结果（加反引号） echo `date` 执行结果：Wed Jan 23 17:33:08 CST 2019]]></content>
      <categories>
        <category>Shell教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell运算符]]></title>
    <url>%2F2019%2F01%2F23%2Fshell-e8-bf-90-e7-ae-97-e7-ac-a6%2F</url>
    <content type="text"><![CDATA[原生bash不支持简单的数学运算，但是可以通过其他命令来实现，例如 awk 和 expr，expr 最常用。 命令的执行需要用反引号括起来，例如：echo `expr 1 + 1` 运算符两遍都要有空格隔开,必须带空格写成 1 + 1，不能连起来写成1+1 算数运算符a=3 b=6 echo a+b=`expr $a + $b` #加法 echo a-b=`expr $a - $b` #减法 echo a*b=`expr $a \* $b` #乘法 echo a/b=`expr $b / $a` #除法 echo a%b=`expr $a % $b` #取余数 c=$b #等号赋值 echo c=$c if [ $a == $b ] #双等号判断两遍是否相等返回boolean值true或false then echo a 等于 b fi if [ $a != $b ] # “!=”号判断两遍值是否不相等返回boolean值true或false then echo a 不等于 b fi执行结果 a+b=9 a-b=-3 a*b=18 a/b=2 a%b=3 c=6 a 不等于 b关系运算符关系运算符只支持数字，不支持字符串，除非字符串的值是数字。 参数 作用 -eq 检测两个数是否 相等 ，相等返回 true。 -ne 检测两个数是否 不相等 ，不相等返回 true。 -gt 检测左边的数是否 大于 右边的，如果是，则返回 true。 -lt 检测左边的数是否 小于 右边的，如果是，则返回 true。 -ge 检测左边的数是否 大于等于 右边的，如果是，则返回 true。 -le 检测左边的数是否 小于等于 右边的，如果是，则返回 true。 a=3 b=6 if [ $a -eq $b ] then echo &quot;$a -eq $b : a 等于 b&quot; else echo &quot;$a -eq $b: a 不等于 b&quot; fi if [ $a -ne $b ] then echo &quot;$a -ne $b: a 不等于 b&quot; else echo &quot;$a -ne $b : a 等于 b&quot; fi if [ $a -gt $b ] then echo &quot;$a -gt $b: a 大于 b&quot; else echo &quot;$a -gt $b: a 不大于 b&quot; fi if [ $a -lt $b ] then echo &quot;$a -lt $b: a 小于 b&quot; else echo &quot;$a -lt $b: a 不小于 b&quot; fi if [ $a -ge $b ] then echo &quot;$a -ge $b: a 大于或等于 b&quot; else echo &quot;$a -ge $b: a 小于 b&quot; fi if [ $a -le $b ] then echo &quot;$a -le $b: a 小于或等于 b&quot; else echo &quot;$a -le $b: a 大于 b&quot; fi执行结果 3 -eq 6: a 不等于 b 3 -ne 6: a 不等于 b 3 -gt 6: a 不大于 b 3 -lt 6: a 小于 b 3 -ge 6: a 小于 b 3 -le 6: a 小于或等于 b布尔运算符参数 作用 ! 非运算，表达式为 true 则返回 false，否则返回 true -o 或运算，有一个表达式为 true 则返回 true。 -a 与运算，两个表达式都为 true 才返回 true。 a=3 b=6 if [ $a != $b ] then echo &quot;$a != $b : a 不等于 b&quot; else echo &quot;$a != $b: a 等于 b&quot; fi if [ $a -lt 10 -a $b -gt 5 ] then echo &quot;$a 小于 10 且 $b 大于 5 : 返回 true&quot; else echo &quot;$a 小于 10 且 $b 大于 5 : 返回 false&quot; fi if [ $a -lt 10 -o $b -gt 10 ] then echo &quot;$a 小于 10 或 $b 大于 10 : 返回 true&quot; else echo &quot;$a 小于 10 或 $b 大于 10 : 返回 false&quot; fi if [ $a -lt 2 -o $b -gt 10 ] then echo &quot;$a 小于 2 或 $b 大于 10 : 返回 true&quot; else echo &quot;$a 小于 2 或 $b 大于 10 : 返回 false&quot; fi执行结果 3 != 6 : a 不等于 b 3 小于 10 且 6 大于 5 : 返回 true 3 小于 10 或 6 大于 10 : 返回 true 3 小于 2 或 6 大于 10 : 返回 false逻辑运算符参数 作用 &amp;&amp; 逻辑的 AND || 逻辑的 OR if [[ $a -lt 10 &amp;&amp; $b -gt 10 ]] then echo &quot;返回 true&quot; else echo &quot;返回 false&quot; fi if [[ $a -lt 10 || $b -gt 10 ]] then echo &quot;返回 true&quot; else echo &quot;返回 false&quot; fi执行结果 返回 false 返回 true字符串运算符参数 作用 = 检测两个字符串是否相等，相等返回 true。 != 检测两个字符串是否相等，不相等返回 true。 -z 检测字符串长度是否为0，为0返回 true。 -n 检测字符串长度是否为0，不为0返回 true。 str 检测字符串是否为空，不为空返回 true。 a=&quot;abc&quot; b=&quot;def&quot; if [ $a = $b ] then echo &quot;$a = $b : a 等于 b&quot; else echo &quot;$a = $b: a 不等于 b&quot; fi if [ $a != $b ] then echo &quot;$a != $b : a 不等于 b&quot; else echo &quot;$a != $b: a 等于 b&quot; fi if [ -z $a ] then echo &quot;-z $a : 字符串长度为 0&quot; else echo &quot;-z $a : 字符串长度不为 0&quot; fi if [ -n $a ] then echo &quot;-n $a : 字符串长度不为 0&quot; else echo &quot;-n $a : 字符串长度为 0&quot; fi if [ $a ] then echo &quot;$a : 字符串不为空&quot; else echo &quot;$a : 字符串为空&quot;执行结果 abc = def: a 不等于 b abc != def : a 不等于 b -z abc : 字符串长度不为 0 -n abc : 字符串长度不为 0 abc : 字符串不为空文件测试运算符参数 作用 -b file 检测文件是否是块设备文件，如果是，则返回 true。 -c file 检测文件是否是字符设备文件，如果是，则返回 true。 -d file 检测文件是否是目录，如果是，则返回 true。 -f file 检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。 -g file 检测文件是否设置了 SGID 位，如果是，则返回 true。 -k file 检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。 -p file 检测文件是否是有名管道，如果是，则返回 true。 -u file 检测文件是否设置了 SUID 位，如果是，则返回 true。 -r file 检测文件是否可读，如果是，则返回 true。 -w file 检测文件是否可写，如果是，则返回 true。 -x file 检测文件是否可执行，如果是，则返回 true。 -s file 检测文件是否为空（文件大小是否大于0），不为空返回 true。 -e file 检测文件（包括目录）是否存在，如果是，则返回 true。 file=&quot;test.sh&quot; if [ -r $file ] then echo &quot;文件可读&quot; else echo &quot;文件不可读&quot; fi if [ -w $file ] then echo &quot;文件可写&quot; else echo &quot;文件不可写&quot; fi if [ -x $file ] then echo &quot;文件可执行&quot; else echo &quot;文件不可执行&quot; fi if [ -f $file ] then echo &quot;文件为普通文件&quot; else执行结果 文件可读 文件可写 文件可执行 文件为普通文件 文件不是个目录 文件不为空 文件存在]]></content>
      <categories>
        <category>Shell教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell字符串]]></title>
    <url>%2F2019%2F01%2F23%2Fshell-e5-ad-97-e7-ac-a6-e4-b8-b2-ef-bb-bf%2F</url>
    <content type="text"><![CDATA[单引号 所见即所得，单引号中的内容会原封不动的输出 双引号 输出双引号内的所有内容；如果内容中有命令(要反引下)、变量、特殊转义符等，会先把变量、命令、转义字符解析出结果，然后再输出最终内容，推荐使用，这称为弱引用 反引号（键盘左上角英文模式 Esc下面的波浪键） 执行命令,例如echo “`ls`“ 会执行ls命令 my_name=&quot;Charlie&quot; echo &apos;$my_name is my name&apos; echo &quot;$my_name is my name&quot; 执行脚本输出： $my_name is my name Charlie is my name获取字符串长度 str=&quot;abcd&quot; echo ${#str} 执行脚本输出： 4查找字符串中某字符的位置(下面例子ef两个字母，哪个字母先出现就计算哪个 ) str=&quot;abcdefg&quot; echo `expr index $str ef` #此处为反引号 执行脚本输出： 5字符串的各种截取方式（复制下面整段自行执行查看，或直接看下文的执行结果） var=https://blog.doeat.cn/index.php echo &quot;变量:var=${var}&quot; printf &quot;\n&quot; echo &apos;执行命令：echo ${var#*o}&apos; echo 输出结果：${var#*/} echo &apos;命令解释：#*/表示删除第一次出现包含/的左边的所有字符&apos; printf &quot;\n&quot; command=##*/ echo 执行命令：&apos;echo ${var&apos;$command&apos;}&apos; #echo 输出结果：${var${command}} #shell没有嵌套变量的这种写法 eval echo 输出结果：&apos;${var&apos;$command&apos;}&apos; echo &apos;命令解释：##*/表示删除最后出现包含/的左边的所有字符&apos; printf &quot;\n&quot; command=%/* echo 执行命令：&apos;echo ${var&apos;$command&apos;}&apos; eval echo 输出结果：&apos;${var&apos;$command&apos;}&apos; echo &apos;命令解释：%/*表示删除最后出现包含/的右边的所有字符&apos; printf &quot;\n&quot; command=%%/* echo 执行命令：&apos;echo ${var&apos;$command&apos;}&apos; eval echo 输出结果：&apos;${var&apos;$command&apos;}&apos; echo &apos;命令解释：%%/*表示删除第一次出现包含/的右边的所有字符&apos; printf &quot;\n&quot; command=:0:5 echo 执行命令：&apos;echo ${var&apos;$command&apos;}&apos; eval echo 输出结果：&apos;${var&apos;$command&apos;}&apos; echo &apos;命令解释：0表示从左边算起第一个字符即截取的开始位置，5表示截取的字符个数，即从第一个字符开始往后截取5个字符&apos; printf &quot;\n&quot; command=:7 echo 执行命令：&apos;echo ${var&apos;$command&apos;}&apos; eval echo 输出结果：&apos;${var&apos;$command&apos;}&apos; echo &apos;命令解释：7表示从左边算起第八个字符，即从第八个字符开始往后截取到字符串结束位置&apos; printf &quot;\n&quot; command=:0-9:5 echo 执行命令：&apos;echo ${var&apos;$command&apos;}&apos; eval echo 输出结果：&apos;${var&apos;$command&apos;}&apos; echo &apos;命令解释：0-9表示从右边算起第十个字符位置，5表示截取的字符个数，即从倒数第十个字符位置开始往后截取5个字符&apos; printf &quot;\n&quot; command=:0-9 echo 执行命令：&apos;echo ${var&apos;$command&apos;}&apos; eval echo 输出结果：&apos;${var&apos;$command&apos;}&apos; echo &apos;命令解释：0-9表示从右边算起第十个字符，即从倒数第十个字符开始往后截取到结束位置&apos;执行结果： 变量:var=https://blog.doeat.cn/index.php 执行命令：echo ${var#*o} 输出结果：/blog.doeat.cn/index.php 命令解释：#*/表示删除第一次出现包含/的左边的所有字符 执行命令：echo ${var##*/} 输出结果：index.php 命令解释：##*/表示删除最后出现包含/的左边的所有字符 执行命令：echo ${var%/*} 输出结果：https://blog.doeat.cn 命令解释：%/*表示删除最后出现包含/的右边的所有字符 执行命令：echo ${var%%/*} 输出结果：https: 命令解释：%%/*表示删除第一次出现包含/的右边的所有字符 执行命令：echo ${var:0:5} 输出结果：https 命令解释：0表示从左边算起第一个字符即截取的开始位置，5表示截取的字符个数，即从第一个字符开始往后截取5个字符 执行命令：echo ${var:7} 输出结果：/blog.doeat.cn/index.php 命令解释：7表示从左边算起第八个字符，即从第八个字符开始往后截取到字符串结束位置 执行命令：echo ${var:0-9:5} 输出结果：index 命令解释：0-9表示从右边算起第十个字符位置，5表示截取的字符个数，即从倒数第十个字符位置开始往后截取5个字符 执行命令：echo ${var:0-9} 输出结果：index.php 命令解释：0-9表示从右边算起第十个字符，即从倒数第十个字符开始往后截取到结束位置]]></content>
      <categories>
        <category>Shell教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell注释]]></title>
    <url>%2F2019%2F01%2F23%2Fshell-e6-b3-a8-e9-87-8a-ef-bb-bf%2F</url>
    <content type="text"><![CDATA[Shell注释 #这是井号单行注释 #这是井号单行注释 #这是井号单行注释 :&lt;&lt;!这是多行注释这是多行注释这是多行注释这是多行注释这是多行注释这是多行注释!]]></content>
      <categories>
        <category>Shell教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell传递参数]]></title>
    <url>%2F2019%2F01%2F23%2Fshell-e4-bc-a0-e9-80-92-e5-8f-82-e6-95-b0%2F</url>
    <content type="text"><![CDATA[在执行shell脚本时可以向脚本内传递参数，脚本中可以通过“ $n”（n是数字）获取到传递的参数 编辑脚本 $ vim test.sh添加以下脚本内容 echo $1 #执行脚本传递的第一个参数 echo $2 #执行脚本传递的第二个参数 echo $3 #执行脚本传递的第三个参数 echo $# #执行脚本传递参数的个数 echo $* #以单个字符串显示所有参数 echo $@ #与$*相同，但是使用时加引号，并在引号中返回每个参数 echo $- #显示Shell使用的当前选项，与set命令功能相同 echo $? #显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误执行命令 $ ./test.sh var1 var2 var3执行脚本结果 var1 var2 var3 3 var1 var2 var3 var1 var2 var3 hB 0 两种获取所有参数的异同点 脚本中添加如下内容 #$*和$@的区别 #相同点：都是引用所有参数。 #不同点：只有在双引号中体现出来。假设在脚本运行时写了三个参数 1、2、3，，则 &quot; * &quot; 等价于 &quot;1 2 3&quot;（传递了一个参数），而 &quot;@&quot; 等价于 &quot;1&quot; &quot;2&quot; &quot;3&quot;（传递了三个参数）。 echo &apos;$* 演示&apos; for i in &quot;$*&quot;;do echo $i done echo &apos;$@ 演示&apos; for i in &quot;$@&quot;;do echo $i done执行命令 $ ./test.sh var1 var2 var3执行脚本结果 $* 演示 var1 var2 var3 $@ 演示 var1 var2 var3]]></content>
      <categories>
        <category>Shell教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell变量]]></title>
    <url>%2F2019%2F01%2F23%2Fshell-e5-8f-98-e9-87-8f%2F</url>
    <content type="text"><![CDATA[定义变量 my_name=”Charlie” #双引号可以不加，#号后面为注释内容 变量等号两边不能有空格命名只能使用英文字母，数字和下划线，首个字符不能以数字开头不能使用bash关键字 使用变量 只需要在变量名之前加上$符号就可以使用 my_name=”Charile”echo $my_nameecho ${my_name} 使用变量的花括号是可选的，花括号是表示边界，例如下面这种情况：如果不加花括号限定边界那变量就会是 $languageScript,没有这个变量 language=”Java”echo ${language}Script 变量可以重复赋值，使用变量是找使用处往上找的最近的一次赋值 my_name=”Charlie”echo $my_namemy_name=”Aiguoba”echo $my_name 只读变量，使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。 my_name=”Charlie”readonly my_namemy_name=”Aiguoba” 运行这个脚本会报错 my_name : readonly variable 删除变量 变量被删除后不能再次使用。unset 命令不能删除只读变量。下面脚本运行没有任何输出 my_name=”Charlie”unset my_nameecho $my_name]]></content>
      <categories>
        <category>Shell教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell介绍和运行]]></title>
    <url>%2F2019%2F01%2F23%2Fshell-e4-bb-8b-e7-bb-8d-e5-92-8c-e8-bf-90-e8-a1-8c%2F</url>
    <content type="text"><![CDATA[Shell介绍 参考百度百科吧；shell 是指一种应用程序,但一般我们说的shell指的是shell脚本 第一个shell脚本 例如添加一个test.sh脚本 vim test.sh 输出”Hello World” (vi或者vim编辑器的使用不再赘述) #!/bin/bashecho “Hello World !” 运行方式 一.bash运行 bash test.sh 二.赋权执行 给脚本赋权 chmod +x test.sh 执行脚本 ./test.sh]]></content>
      <categories>
        <category>Shell教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[maven打包指定包名]]></title>
    <url>%2F2019%2F01%2F23%2Fmaven%E6%89%93%E5%8C%85%E6%8C%87%E5%AE%9A%E5%8C%85%E5%90%8D%2F</url>
    <content type="text"><![CDATA[在&lt;build&gt;标签中添加&lt;finalName&gt;标签指定打包的包名123456789&lt;build&gt; &lt;finalName&gt;brain-system-status&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt;]]></content>
      <categories>
        <category>maven</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[Err] 1235 - This version of MySQL doesn't yet support 'LIMIT & IN/ALL/ANY/SOME subquery']]></title>
    <url>%2F2019%2F01%2F21%2Ferr-1235-this-version-of-mysql-doesnt-yet-support-limit-in-all-any-some-subquery%2F</url>
    <content type="text"><![CDATA[IN/ALL/ANY/SOME 的子查询中用了limit关键词无法执行 1select * from table\_example where id in (select t.id from table\_example_b t limit 5) 这样执行会报错 1_\[Err\] 1235 - This version of MySQL doesn&apos;t yet support &apos;LIMIT &amp; IN/ALL/ANY/SOME subquery&apos;_ 只需要在in里面在包一层select即可解决问题 1select * from table\_example where id in (select t.id (select id from table\_example_b limit 5) t)]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux批量删除进程（以java进程举例）]]></title>
    <url>%2F2019%2F01%2F21%2FLinux%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4%E8%BF%9B%E7%A8%8B%EF%BC%88%E4%BB%A5java%E8%BF%9B%E7%A8%8B%E4%B8%BE%E4%BE%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Linux查看所有Java进程 (“grep -v grep”是在列出的进程中去除含有关键字”grep”的进程) ps -ef | grep java | grep -v grep 使用awk分割结果，获取PID awk ‘{print $2}’ ps -ef | grep java | grep -v grep | awk ‘{print $2}’ 杀死进程命令 kill -9 PID xargs 作用是将参数列表转换成小块分段传递给其他命令，以避免参数列表过长的问题 ps -ef | grep java | grep -v grep | awk ‘{print $2}’ | xargs kill -9]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[限制服务器用户登录]]></title>
    <url>%2F2019%2F01%2F15%2F%E9%99%90%E5%88%B6%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%94%A8%E6%88%B7%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[查看IP尝试登录次数脚本 1cat /var/log/secure|awk '/Failed/&#123;print $(NF-3)&#125;'|sort|uniq -c|awk '&#123;print $2" = "$1;&#125;' 直接查看尝试登录的日志 1tail -fn500 /var/ogs/secure 限制某个ip的访问（编辑/etc/hosts.deny,添加限制IP信息） 1vim /etc/hosts.deny 限制192.168.11.112的访问 1sshd:192.168.11.112 直接用通配符限制某个网段 1sshd:192.168.1.* 限制登录次数 1vim /etc/ssh/sshd_config 设置验证超过10次断开连接 1MaxAuthTries=10]]></content>
      <categories>
        <category>Linux</category>
        <category>Linux安全</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spring cloud中使用Ribbon实现客户端的软负载均衡]]></title>
    <url>%2F2019%2F01%2F10%2Fspring%20cloud%E4%B8%AD%E4%BD%BF%E7%94%A8Ribbon%E5%AE%9E%E7%8E%B0%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E8%BD%AF%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/liuchuanhong1/article/details/54691566]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7 开放端口]]></title>
    <url>%2F2019%2F01%2F09%2FCentOS%207%20%E5%BC%80%E6%94%BE%E7%AB%AF%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[查看到zone名称 firewall-cmd –get-active-zones 添加开放端口（public为上一步查出的zone名称） firewall-cmd –zone=public –add-port=6379/tcp –permanent 重启防火墙 firewall-cmd –reload 查看端口开启状态 firewall-cmd –query-port=6379/tcp]]></content>
      <categories>
        <category>Linux</category>
        <category>Linux安全</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java并行执行任务]]></title>
    <url>%2F2019%2F01%2F03%2FJava%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839/** * 并行执行任务 */public class TestFuture &#123; @Test public void test()&#123; CompletableFuture&lt;String&gt; future1 = CompletableFuture.supplyAsync(() -&gt; method1()); CompletableFuture&lt;String&gt; future2 = CompletableFuture.supplyAsync(() -&gt; method2()); try &#123; System.out.println(future1.get()); System.out.println(future2.get()); &#125;catch (Exception e)&#123; &#125; &#125; public String method1()&#123; try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return "method1"; &#125; private String method2()&#123; try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return "method2"; &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data truncation: Incorrect datetime value: ” for column ‘create_time’ at row 1]]></title>
    <url>%2F2018%2F12%2F28%2F%E6%8A%A5%E9%94%99%EF%BC%9AData%20truncation%20Incorrect%C2%B7%C2%B7%C2%B7%2F</url>
    <content type="text"><![CDATA[报错：1'Data truncation: Incorrect datetime value: ” for column ‘create_time’ at row 1' 换mysql-connector-java依赖包版本从5.0.4换到5.1.35]]></content>
      <categories>
        <category>Java</category>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[服务注册发现consul之一：consul介绍、安装、及功能介绍]]></title>
    <url>%2F2018%2F12%2F10%2F%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E5%8F%91%E7%8E%B0consul%E4%B9%8B%E4%B8%80%EF%BC%9Aconsul%E4%BB%8B%E7%BB%8D%E3%80%81%E5%AE%89%E8%A3%85%E3%80%81%E5%8F%8A%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/duanxz/p/7053301.html]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[windows下 安装 rabbitMQ 及操作常用命令]]></title>
    <url>%2F2018%2F12%2F10%2Fwindows%E4%B8%8B%20%E5%AE%89%E8%A3%85%20rabbitMQ%20%E5%8F%8A%E6%93%8D%E4%BD%9C%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/ericli-ericli/p/5902270.html]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java system.getproperty获取环境属性]]></title>
    <url>%2F2018%2F12%2F10%2FJava%20system.getproperty%E8%8E%B7%E5%8F%96%E7%8E%AF%E5%A2%83%E5%B1%9E%E6%80%A7%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819public class Properties &#123; public static void main(String args[]) &#123; System.out.println("Java运行环境的版本:" + System.getProperty("java.version")); System.out.println("Java运行环境的生产商:" + System.getProperty("java.vendor")); System.out.println("Java的安装路径：" + System.getProperty("java.home")); System.out.println("虚拟机实现的版本：" + System.getProperty("java.vm.version")); System.out.println("虚拟机实现的生产商：" + System.getProperty("java.vm.vendor")); System.out.println("默认的临时文件路径：" + System.getProperty("java.io.tmpdir")); System.out.println("用户的账户名称：" + System.getProperty("user.name")); System.out.println("当前用户工作目录：" + System.getProperty("user.dir")); System.out.println("用户的home路径：" + System.getProperty("user.home")); System.out.println("操作系统的名称:" + System.getProperty("os.name")); System.out.println("操作系统的版本：" + System.getProperty("os.version")); System.out.println("操作系统的架构：" + System.getProperty("os.arch")); System.out.println("运行环境规范的名称:" + System.getProperty("java.specification.name")); System.out.println("Java类格式化的版本号：" + System.getProperty("java.class.version")); System.out.println("类所在的路径：" + System.getProperty("java.class.path")); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[java文件下载中文文件名乱码]]></title>
    <url>%2F2018%2F11%2F28%2Fjava%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD%E4%B8%AD%E6%96%87%E6%96%87%E4%BB%B6%E5%90%8D%E4%B9%B1%E7%A0%81%2F</url>
    <content type="text"><![CDATA[1fileName = new String( fileName.getBytes("gb2312"), "ISO8859-1" );//文件名]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[记一次删库——主库全量备份还原到从库的的经过]]></title>
    <url>%2F2018%2F11%2F27%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%88%A0%E5%BA%93%E2%80%94%E2%80%94%E4%B8%BB%E5%BA%93%E5%85%A8%E9%87%8F%E5%A4%87%E4%BB%BD%E8%BF%98%E5%8E%9F%E5%88%B0%E4%BB%8E%E5%BA%93%E7%9A%84%E7%9A%84%E7%BB%8F%E8%BF%87%2F</url>
    <content type="text"><![CDATA[一：主库的操作执行全量备份命令：mysqldump -uUSER -pPASSWORD pt_brain --single-transaction --routines --triggers --events --master-data=2 --flush-logs &gt; pt_brain.sql -----single-transaction：简单理解不锁表保持事务一致性 -----routines ：备份存储过程和函数 -----triggers ：备份触发器 -----events ：备份事件 -----master-data=2：在备份文件中记录当前二进制日志的位置，并且为注释的，设置文1是不注释掉在主从复制中才有意义 -----flush-logs：滚动一次日志查看备份的文件开头几行MASTER_LOG_FILE文件和 MASTER_LOG_POS位置（用于主从复制的时候设置复制点）：more pt_brin.sql压缩备份文件：tar zcvf pt_brain.sql.tgz pt_brain.sql传文件到从库服务器（到此主库的操作结束）：scp root@IP:/data/backup/pt_brain.sql.tgz /data/backup/import/参数说明：此命令是在接受文件的服务器上运行的，此处的ip指的是发送方的ip IP：发送方的ip，如果同一内网写内网地址 /data/backup/pt_brain.sql.tgz 发送方服务器的文件位置 /data/backup/import/ 接收方文件存放的位置 二：从库的操作配置mysql配置文件添加属性提高导入速度：innodb_flush_log_at_trx_commit = 0 sync_binlog=1000重启MySQL服务：service mysqld restart;解压备份文件：tar zxvf pt_brain.sql.tgz pt_brain.sql编写脚本方便查看导入开始到结束时间：vim import.sh复制下面脚本到import.sh#!/bin/bash #use mysql to import mysql data BakDir=/data/backup/import #备份sql文件所在绝对位置，提前创建好文件夹 LogFile=/data/backup/import/import.log #记录导入开始和结束时间的文件 Date=`date +%Y%m%d` Begin=`date +&quot;%Y年%m月%d日 %H:%M:%S&quot;` cd $BakDir mysql -uUSER -pPASSWORD pt_brain &lt; pt_brain.sql #导入命令 Last=`date +&quot;%Y年%m月%d日 %H:%M:%S&quot;` echo 导入开始时间：$Begin 导入结束时间：$Last &gt;&gt; $LogFile赋予脚本权限：chmod a+x import.sh执行脚本保护进程（nohup命令）并后台运行（&amp;）导入数据库：nohup ./importFull.sh &amp;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[org.apache.poi的Excel导出报错“Invalid column index (256). Allowable column range for BIFF8 is (0..255) or ('A'..'IV') ”]]></title>
    <url>%2F2018%2F11%2F22%2Forg.apache.poi%E7%9A%84Excel%E5%AF%BC%E5%87%BA%E6%8A%A5%E9%94%99Invalid%20column%20index%C2%B7%C2%B7%C2%B7%2F</url>
    <content type="text"><![CDATA[原因：HSSFWorkbook类只能生成Excel最多256列数据 HSSFWorkbook workbook = new HSSFWorkbook();// 声明一个工作薄 解决：改用XSSFWorkbook类，能生成Excel最多16384列数据 XSSFWorkbook workbook = new XSSFWorkbook();// 声明一个工作薄]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[java的时间工具类，包括找各种时间，根据固定格式返回时间]]></title>
    <url>%2F2018%2F11%2F22%2Fjava%E7%9A%84%E6%97%B6%E9%97%B4%E5%B7%A5%E5%85%B7%E7%B1%BB%EF%BC%8C%E5%8C%85%E6%8B%AC%E6%89%BE%E5%90%84%E7%A7%8D%E6%97%B6%E9%97%B4%EF%BC%8C%E6%A0%B9%E6%8D%AE%E5%9B%BA%E5%AE%9A%E6%A0%BC%E5%BC%8F%E8%BF%94%E5%9B%9E%E6%97%B6%E9%97%B4%2F</url>
    <content type="text"><![CDATA[import org.apache.commons.lang3.time.DateFormatUtils; import java.sql.Timestamp;import java.util.ArrayList;import java.util.Calendar;import java.util.Date;import java.util.GregorianCalendar;import java.util.List; /* \ @author: Charlie */ public class DateUtils { public static final String DAYTYPE = &quot;yyyy-MM-dd&quot;; public static final String DAYTIMETYPE = &quot;yyyy-MM-dd HH:mm&quot;; public static final String TIMETYPE = &quot;yyyy-MM-dd HH:mm:ss&quot;; public static final String DATETYPE = &quot;yyyy年MM月dd日&quot;; public static final String DateTypeYear = &quot;yyyy&quot;; public static final String DateTypeMonth = &quot;yyyyMM&quot;; public static final String Date4ExportType = &quot;yyyyMMdd&quot;; public static final String DateDetail4ExportType = &quot;yyyyMMddHHmmss&quot;; //配合上面的类型传入format参数，返回想要的时间格式 public static String formatDate(Date date, String format) { if (null == date) { return null; } return DateFormatUtils.format(date, format); } //获取当天的开始时间 public static Date getDayBegin() { Calendar cal = new GregorianCalendar(); cal.set(Calendar.HOUR_OF_DAY, 0); cal.set(Calendar.MINUTE, 0); cal.set(Calendar.SECOND, 0); cal.set(Calendar.MILLISECOND, 0); return cal.getTime(); } //获取当天的结束时间 public static Date getDayEnd() { Calendar cal = new GregorianCalendar(); cal.set(Calendar.HOUR_OF_DAY, 23); cal.set(Calendar.MINUTE, 59); cal.set(Calendar.SECOND, 59); cal.set(Calendar.MILLISECOND, 999); return cal.getTime(); } //获取昨天的开始时间 public static Date getBeginDayOfYesterday() { Calendar cal = new GregorianCalendar(); cal.setTime(getDayBegin()); cal.add(Calendar.DAY_OF_MONTH, -1); return cal.getTime(); } //获取昨天的结束时间 public static Date getEndDayOfYesterDay() { Calendar cal = new GregorianCalendar(); cal.setTime(getDayEnd()); cal.add(Calendar.DAY_OF_MONTH, -1); return cal.getTime(); } //获取明天的开始时间 public static Date getBeginDayOfTomorrow() { Calendar cal = new GregorianCalendar(); cal.setTime(getDayBegin()); cal.add(Calendar.DAY_OF_MONTH, 1); return cal.getTime(); } //获取明天的结束时间 public static Date getEndDayOfTomorrow() { Calendar cal = new GregorianCalendar(); cal.setTime(getDayEnd()); cal.add(Calendar.DAY_OF_MONTH, 1); return cal.getTime(); } //获取本周的开始时间 public static Date getBeginDayOfWeek() { Date date = new Date(); if (date == null) { return null; } Calendar cal = Calendar.getInstance(); cal.setTime(date); int dayofweek = cal.get(Calendar.DAY_OF_WEEK); if (dayofweek == 1) { dayofweek += 7; } cal.add(Calendar.DATE, 2 - dayofweek); return getDayStartTime(cal.getTime()); } //获取本周的结束时间 public static Date getEndDayOfWeek(){ Calendar cal = Calendar.getInstance(); cal.setTime(getBeginDayOfWeek()); cal.add(Calendar.DAY_OF_WEEK, 6); Date weekEndSta = cal.getTime(); return getDayEndTime(weekEndSta); } //获取本月的开始时间 public static Date getBeginDayOfMonth() { Calendar calendar = Calendar.getInstance(); calendar.set(getNowYear(), getNowMonth() - 1, 1); return getDayStartTime(calendar.getTime()); } //获取本月的结束时间 public static Date getEndDayOfMonth() { Calendar calendar = Calendar.getInstance(); calendar.set(getNowYear(), getNowMonth() - 1, 1); int day = calendar.getActualMaximum(5); calendar.set(getNowYear(), getNowMonth() - 1, day); return getDayEndTime(calendar.getTime()); } //获取本年的开始时间 public static Date getBeginDayOfYear() { Calendar cal = Calendar.getInstance(); cal.set(Calendar.YEAR, getNowYear()); cal.set(Calendar.MONTH, Calendar.JANUARY); cal.set(Calendar.DATE, 1); return getDayStartTime(cal.getTime()); } //获取本年的结束时间 public static Date getEndDayOfYear() { Calendar cal = Calendar.getInstance(); cal.set(Calendar.YEAR, getNowYear()); cal.set(Calendar.MONTH, Calendar.DECEMBER); cal.set(Calendar.DATE, 31); return getDayEndTime(cal.getTime()); } //获取某个日期的开始时间 public static Timestamp getDayStartTime(Date d) { Calendar calendar = Calendar.getInstance(); if(null != d){ calendar.setTime(d); } calendar.set(calendar.get(Calendar.YEAR), calendar.get(Calendar.MONTH), calendar.get(Calendar.DAY_OF_MONTH), 0, 0, 0); calendar.set(Calendar.MILLISECOND, 0); return new Timestamp(calendar.getTimeInMillis()); } //获取某个日期的结束时间 public static Timestamp getDayEndTime(Date d) { Calendar calendar = Calendar.getInstance(); if(null != d) { calendar.setTime(d); } calendar.set(calendar.get(Calendar.YEAR), calendar.get(Calendar.MONTH), calendar.get(Calendar.DAY_OF_MONTH), 23, 59, 59); calendar.set(Calendar.MILLISECOND, 999); return new Timestamp(calendar.getTimeInMillis()); } //获取今年是哪一年 public static Integer getNowYear() { Date date = new Date(); GregorianCalendar gc = (GregorianCalendar) Calendar.getInstance(); gc.setTime(date); return Integer.valueOf(gc.get(1)); } //获取本月是哪一月 public static int getNowMonth() { Date date = new Date(); GregorianCalendar gc = (GregorianCalendar) Calendar.getInstance(); gc.setTime(date); return gc.get(2) + 1; } //两个日期相减得到的天数 public static int getDiffDays(Date beginDate, Date endDate) { if (beginDate == null || endDate == null) { throw new IllegalArgumentException(“getDiffDays param is null!”); } long diff = (endDate.getTime() - beginDate.getTime())/ (1000 * 60 * 60 * 24); int days = new Long(diff).intValue(); return days; } //两个日期相减得到的毫秒数 public static long dateDiff(Date beginDate, Date endDate) { long date1ms = beginDate.getTime(); long date2ms = endDate.getTime(); return date2ms - date1ms; } //获取两个日期中的最大日期 public static Date max(Date beginDate, Date endDate) { if (beginDate == null) { return endDate; } if (endDate == null) { return beginDate; } if (beginDate.after(endDate)) { return beginDate; } return endDate; } //获取两个日期中的最小日期 public static Date min(Date beginDate, Date endDate) { if (beginDate == null) { return endDate; } if (endDate == null) { return beginDate; } if (beginDate.after(endDate)) { return endDate; } return beginDate; } //返回某月该季度的第一个月 public static Date getFirstSeasonDate(Date date) { final int[] SEASON = { 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4 }; Calendar cal = Calendar.getInstance(); cal.setTime(date); int sean = SEASON[cal.get(Calendar.MONTH)]; cal.set(Calendar.MONTH, sean * 3 - 3); return cal.getTime(); } //返回某个日期下几天的日期 public static Date getNextDay(Date date, int i) { Calendar cal = new GregorianCalendar(); cal.setTime(date); cal.set(Calendar.DATE, cal.get(Calendar.DATE) + i); return cal.getTime(); } //返回某个日期前几天的日期 public static Date getFrontDay(Date date, int i) { Calendar cal = new GregorianCalendar(); cal.setTime(date); cal.set(Calendar.DATE, cal.get(Calendar.DATE) - i); return cal.getTime(); } //获取某年某月到某年某月按天的切片日期集合（间隔天数的日期集合） public static List getTimeList(int beginYear, int beginMonth, int endYear,int endMonth, int k) { List list = new ArrayList(); if (beginYear == endYear) { for (int j = beginMonth; j &lt;= endMonth; j++) { list.add(getTimeList(beginYear, j, k)); } } else { for (int j = beginMonth; j &lt; 12; j++) { list.add(getTimeList(beginYear, j, k)); } for (int i = beginYear + 1; i &lt; endYear; i++) { for (int j = 0; j &lt; 12; j++) { list.add(getTimeList(i, j, k)); } } for (int j = 0; j &lt;= endMonth; j++) { list.add(getTimeList(endYear, j, k)); } } return list; } //获取某年某月按天切片日期集合（某个月间隔多少天的日期集合） public static List getTimeList(int beginYear, int beginMonth, int k) { List list = new ArrayList(); Calendar begincal = new GregorianCalendar(beginYear, beginMonth, 1); int max = begincal.getActualMaximum(Calendar.DATE); for (int i = 1; i &lt; max; i = i + k) { list.add(begincal.getTime()); begincal.add(Calendar.DATE, k); } begincal = new GregorianCalendar(beginYear, beginMonth, max); list.add(begincal.getTime()); return list; } //获取某年某月的第一天日期 public static Date getStartMonthDate(int year, int month) { Calendar calendar = Calendar.getInstance(); calendar.set(year, month - 1, 1); return calendar.getTime(); } //获取某年某月的最后一天日期 public static Date getEndMonthDate(int year, int month) { Calendar calendar = Calendar.getInstance(); calendar.set(year, month - 1, 1); int day = calendar.getActualMaximum(5); calendar.set(year, month - 1, day); return calendar.getTime(); } }]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql的全量备份和差量备份]]></title>
    <url>%2F2018%2F11%2F21%2Fmysql-e7-9a-84-e5-85-a8-e9-87-8f-e5-a4-87-e4-bb-bd-e5-92-8c-e5-b7-ae-e9-87-8f-e5-a4-87-e4-bb-bd%2F</url>
    <content type="text"><![CDATA[待整理··· CentOS7安装Mysql5.7: 参考网址：https://blog.csdn.net/nullbull/article/details/79507996 Mysql使用ProxySQL实现读写分离 http://blog.51cto.com/bigboss/2103290 nohup ./xxx.sh &amp; 后台保护进程执行脚本查看是否开启了bin_logmysql&gt; show variables like ‘log_%’; 全量备份各种参数：https://www.cnblogs.com/qq78292959/p/3637135.html USER是用户名，PASSWORD是密码，table是要备份的数据库名，bakname是自己起的备份名 备份数据 mysqldump -uUSER -pPASSWORD table &gt; bakname.sql–mast-data=2 记录pos位置 恢复数据 mysql -uUSER -pPASSWORD table &lt; bakname.sql 备份时候加的参数 https://blog.csdn.net/cug_jiang126com/article/details/49824471 增量备份备份：mysqladmin -uroot -p12345 flush-logs； 查看binlog日志：mysql&gt;show binlog events in ‘mysql-bin.000003’; https://blog.csdn.net/leshami/article/details/41962243 恢复数据：mysqlbinlog –start-position=2051 –stop-position=2399 /app/mysql_binlog/mysql-bin.000003 | mysql -uroot -p ，回车后输入密码。 待续 ···· 资料待整理 #https://blog.csdn.net/fengyong7723131/article/details/80447786 https://www.cnblogs.com/kevingrace/p/5907254.html ProxySql 读写分离 CentOS7安装Mysql5.7: 参考网址：https://blog.csdn.net/nullbull/article/details/79507996 备份MySQL数据库的命令 mysqldump -hhostname -uusername -ppassword databasename &gt; backupfile.sql 备份MySQL数据库为带删除表的格式 备份MySQL数据库为带删除表的格式，能够让该备份覆盖已有数据库而不需要手动删除原有数据库。 mysqldump -–add-drop-table -uusername -ppassword databasename &gt; backupfile.sql 直接将MySQL数据库压缩备份 mysqldump -hhostname -uusername -ppassword databasename | gzip &gt; backupfile.sql.gz 备份MySQL数据库某个(些)表 mysqldump -hhostname -uusername -ppassword databasename specific_table1 specific_table2 &gt; backupfile.sql 同时备份多个MySQL数据库 mysqldump -hhostname -uusername -ppassword –databases databasename1 databasename2 databasename3 &gt; multibackupfile.sql 仅仅备份数据库结构 mysqldump –no-data –databases databasename1 databasename2 databasename3 &gt; structurebackupfile.sql 备份服务器上所有数据库 mysqldump –all-databases &gt; allbackupfile.sql 还原MySQL数据库的命令 mysql -hhostname -uusername -ppassword databasename &lt; backupfile.sql 还原压缩的MySQL数据库 gunzip &lt; backupfile.sql.gz | mysql -uusername -ppassword databasename 将数据库转移到新服务器 mysqldump -uusername -ppassword databasename | mysql –host=... -C databasename ——————— 作者：null_plf 来源：CSDN 原文：https://blog.csdn.net/plfplc/article/details/80704018 版权声明：本文为博主原创文章，转载请附上博文链接！ 找到 my.cnf 文件，添加以下两行 skip_innodb_doublewrite innodb_flush_log_at_trx_commit = 0 重启Mysql，重新导入数据，见证奇迹。 skip_innodb_doublewrite 禁用Mysql 的两次写功能。 innodb_flush_log_at_trx_commit = 0 当设置该值为1时，每次事务提交都要做一次fsync，这是最安全的配置，即使宕机也不会丢失事务； 当设置为2时，则在事务提交时只做write操作，只保证写到系统的page cache，因此实例crash不会丢失事务，但宕机则可能丢失事务； 当设置为0时，事务提交不会触发redo写操作，而是留给后台线程每秒一次的刷盘操作，因此实例crash将最多丢失1秒钟内的事务。 ——————— 作者：程序猿老曹 来源：CSDN 原文：https://blog.csdn.net/starscao/article/details/72819108 版权声明：本文为博主原创文章，转载请附上博文链接！ 如何启动/停止/重启MySQL 发启动、停止、重启 MySQL 是每个拥有独立主机的站长必须要撑握的操作，下面为大家简单介绍一下操作方法： 一、启动方式 1、使用 service 启动：service mysqld start 2、使用 mysqld 脚本启动：/etc/inint.d/mysqld start 3、使用 safe_mysqld 启动：safe_mysqld&amp; 二、停止 1、使用 service 启动：service mysqld stop 2、使用 mysqld 脚本启动：/etc/inint.d/mysqld stop 3、mysqladmin shutdown 三、重启 1、使用 service 启动：service mysqld restart 2、使用 mysqld 脚本启动：/etc/inint.d/mysqld restart 好久没在linux下重启mysql了，看来是服务跑得太稳定了呵呵，最近机器死掉，重启后mysql服务也得重启，竟然命令忘记了，只好百度、google啦！ 下面是从网上搜的，应该是最靠谱的喽，大家试试好用不！ 好久没有折腾服务器了，一大早打开要调试的页面，发现/tmp目录空间满了，导致MySQL没有办法正常运行了。于是修改了修改了my.cnf中mysql.sock的存放路径以及php.ini中的相关设置，这个时候需要重启MySQL，呵呵，好长时间没有在Linux下折腾服务器了，尽然突然想不起来怎么在Linux操作系统下重启MySQL服务的命令了：（ 于是乎百度、google了“Linux下重启MySQL命令”，搜索结果一显示，从前几条的摘要信息中就已经找到答案了：）以下Linux下重启MySQL的正确方法： 1、通过rpm包安装的MySQL service mysqld restart 2、从源码包安装的MySQL // linux关闭MySQL的命令 $mysql_dir/bin/mysqladmin -uroot -p shutdown // linux启动MySQL的命令 $mysql_dir/bin/mysqld_safe &amp; 其中mysql_dir为MySQL的安装目录，mysqladmin和mysqld_safe位于MySQL安装目录的bin目录下，很容易找到的。 3、以上方法都无效的时候，可以通过强行命令：“killall mysql”来关闭MySQL，但是不建议用这样的方式，因为这种野蛮的方法会强行终止MySQL数据库服务，有可能导致表损坏……所以自己掂量着用。 问题描述：sh文件中，在win环境下，用WinSCP编辑，出现如下错误： -bash: ./start.sh: /bin/sh^M: bad interpreter: No such file or directory 解决方案：这是win的编码引起的，可通过如下解决。 1.查看该文件：vim start.sh 2.查看该错误文件的格式（一般报错的文件格式是DOS）： :set ff 3.修改该文件格式为UNIX： :set ff=unix 4.再保存。 :wq! ——————— 作者：lizhengnanhua 来源：CSDN 原文：https://blog.csdn.net/lizhengnanhua/article/details/51724396 版权声明：本文为博主原创文章，转载请附上博文链接！ 数据恢复http://blog.51cto.com/dbguy/1583063 查看linux系统常用的命令，Linux查看系统配置常用命令https://www.cnblogs.com/xuchunlin/p/5671572.html]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[xshell直接用命令上传下载文件到服务器]]></title>
    <url>%2F2018%2F11%2F21%2Fxshell%E7%9B%B4%E6%8E%A5%E7%94%A8%E5%91%BD%E4%BB%A4%E4%B8%8A%E4%BC%A0%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%E5%88%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Xftp很好用，但是有时候没安装这个软件，还需要安装一遍会很麻烦，还是用命令吧：服务器只需要安装lrzsz软件1yum install lrzsz 使用rz命令是上传文件到linux1rz 使用sz命令是下载文件到windows1sz 文件名]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[IDEA中使用lombok]]></title>
    <url>%2F2018%2F11%2F13%2FIDEA%E4%B8%AD%E4%BD%BF%E7%94%A8lombok%2F</url>
    <content type="text"><![CDATA[File→Settings→Plugins点击Browse repositories搜索安装lombok插件，install安装lombok插件重启idea（或者直接下载从本地安装，安装方式不限） File→Settings→Build，Execution，Deployment→Annotion Processors勾选Enable annotation processing]]></content>
      <categories>
        <category>IDE</category>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[IDEA中使用Git管理项目]]></title>
    <url>%2F2018%2F11%2F13%2FIDEA%E4%B8%AD%E4%BD%BF%E7%94%A8Git%E7%AE%A1%E7%90%86%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[菜单栏VCS→Checkout from Version Contorol→Git拉下版本库代码 菜单栏File→Open导入项目，并根据项目进行配置 右键项目如果没有git的选项需要加入版本库才可以提交代码，菜单栏VCS→Enable Version Control Integration如图选择Git，点击OK按钮 这时项目右键就有git选项 Commit Directory，具体如下图，可以直接Commit and push推送线上也可以先提交到本地库再Repository→Push]]></content>
      <categories>
        <category>Git</category>
        <category>IDE</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Git拉取分支]]></title>
    <url>%2F2018%2F11%2F12%2FGit%E6%8B%89%E5%8F%96%E5%88%86%E6%94%AF%2F</url>
    <content type="text"><![CDATA[git clone 不指定分支默认拉master主分支的代码 1https://example.com/account/service.git git clone 指定分支加上-b参数，后面跟上分支名，以上面为例，branch2就是另外一个分支 1git clone -b branch2 https://example.com/account/service.git]]></content>
      <categories>
        <category>Git</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql字符串拼接函数CONCAT()、CONCAT_WS()、GROUP_CONCAT()的使用]]></title>
    <url>%2F2018%2F11%2F09%2Fmysql%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8B%BC%E6%8E%A5%E5%87%BD%E6%95%B0CONCAT()%E3%80%81CONCAT_WS()%E3%80%81GROUP_CONCAT()%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[建表插数据 CREATE TABLE `score_info`(`id` INT(11) NOT NULL AUTO_INCREMENT,`stuName` VARCHAR(22) DEFAULT NULL,`course` VARCHAR(22) DEFAULT NULL,`score` INT(11) DEFAULT NULL,PRIMARY KEY(`id`)) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8; INSERT INTO `score_info` VALUES (‘1’,’张三’,’语文’,’87’);INSERT INTO `score_info` VALUES (‘2’,’张三’,’数学’,’91’);INSERT INTO `score_info` VALUES (‘3’,’张三’,’英语’,’90’);INSERT INTO `score_info` VALUES (‘4’,’李四’,’语文’,’95’);INSERT INTO `score_info` VALUES (‘5’,’李四’,’数学’,’87’);INSERT INTO `score_info` VALUES (‘6’,’李四’,’英语’,’93’);INSERT INTO `score_info` VALUES (‘7’,’王五’,’语文’,’83’);INSERT INTO `score_info` VALUES (‘8’,’王五’,’数学’,’78’);INSERT INTO `score_info` VALUES (‘9’,’王五’,’英语’,’98’); CONCAT(str1,str2,…) SELECT CONCAT(student_name,course,score) FROM score_info; 查询结果： 如果需要添加分割符，直接在中间添加符号的字符串 SELECT CONCAT(student_name,”,”,course,”#”,score,”@”) FROM score_info; 查询结果： CONCAT_WS(separator,str1,str2,…) 与CONCAT()函数功能相同只是把分割符号放在方法里面的第一个参数 SELECT CONCAT_WS(“@%”,student_name,course,score) FROM score_info; 查询结果： GROUP_CONCAT(expr) SELECT GROUP_CONCAT(score),student_name FROM score_info GROUP BY student_name 查询结果：]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Git开启二次验证需要配置Access_Tokens才可以拉代码]]></title>
    <url>%2F2018%2F11%2F09%2FGit%E5%BC%80%E5%90%AF%E4%BA%8C%E6%AC%A1%E9%AA%8C%E8%AF%81%E9%9C%80%E8%A6%81%E9%85%8D%E7%BD%AEAccess_Tokens%E6%89%8D%E5%8F%AF%E4%BB%A5%E6%8B%89%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[gitlab开启二次验证拉代码会报权限错，这时候需要生成个人令牌通过令牌拉代码。 拉代码就需要用如下方式才能拉到代码： git clone https://oauth2:Access_Tokens@example.com/package.git 举例说明步骤如下： 登录gitlab右上角头像Settings（设置）→ Access Tokens（访问令牌） 填写Personal Access Token（个人访问令牌）名称和勾选范围，点击生成按钮 生成了个人访问令牌，复制这个令牌，拉代码需要通过这个令牌才能够拉取 按照本文图例就是对应输入 git clone https://oauth2:ogr73sajdr8obJGtDAAn@example.com/package.git]]></content>
      <categories>
        <category>Git</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sql查询varchar类型MAX函数查询不准确，需要String转int]]></title>
    <url>%2F2018%2F08%2F22%2Fsql%E6%9F%A5%E8%AF%A2varchar%E7%B1%BB%E5%9E%8BMAX%E5%87%BD%E6%95%B0%E6%9F%A5%E8%AF%A2%E4%B8%8D%E5%87%86%E7%A1%AE%EF%BC%8C%E9%9C%80%E8%A6%81String%E8%BD%ACint%2F</url>
    <content type="text"><![CDATA[看个例子 最大的数明明是10000但是查出来的最大数是9999，因为这个是varchar类型，需要在sql的时候转类型 sql解决办法 SELECT MAX(CAST(m.student_code AS SIGNED)) FROM member_detail m;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[nginx.conf]]></title>
    <url>%2F2018%2F08%2F21%2Fnginx-conf%2F</url>
    <content type="text"><![CDATA[#user nobody; worker_processes 1; #error_log logs/error.log; #error_log logs/error.log notice; #error_log logs/error.log info; #pid logs/nginx.pid; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; #access_log logs/access.log main; access_log off; keepalive_requests 5120; keepalive_timeout 120; gzip on; gzip_min_length 10k; gzip_types text/plain text/css application/x-javascript application/xml application/json; client_max_body_size 500m; client_body_buffer_size 256k; client_header_timeout 1m; client_body_timeout 1m; client_header_buffer_size 16k; large_client_header_buffers 4 64k; send_timeout 1m; proxy_connect_timeout 180; proxy_read_timeout 180; proxy_send_timeout 180; proxy_buffers 4 64k; proxy_buffer_size 64k; include sites/*.conf; }]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[localtunnel在windows环境下内网穿透]]></title>
    <url>%2F2018%2F08%2F16%2Flocaltunnel%E5%9C%A8windows%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F%2F</url>
    <content type="text"><![CDATA[本机需要有nodejs环境，不做赘述 *命令提示符输入下面命令安装localtunnel * npm i localtunnel -g 配置环境变量 我是安装到下面目录（安装完成之后会提示安装到了哪个目录），目录会有个lt.cmd C:\Users\charlie\AppData\Roaming\npm 将这个配置到环境变量Path后面 使用localtunnel 运行命令：端口号填写什么就是代理什么端口出去，然后localtunnel会给你一个公网可以访问的域名，然后就可以用这个域名愉快地玩耍了 lt –port 端口号]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL索引优化]]></title>
    <url>%2F2018%2F08%2F13%2Fmysql-e4-bc-98-e5-8c-96-e5-ae-9e-e6-88-98%2F</url>
    <content type="text"><![CDATA[索引：索引是数据结构；最简单的索引相当于书的目录，通过目录找书本章节内容，顺序结构 索引分类 普通索引：只有一个索引只包含一个列，一个表可以有多个单列索引 唯一索引(Unique)：索引列的值必须唯一，但允许有空值 复合索引：即一个索引包含多个列 聚簇索引：并不是一种单独的索引类型，而是一种数据存储方式。具体细节取决于不同的实现，InnoDB的聚簇索引其实就是在同一个结构中保存了B-Tree索引（技术上来说就是B+Tree）和数据行。 非聚簇索引：不是聚簇索引就是非聚簇索引 Show global variables like “%datadir%”; 基础语法:略 查看索引：show index from 表名; 创建索引： ①CREATE UNIQUE INDEX 索引名 ON 表名(字段名); ②ALTER TABLE 表名 add UNIQUE 索引名(字段名); 删除索引：DROP INDEX 索引名 ON 表名; 执行计划 执行计划是什么：使用explain关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理你的SQL语句的。分析你的查询语句或是表结构的性能瓶颈 possible_keys：可能有索引 key：SQL用到了索引，key就是有值的 key_len：表示索引使用的字节数 索引长度，可以衡量索引是否生效，比如explain select * from t1 where col1 = ‘ab’;会查出key_len为13，如果explain select * from t1 where col1 = ‘ab’ and col2 = ‘cd’;会查出key_len为26，说明用了两个索引 索引在char和varchar建立 Utf-8占三位字符 varchar(n)变长字段+不允许NULL = n(utf-8=3,gbk=2,latin=1)+2 *ref表示哪一列用到了索引 **优化实战 1.全职匹配 2.最佳左前缀法则：如果索引是多个，要遵循此原则。指的是查询从索引的最左边开始并且不跳过字段。如果聚合索引是name,age,postion三个字段的聚合索引；那如果通过age和postion查询，那索引会失效，因为name头不在了，无法走索引 3.不能再索引列上做任何操作（计算，函数，），会跳过索引 4.范围条件放最后：存储引擎不能使用索引中范围条件右边的列 比如 select * from staffs where name = “张三” and age &gt;15 and postion = “上海” 这样的话只会走name和age两个索引，因为age做了范围搜索之后就不在进入索引 5.尽量使用覆盖索引：只访问索引的查询，减少“select ”的；比如只查询name，age，postion三个参数，那就不要用select * from~ 而是 select name,age,postion from staffs where~ 6.不等于要慎用：mysql在使用不等于（！=或&lt;&gt;）的时候无法使用索引，会导致全表扫描； select * from staffs where name != “张三” 这样无法使用索引 select name,age,postion from staffs where name != “张三” 这样使用了上面的覆盖索引原则，这样可以使用索引 7.Null/Not有影响 : select * from staffs where name is not null; 是否会全表扫描要根据字段来决定；如果查询的字段没有设置为 not null那就不会全表扫描 8.Like查询要当心：左模糊查询会造成全表扫描，避免使用左模糊查询；但是实际项目无法避免，所以要用到之前的覆盖索引原则 9.字符串不加单引号索引失效：如果数据类型是varchar那一定要在查询条件上加上引号；如果不用引号那就会全表扫描 10.OR改UNION效率高：能用union就不要用or，用or会导致索引失效]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[(转载)一个四五年的Java开发程序员，该准备哪些去面试？]]></title>
    <url>%2F2018%2F08%2F07%2F(%E8%BD%AC%E8%BD%BD)%E4%B8%80%E4%B8%AA%E5%9B%9B%E4%BA%94%E5%B9%B4%E7%9A%84Java%E5%BC%80%E5%8F%91%E7%A8%8B%E5%BA%8F%E5%91%98%EF%BC%8C%E8%AF%A5%E5%87%86%E5%A4%87%E5%93%AA%E4%BA%9B%E5%8E%BB%E9%9D%A2%E8%AF%95%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[转自：https://www.cnblogs.com/zgghb/p/7073136.html 上周面试了一周，感触颇深，总结一下。 面试了公司大概有阿里，携程，爱奇艺，唯品会，途牛，bilibili，大众点评，阿里和爱奇艺是电话面试，其他现场面试。 首先，五年左右，应该算高级开发工程师，大部分面试不用去做题，背string和stringbuilder区别的，几乎都是底层和远离，分布式等。虽然一个程序员，在工作中大部分还是写流水代码，增删查改。 1 JVM这个大公司基本都会问，内存模型，GC，jvm都有哪些区域？栈桢存了什么？ 常用垃圾回收器哪些，特点？jvm性能调优，这些从周志明的jvm特性那本书基本都有答案，一个高级程序员必读，出去面试，至少得读两遍，理解，不是死记硬背，面试官会问的很细，你如果死记硬背，人家随便一问，你就跪。 2 线程池这个基本必考，高级程序员不会问你启动线程哪几种方式，都是高并发场景。比如四种线程池，都在那哪几种场景使用的多。线程池的几个重要参数哪些，这些重要参数，在不同线程池下比较。高并发下阻塞队列用哪个，我记得是大众点评问我的，用linkblockqueue还是arrayblockqueue，我当时候说，后者吧，队列添加任务快，基于列表，人家说错了，前者，array是连续的，回收的快，这些都是细节。剩下的就是阻塞队列拒绝策略。默认都是拒绝，有个面试官问我，假如阻塞队列满了，请求拒绝，但是这些请求是重要的，怎么办？这种情况实际开发中会遇到，每个系统总有个峰值，达到峰值，系统可能处理不了，他说，这个时候先把所有请求落数据库，或者mq，后面处理，真实场景会遇到，可能之前你没遇到，但是面试官会问这些，觉得你有没有思考的能力。线程池，tomcat默认就有线程池，会问到调优，所有系统基本离不开吧。 3 锁这个也是必考，虽然开发中有时候不用，基本会问，syncnizched关键字，作用，类锁，对象锁，方法锁区别？ 静态方法加锁，两线程会互斥吗，非静态方法，加锁，会互斥吗？这个比较简单。这个关键字，内置对象锁，jvm创建对象后，会在对象的对象头，存着。然后和lock锁区别，这个如果你去背，可能不理解，我是这么理解的，假如有个厕所，很多人想去上，假如是sync锁，所有人会不定时去敲门，厕所有人吗，非常耗性能，CPU切换，但是lock锁，相当于加了个管理员，举着个牌子，进去一个人，排着写着有人，剩下的看到，就不会去敲门了，人走了，把牌子改下，Java，是基于volitale关键字，通过aqs保证，还有sync阻塞的，比如有个人在厕所玩手机，sync只能等。释放锁，但是lock，可以中断，或其他操作。问的深的会问锁优化之类的，虽然。开发基本用不到，但是你想拿高薪，必须要会。另外，数据库的锁会问。表锁。lock的读锁，写锁，怎么用。原理。这块必须深入理解。不然只能被虐。 4 缓存这个基本必考，redis，问的问题挺多，es可能会问，分布式系统，redis的唯一ID能做什么？新数据来了，是先存数据库还是redis，过期策略，什么时候用redis做缓存，什么时候用做数据库，和es，mongo，区别。这个只是个缓存，稍微看下，不会问的很深。 5 MQ消息中间价，分布式系统基本使用，原理，哪几个组成部分，kafka和rabbitmq，区别？ 如何保证消息不丢失。基于哪种协议？他的好处？你的项目怎么使用它的。点对点，订阅发布，区别，你的理解。 6 分布式分布式是啥。一个服务器问题造成血崩怎么办，四层负载均衡和七层区别，说下用过哪些RPC框架。dubbo基于哪些协议，服务如何注册，如何被发现？这个基本都考些，当然，也不要求你都精通，大致原理懂就好。 7 springbean的生命周期，spring得核心，spring的代理模式，动态代理，第三方cglib代理哪些场景使用，spring用了哪些设计模式。 spring的事务，传播行为。哪些常用注解？ 8 springmvc原理，常用注解。和struts区别，这个也要花点时间准备。 9 集合这个必考，重要重要重要，说三遍。哪些常用集合，几种集合，区别，比如ArrayList和linklist区别，性能，这个是基础，问的最多，最多，最多，hashmap，concurrentHashmap，JDK7和JDK8concurrentHashmap区别，实现，原理，两种JDK的size() 方法怎么实现？这个最好把源码看几遍，什么初始化，扩容，底层为什么要用数组加链表加红黑树，什么是红黑树，这个大部分公司必考，看你能力。 10 数据库索引，必考，基于b+树，唯一索引，普通索引，联合索引，join是否让索引失效，联合索引的最左原则，两个字段加了联合索引，两个一起用，或者第一个字段，查询，都不会实现，用右边的字段，失效，等等。有的会让你手写SQL，数据库引擎，myisam和innoDB区别，基本不会问别的。 11 同步io阻塞ionio bio aio 区别，用法，b阿里和爱奇艺都问了。 12 其他其他问的不多，上面基本全了，性能优化。系统稳定性。问之前的项目。 总结大部分问的，开发用不到，但是想拿高薪。还是准备下基础，因为公司会把这个面试情况衡量你是否是一个人才。另外，偶尔还会有笔试，去bilibili，途牛，笔试题，写算法，哎，坑爹的，总之一句话，多准备，多面试。另外遇到面试官问的，你不会，要问他答案。不然下一个面试官问到你还是不会。]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[冒泡算法对String字符串的数据进行排序]]></title>
    <url>%2F2018%2F08%2F01%2F%E5%86%92%E6%B3%A1%E7%AE%97%E6%B3%95%E5%AF%B9String%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324public class test &#123; public static void main(String\[\] args) &#123; String s = "58627"; //split转换s为arr数组 String\[\] arr = s.split(""); //冒泡对arr数组进行排序 for(int i=0;i&lt;arr.length-1;i++)&#123; for (int j=0;j&lt;arr.length-1-i;j++) &#123; if(Integer.parseInt(arr\[j\])&gt;Integer.parseInt(arr\[j+1\]))&#123; String temp = arr\[j+1\]; arr\[j + 1\] = arr\[j\]; arr\[j\] = temp; &#125; &#125; &#125; //数组arr转换成字符串finalString StringBuilder sb = new StringBuilder(); for (String s1 : arr) &#123; sb.append(s1); &#125; String finalString = sb.toString(); System.out.println(finalString); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[线程安全性有序性]]></title>
    <url>%2F2018%2F08%2F01%2F%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E6%80%A7%E6%9C%89%E5%BA%8F%E6%80%A7%2F</url>
    <content type="text"><![CDATA[Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性 Java里面可以通过volatile、synchronized、Lock保证有序性 happens-before原则 程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作； 锁定规则：一个unLock操作先行发生于后面对同一个锁的lock操作 volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作 传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C 线程启动规则：Thread对象的start()方法先行发生于此线程的每个动作 线程中断原则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生 线程终结规则：线程中所有的操作都先行发生于现成的终止检测，我们可以通过Thread.join()方法结束、Thread。isAlive()的返回值手段检测到线程已经终止执行 对象终结原则：一个对象的初始化完成先行发生于他的finalize()方法的开始]]></content>
      <categories>
        <category>高并发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[线程可见性]]></title>
    <url>%2F2018%2F07%2F31%2F%E7%BA%BF%E7%A8%8B%E5%8F%AF%E8%A7%81%E6%80%A7%2F</url>
    <content type="text"><![CDATA[导致共享变量在线程间不可见的原因 线程交叉执行 重排序结合线程交叉执行 共享变量更新后的值没有在工作内存和主内存直接即时更新 可见性synchronizedJMM（java内存模型）关于synchronized的两条规定： 线程解锁前，必须把共享变量的最新值刷新到主内存 线程加锁时，将清空工作内存中共享变量的值，从而使用共享变量时需要从主内存中重新读取最新的值（注意：加锁和解锁是通一把锁） volatile通过加入内存屏障和禁止重排序优化来实现对volatile变量写操作时，会在写操作后加入一条store屏障指令，将本地内存中的共享变量值刷新到主内存（每次线程访问volatile变量都会强迫从主内存读取变量值） volatile写操作会在volatile写操作之前加上StoreStore屏障禁止前面的普通写和后面的volatile写重排序， 会在Volatile写操作之后加上StoreLoad屏障，防止前面的volatile写与后面可能有的volatile读/写重排序。 对volatile变量读操作时，会在读操作前加入一条load屏障指令，从主内存中读取共享变量（volatile变量发生改变都会强迫线程刷新变量到主内存） volatile读操作会在volatile读操作和普通读操作中间加上LoadLoad屏障禁止后面的所有普通读操作和volatile读操作重排序； LoadStore屏障禁止后面的所有写操作和前面的volatile读操作重排序。]]></content>
      <categories>
        <category>高并发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[线程安全原子性对比]]></title>
    <url>%2F2018%2F07%2F31%2F%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E5%8E%9F%E5%AD%90%E6%80%A7%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[synchronized：不可中断锁，适合竞争不激烈，可读性好 Lock：可中断锁。多样化同步，竞争激烈时能维持常态 Atomic：竞争激烈时能维持常态，比Lock性能好；只能同步一个值]]></content>
      <categories>
        <category>高并发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[synchronized]]></title>
    <url>%2F2018%2F07%2F31%2Fsynchronized%2F</url>
    <content type="text"><![CDATA[修饰代码块：大括号括起来的代码，作用于调用的对象 修饰方法：整个方法，作用于调用对象 修饰静态方法：整个静态方法，作用于这个类所有对象 修饰类：括号括起来的部分，作用于这个类的所有对象 如果子类继承的父类有synchronized修饰的方法那调用父类的这个方法，不是synchronized修饰的方法，需要显示的写上synchronized修饰符]]></content>
      <categories>
        <category>高并发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Maven打包本地仓库示例]]></title>
    <url>%2F2018%2F07%2F19%2FMaven%E6%89%93%E5%8C%85%E6%9C%AC%E5%9C%B0%E4%BB%93%E5%BA%93%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[以支付宝SDK为例： 从支付宝官网下载的sdk：alipay-sdk-java-3.3.4.jar （地址：https://docs.open.alipay.com/54/103419/根据不同语言可以下载不同的SDK） 一.maven的配置（不做赘述） 下载maven； 配置maven本地环境变量； 自定义maven仓库位置。 二.打包到本地仓库 先到jar包本地的位置 执行maven打包操作 1mvn install:install-file -DgroupId=com.alipay -DartifactId=sdk-java -Dversion=3.3.4 -Dpackaging=jar -Dfile=alipay-sdk-java-3.3.4.jar 三.项目中的pom.xml引用： 12345&lt;dependency&gt; &lt;groupId&gt;com.alipay&lt;/groupId&gt; &lt;artifactId&gt;sdk-java&lt;/artifactId&gt; &lt;version&gt;3.3.4&lt;/version&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>maven</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java内存模型（Java Memory Model,JMM）— 同步八种操作和规则]]></title>
    <url>%2F2018%2F07%2F13%2F'Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%EF%BC%88Java%20Memory%20Model%2CJMM%EF%BC%89%E2%80%94%20%E5%90%8C%E6%AD%A5%E5%85%AB%E7%A7%8D%E6%93%8D%E4%BD%9C%E5%92%8C%E8%A7%84%E5%88%99'%2F</url>
    <content type="text"><![CDATA[lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态 unlock（解锁）：作用于主内存的变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他现成锁定 read（读取）：作用于主内存的变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用 load（载入）：作用于工作不存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中 use（使用）：作用于工作内存的变量，把工作内存中的一个变量传递给执行引擎 assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收的值赋值给工作内存的变量 store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作 write（写入）：作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中 如果要把一个变量从主内存中复制到工作内存，就需要按顺序执行read和load操作，如果把变量从工作内存中同步回主内存中，就要按顺序执行store和write操作。但是Java内存模型只要求上述操作必须顺序执行，而没有保证必须是连续执行 不允许read和load、store和write操作之一单独出现 不允许一个线程丢弃它的最近assign的操作，即变量在工作内存中改变了之后必须同步到主内存中 不允许一个线程无原因的（没有发生过任何assign操作）把数据从工作内存同步回主内存中 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量。即就是对一个变量实施use和store操作之前，必须先执行过快了assign和load操作 一个变量在同一个时刻只允许一条线程对其进行lock操作，但lock操作可以被同一条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。lock和unlock必须成对出现 如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前需要重新执行load或assign操作初始化变量的值 如果一个变量实现没有被lock操作锁定，则不允许对它执行unlock操作；也不允许去unlock一个被其他线程锁定的变量 对一个变量执行unlock操作之前，必须先把此变量同步到主内存中（执行store和write操作）]]></content>
      <categories>
        <category>高并发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[cpu多级缓存]]></title>
    <url>%2F2018%2F07%2F12%2Fcpu%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[为什么需要CPU cache： CPU的频率太快，快到内存跟不上，这样在处理器时钟周期内，CPU常常需要等待内存，浪费资源。所以cache的出现，为乐缓解CPU和内存之间的速度不匹配问题（结构：CPU—cache—memory） CPU cache有什么意义： 1.时间局限性：如果某个数据被访问，那么在不久的将来它很可能再次被访问 2.空间局限性：如果某个数据被访问，那么与它相邻的数据很快也可能被访问 CPU多级缓存—缓存一致性（MESI） MESI协议为了保证CPU cache之间缓存共享数据的一致性 M（modified）该行数据被修改，以和该数据在内存中的映像所不同。最新的数据只存在于Cache中 E（Exclusive）该行数据有效，且数据与内存中的数据一致，但数据值只存在于本Cache中。通俗来说，该数据只在Cache中独一份 S（Share）该行数据有效，且该数据与内存中的数据一致。同时，该数据存在与其它多个Cache中 I（Invalid）该行数据无效 CPU多级缓存—乱序执行优化 可能一个乘法a*b=c，但是处理器为了提高速度会违背代码原有的执行顺序进行优化可能运行的会是b*a=c]]></content>
      <categories>
        <category>高并发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[40个Java多线程问题总结]]></title>
    <url>%2F2018%2F07%2F12%2F40%E4%B8%AAJava%E5%A4%9A%E7%BA%BF%E7%A8%8B%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[转自http://www.cnblogs.com/xrq730/p/5060921.html 前言 Java多线程分类中写了21篇多线程的文章，21篇文章的内容很多，个人认为，学习，内容越多、越杂的知识，越需要进行深刻的总结，这样才能记忆深刻，将知识变成自己的。这篇文章主要是对多线程的问题进行总结的，因此罗列了40个多线程的问题。 这些多线程的问题，有些来源于各大网站、有些来源于自己的思考。可能有些问题网上有、可能有些问题对应的答案也有、也可能有些各位网友也都看过，但是本文写作的重心就是所有的问题都会按照自己的理解回答一遍，不会去看网上的答案，因此可能有些问题讲的不对，能指正的希望大家不吝指教。 40个问题汇总 1、多线程有什么用？ 一个可能在很多人看来很扯淡的一个问题：我会用多线程就好了，还管它有什么用？在我看来，这个回答更扯淡。所谓”知其然知其所以然”，”会用”只是”知其然”，”为什么用”才是”知其所以然”，只有达到”知其然知其所以然”的程度才可以说是把一个知识点运用自如。OK，下面说说我对这个问题的看法： （1）发挥多核CPU的优势 随着工业的进步，现在的笔记本、台式机乃至商用的应用服务器至少也都是双核的，4核、8核甚至16核的也都不少见，如果是单线程的程序，那么在双核CPU上就浪费了50%，在4核CPU上就浪费了75%。单核CPU上所谓的”多线程”那是假的多线程，同一时间处理器只会处理一段逻辑，只不过线程之间切换得比较快，看着像多个线程”同时”运行罢了。多核CPU上的多线程才是真正的多线程，它能让你的多段逻辑同时工作，多线程，可以真正发挥出多核CPU的优势来，达到充分利用CPU的目的。 （2）防止阻塞 从程序运行效率的角度来看，单核CPU不但不会发挥出多线程的优势，反而会因为在单核CPU上运行多线程导致线程上下文的切换，而降低程序整体的效率。但是单核CPU我们还是要应用多线程，就是为了防止阻塞。试想，如果单核CPU使用单线程，那么只要这个线程阻塞了，比方说远程读取某个数据吧，对端迟迟未返回又没有设置超时时间，那么你的整个程序在数据返回回来之前就停止运行了。多线程可以防止这个问题，多条线程同时运行，哪怕一条线程的代码执行读取数据阻塞，也不会影响其它任务的执行。 （3）便于建模 这是另外一个没有这么明显的优点了。假设有一个大的任务A，单线程编程，那么就要考虑很多，建立整个程序模型比较麻烦。但是如果把这个大的任务A分解成几个小任务，任务B、任务C、任务D，分别建立程序模型，并通过多线程分别运行这几个任务，那就简单很多了。 2、创建线程的方式 比较常见的一个问题了，一般就是两种： （1）继承Thread类 （2）实现Runnable接口 至于哪个好，不用说肯定是后者好，因为实现接口的方式比继承类的方式更灵活，也能减少程序之间的耦合度，面向接口编程也是设计模式6大原则的核心。 3、start()方法和run()方法的区别 只有调用了start()方法，才会表现出多线程的特性，不同线程的run()方法里面的代码交替执行。如果只是调用run()方法，那么代码还是同步执行的，必须等待一个线程的run()方法里面的代码全部执行完毕之后，另外一个线程才可以执行其run()方法里面的代码。 4、Runnable接口和Callable接口的区别 有点深的问题了，也看出一个Java程序员学习知识的广度。 Runnable接口中的run()方法的返回值是void，它做的事情只是纯粹地去执行run()方法中的代码而已；Callable接口中的call()方法是有返回值的，是一个泛型，和Future、FutureTask配合可以用来获取异步执行的结果。 这其实是很有用的一个特性，因为多线程相比单线程更难、更复杂的一个重要原因就是因为多线程充满着未知性，某条线程是否执行了？某条线程执行了多久？某条线程执行的时候我们期望的数据是否已经赋值完毕？无法得知，我们能做的只是等待这条多线程的任务执行完毕而已。而Callable+Future/FutureTask却可以获取多线程运行的结果，可以在等待时间太长没获取到需要的数据的情况下取消该线程的任务，真的是非常有用。 5、CyclicBarrier和CountDownLatch的区别 两个看上去有点像的类，都在java.util.concurrent下，都可以用来表示代码运行到某个点上，二者的区别在于： （1）CyclicBarrier的某个线程运行到某个点上之后，该线程即停止运行，直到所有的线程都到达了这个点，所有线程才重新运行；CountDownLatch则不是，某线程运行到某个点上之后，只是给某个数值-1而已，该线程继续运行 （2）CyclicBarrier只能唤起一个任务，CountDownLatch可以唤起多个任务 （3）CyclicBarrier可重用，CountDownLatch不可重用，计数值为0该CountDownLatch就不可再用了 6、volatile关键字的作用 一个非常重要的问题，是每个学习、应用多线程的Java程序员都必须掌握的。理解volatile关键字的作用的前提是要理解Java内存模型，这里就不讲Java内存模型了，可以参见第31点，volatile关键字的作用主要有两个： （1）多线程主要围绕可见性和原子性两个特性而展开，使用volatile关键字修饰的变量，保证了其在多线程之间的可见性，即每次读取到volatile变量，一定是最新的数据 （2）代码底层执行不像我们看到的高级语言—-Java程序这么简单，它的执行是Java代码–&gt;字节码–&gt;根据字节码执行对应的C/C++代码–&gt;C/C++代码被编译成汇编语言–&gt;和硬件电路交互，现实中，为了获取更好的性能JVM可能会对指令进行重排序，多线程下可能会出现一些意想不到的问题。使用volatile则会对禁止语义重排序，当然这也一定程度上降低了代码执行效率 从实践角度而言，volatile的一个重要作用就是和CAS结合，保证了原子性，详细的可以参见java.util.concurrent.atomic包下的类，比如AtomicInteger。 7、什么是线程安全 又是一个理论的问题，各式各样的答案有很多，我给出一个个人认为解释地最好的：如果你的代码在多线程下执行和在单线程下执行永远都能获得一样的结果，那么你的代码就是线程安全的。 这个问题有值得一提的地方，就是线程安全也是有几个级别的： （1）不可变 像String、Integer、Long这些，都是final类型的类，任何一个线程都改变不了它们的值，要改变除非新创建一个，因此这些不可变对象不需要任何同步手段就可以直接在多线程环境下使用 （2）绝对线程安全 不管运行时环境如何，调用者都不需要额外的同步措施。要做到这一点通常需要付出许多额外的代价，Java中标注自己是线程安全的类，实际上绝大多数都不是线程安全的，不过绝对线程安全的类，Java中也有，比方说CopyOnWriteArrayList、CopyOnWriteArraySet （3）相对线程安全 相对线程安全也就是我们通常意义上所说的线程安全，像Vector这种，add、remove方法都是原子操作，不会被打断，但也仅限于此，如果有个线程在遍历某个Vector、有个线程同时在add这个Vector，99%的情况下都会出现ConcurrentModificationException，也就是fail-fast机制。 （4）线程非安全 这个就没什么好说的了，ArrayList、LinkedList、HashMap等都是线程非安全的类 8、Java中如何获取到线程dump文件 死循环、死锁、阻塞、页面打开慢等问题，打线程dump是最好的解决问题的途径。所谓线程dump也就是线程堆栈，获取到线程堆栈有两步： （1）获取到线程的pid，可以通过使用jps命令，在Linux环境下还可以使用ps -ef | grep java （2）打印线程堆栈，可以通过使用jstack pid命令，在Linux环境下还可以使用kill -3 pid 另外提一点，Thread类提供了一个getStackTrace()方法也可以用于获取线程堆栈。这是一个实例方法，因此此方法是和具体线程实例绑定的，每次获取获取到的是具体某个线程当前运行的堆栈， 9、一个线程如果出现了运行时异常会怎么样 如果这个异常没有被捕获的话，这个线程就停止执行了。另外重要的一点是：如果这个线程持有某个某个对象的监视器，那么这个对象监视器会被立即释放 10、如何在两个线程之间共享数据 通过在线程之间共享对象就可以了，然后通过wait/notify/notifyAll、await/signal/signalAll进行唤起和等待，比方说阻塞队列BlockingQueue就是为线程之间共享数据而设计的 11、sleep方法和wait方法有什么区别 这个问题常问，sleep方法和wait方法都可以用来放弃CPU一定的时间，不同点在于如果线程持有某个对象的监视器，sleep方法不会放弃这个对象的监视器，wait方法会放弃这个对象的监视器 12、生产者消费者模型的作用是什么 这个问题很理论，但是很重要： （1）通过平衡生产者的生产能力和消费者的消费能力来提升整个系统的运行效率，这是生产者消费者模型最重要的作用 （2）解耦，这是生产者消费者模型附带的作用，解耦意味着生产者和消费者之间的联系少，联系越少越可以独自发展而不需要收到相互的制约 13、ThreadLocal有什么用 简单说ThreadLocal就是一种以空间换时间的做法，在每个Thread里面维护了一个以开地址法实现的ThreadLocal.ThreadLocalMap，把数据进行隔离，数据不共享，自然就没有线程安全方面的问题了 14、为什么wait()方法和notify()/notifyAll()方法要在同步块中被调用 这是JDK强制的，wait()方法和notify()/notifyAll()方法在调用前都必须先获得对象的锁 15、wait()方法和notify()/notifyAll()方法在放弃对象监视器时有什么区别 wait()方法和notify()/notifyAll()方法在放弃对象监视器的时候的区别在于：wait()方法立即释放对象监视器，notify()/notifyAll()方法则会等待线程剩余代码执行完毕才会放弃对象监视器。 16、为什么要使用线程池 避免频繁地创建和销毁线程，达到线程对象的重用。另外，使用线程池还可以根据项目灵活地控制并发的数目。 17、怎么检测一个线程是否持有对象监视器 我也是在网上看到一道多线程面试题才知道有方法可以判断某个线程是否持有对象监视器：Thread类提供了一个holdsLock(Object obj)方法，当且仅当对象obj的监视器被某条线程持有的时候才会返回true，注意这是一个static方法，这意味着“某条线程”指的是当前线程。 18、synchronized和ReentrantLock的区别 synchronized是和if、else、for、while一样的关键字，ReentrantLock是类，这是二者的本质区别。既然ReentrantLock是类，那么它就提供了比synchronized更多更灵活的特性，可以被继承、可以有方法、可以有各种各样的类变量，ReentrantLock比synchronized的扩展性体现在几点上： （1）ReentrantLock可以对获取锁的等待时间进行设置，这样就避免了死锁 （2）ReentrantLock可以获取各种锁的信息 （3）ReentrantLock可以灵活地实现多路通知 另外，二者的锁机制其实也是不一样的。ReentrantLock底层调用的是Unsafe的park方法加锁，synchronized操作的应该是对象头中mark word，这点我不能确定。 19、ConcurrentHashMap的并发度是什么 ConcurrentHashMap的并发度就是segment的大小，默认为16，这意味着最多同时可以有16条线程操作ConcurrentHashMap，这也是ConcurrentHashMap对Hashtable的最大优势，任何情况下，Hashtable能同时有两条线程获取Hashtable中的数据吗？ 20、ReadWriteLock是什么 首先明确一下，不是说ReentrantLock不好，只是ReentrantLock某些时候有局限。如果使用ReentrantLock，可能本身是为了防止线程A在写数据、线程B在读数据造成的数据不一致，但这样，如果线程C在读数据、线程D也在读数据，读数据是不会改变数据的，没有必要加锁，但是还是加锁了，降低了程序的性能。 因为这个，才诞生了读写锁ReadWriteLock。ReadWriteLock是一个读写锁接口，ReentrantReadWriteLock是ReadWriteLock接口的一个具体实现，实现了读写的分离，读锁是共享的，写锁是独占的，读和读之间不会互斥，读和写、写和读、写和写之间才会互斥，提升了读写的性能。 21、FutureTask是什么 这个其实前面有提到过，FutureTask表示一个异步运算的任务。FutureTask里面可以传入一个Callable的具体实现类，可以对这个异步运算的任务的结果进行等待获取、判断是否已经完成、取消任务等操作。当然，由于FutureTask也是Runnable接口的实现类，所以FutureTask也可以放入线程池中。 22、Linux环境下如何查找哪个线程使用CPU最长 这是一个比较偏实践的问题，这种问题我觉得挺有意义的。可以这么做： （1）获取项目的pid，jps或者ps -ef | grep java，这个前面有讲过 （2）top -H -p pid，顺序不能改变 这样就可以打印出当前的项目，每条线程占用CPU时间的百分比。注意这里打出的是LWP，也就是操作系统原生线程的线程号，我笔记本山没有部署Linux环境下的Java工程，因此没有办法截图演示，网友朋友们如果公司是使用Linux环境部署项目的话，可以尝试一下。 使用”top -H -p pid”+”jps pid”可以很容易地找到某条占用CPU高的线程的线程堆栈，从而定位占用CPU高的原因，一般是因为不当的代码操作导致了死循环。 最后提一点，”top -H -p pid”打出来的LWP是十进制的，”jps pid”打出来的本地线程号是十六进制的，转换一下，就能定位到占用CPU高的线程的当前线程堆栈了。 23、Java编程写一个会导致死锁的程序 第一次看到这个题目，觉得这是一个非常好的问题。很多人都知道死锁是怎么一回事儿：线程A和线程B相互等待对方持有的锁导致程序无限死循环下去。当然也仅限于此了，问一下怎么写一个死锁的程序就不知道了，这种情况说白了就是不懂什么是死锁，懂一个理论就完事儿了，实践中碰到死锁的问题基本上是看不出来的。 真正理解什么是死锁，这个问题其实不难，几个步骤： （1）两个线程里面分别持有两个Object对象：lock1和lock2。这两个lock作为同步代码块的锁； （2）线程1的run()方法中同步代码块先获取lock1的对象锁，Thread.sleep(xxx)，时间不需要太多，50毫秒差不多了，然后接着获取lock2的对象锁。这么做主要是为了防止线程1启动一下子就连续获得了lock1和lock2两个对象的对象锁 （3）线程2的run)(方法中同步代码块先获取lock2的对象锁，接着获取lock1的对象锁，当然这时lock1的对象锁已经被线程1锁持有，线程2肯定是要等待线程1释放lock1的对象锁的 这样，线程1”睡觉”睡完，线程2已经获取了lock2的对象锁了，线程1此时尝试获取lock2的对象锁，便被阻塞，此时一个死锁就形成了。代码就不写了，占的篇幅有点多，Java多线程7：死锁这篇文章里面有，就是上面步骤的代码实现。 24、怎么唤醒一个阻塞的线程 如果线程是因为调用了wait()、sleep()或者join()方法而导致的阻塞，可以中断线程，并且通过抛出InterruptedException来唤醒它；如果线程遇到了IO阻塞，无能为力，因为IO是操作系统实现的，Java代码并没有办法直接接触到操作系统。 25、不可变对象对多线程有什么帮助 前面有提到过的一个问题，不可变对象保证了对象的内存可见性，对不可变对象的读取不需要进行额外的同步手段，提升了代码执行效率。 26、什么是多线程的上下文切换 多线程的上下文切换是指CPU控制权由一个已经正在运行的线程切换到另外一个就绪并等待获取CPU执行权的线程的过程。 27、如果你提交任务时，线程池队列已满，这时会发生什么 这里区分一下： 如果使用的是无界队列LinkedBlockingQueue，也就是无界队列的话，没关系，继续添加任务到阻塞队列中等待执行，因为LinkedBlockingQueue可以近乎认为是一个无穷大的队列，可以无限存放任务 如果使用的是有界队列比如ArrayBlockingQueue，任务首先会被添加到ArrayBlockingQueue中，ArrayBlockingQueue满了，会根据maximumPoolSize的值增加线程数量，如果增加了线程数量还是处理不过来，ArrayBlockingQueue继续满，那么则会使用拒绝策略RejectedExecutionHandler处理满了的任务，默认是AbortPolicy 28、Java中用到的线程调度算法是什么 抢占式。一个线程用完CPU之后，操作系统会根据线程优先级、线程饥饿情况等数据算出一个总的优先级并分配下一个时间片给某个线程执行。 29、Thread.sleep(0)的作用是什么 这个问题和上面那个问题是相关的，我就连在一起了。由于Java采用抢占式的线程调度算法，因此可能会出现某条线程常常获取到CPU控制权的情况，为了让某些优先级比较低的线程也能获取到CPU控制权，可以使用Thread.sleep(0)手动触发一次操作系统分配时间片的操作，这也是平衡CPU控制权的一种操作。 30、什么是自旋 很多synchronized里面的代码只是一些很简单的代码，执行时间非常快，此时等待的线程都加锁可能是一种不太值得的操作，因为线程阻塞涉及到用户态和内核态切换的问题。既然synchronized里面的代码执行得非常快，不妨让等待锁的线程不要被阻塞，而是在synchronized的边界做忙循环，这就是自旋。如果做了多次忙循环发现还没有获得锁，再阻塞，这样可能是一种更好的策略。 31、什么是Java内存模型 Java内存模型定义了一种多线程访问Java内存的规范。Java内存模型要完整讲不是这里几句话能说清楚的，我简单总结一下Java内存模型的几部分内容： （1）Java内存模型将内存分为了主内存和工作内存。类的状态，也就是类之间共享的变量，是存储在主内存中的，每次Java线程用到这些主内存中的变量的时候，会读一次主内存中的变量，并让这些内存在自己的工作内存中有一份拷贝，运行自己线程代码的时候，用到这些变量，操作的都是自己工作内存中的那一份。在线程代码执行完毕之后，会将最新的值更新到主内存中去 （2）定义了几个原子操作，用于操作主内存和工作内存中的变量 （3）定义了volatile变量的使用规则 （4）happens-before，即先行发生原则，定义了操作A必然先行发生于操作B的一些规则，比如在同一个线程内控制流前面的代码一定先行发生于控制流后面的代码、一个释放锁unlock的动作一定先行发生于后面对于同一个锁进行锁定lock的动作等等，只要符合这些规则，则不需要额外做同步措施，如果某段代码不符合所有的happens-before规则，则这段代码一定是线程非安全的 32、什么是CAS CAS，全称为Compare and Swap，即比较-替换。假设有三个操作数：内存值V、旧的预期值A、要修改的值B，当且仅当预期值A和内存值V相同时，才会将内存值修改为B并返回true，否则什么都不做并返回false。当然CAS一定要volatile变量配合，这样才能保证每次拿到的变量是主内存中最新的那个值，否则旧的预期值A对某条线程来说，永远是一个不会变的值A，只要某次CAS操作失败，永远都不可能成功。 33、什么是乐观锁和悲观锁 （1）乐观锁：就像它的名字一样，对于并发间操作产生的线程安全问题持乐观状态，乐观锁认为竞争不总是会发生，因此它不需要持有锁，将比较-替换这两个动作作为一个原子操作尝试去修改内存中的变量，如果失败则表示发生冲突，那么就应该有相应的重试逻辑。 （2）悲观锁：还是像它的名字一样，对于并发间操作产生的线程安全问题持悲观状态，悲观锁认为竞争总是会发生，因此每次对某资源进行操作时，都会持有一个独占的锁，就像synchronized，不管三七二十一，直接上了锁就操作资源了。 34、什么是AQS 简单说一下AQS，AQS全称为AbstractQueuedSychronizer，翻译过来应该是抽象队列同步器。 如果说java.util.concurrent的基础是CAS的话，那么AQS就是整个Java并发包的核心了，ReentrantLock、CountDownLatch、Semaphore等等都用到了它。AQS实际上以双向队列的形式连接所有的Entry，比方说ReentrantLock，所有等待的线程都被放在一个Entry中并连成双向队列，前面一个线程使用ReentrantLock好了，则双向队列实际上的第一个Entry开始运行。 AQS定义了对双向队列所有的操作，而只开放了tryLock和tryRelease方法给开发者使用，开发者可以根据自己的实现重写tryLock和tryRelease方法，以实现自己的并发功能。 35、单例模式的线程安全性 老生常谈的问题了，首先要说的是单例模式的线程安全意味着：某个类的实例在多线程环境下只会被创建一次出来。单例模式有很多种的写法，我总结一下： （1）饿汉式单例模式的写法：线程安全 （2）懒汉式单例模式的写法：非线程安全 （3）双检锁单例模式的写法：线程安全 36、Semaphore有什么作用 Semaphore就是一个信号量，它的作用是限制某段代码块的并发数。Semaphore有一个构造函数，可以传入一个int型整数n，表示某段代码最多只有n个线程可以访问，如果超出了n，那么请等待，等到某个线程执行完毕这段代码块，下一个线程再进入。由此可以看出如果Semaphore构造函数中传入的int型整数n=1，相当于变成了一个synchronized了。 37、Hashtable的size()方法中明明只有一条语句”return count”，为什么还要做同步？ 这是我之前的一个困惑，不知道大家有没有想过这个问题。某个方法中如果有多条语句，并且都在操作同一个类变量，那么在多线程环境下不加锁，势必会引发线程安全问题，这很好理解，但是size()方法明明只有一条语句，为什么还要加锁？ 关于这个问题，在慢慢地工作、学习中，有了理解，主要原因有两点： （1）同一时间只能有一条线程执行固定类的同步方法，但是对于类的非同步方法，可以多条线程同时访问。所以，这样就有问题了，可能线程A在执行Hashtable的put方法添加数据，线程B则可以正常调用size()方法读取Hashtable中当前元素的个数，那读取到的值可能不是最新的，可能线程A添加了完了数据，但是没有对size++，线程B就已经读取size了，那么对于线程B来说读取到的size一定是不准确的。而给size()方法加了同步之后，意味着线程B调用size()方法只有在线程A调用put方法完毕之后才可以调用，这样就保证了线程安全性 （2）CPU执行代码，执行的不是Java代码，这点很关键，一定得记住。Java代码最终是被翻译成机器码执行的，机器码才是真正可以和硬件电路交互的代码。即使你看到Java代码只有一行，甚至你看到Java代码编译之后生成的字节码也只有一行，也不意味着对于底层来说这句语句的操作只有一个。一句”return count”假设被翻译成了三句汇编语句执行，一句汇编语句和其机器码做对应，完全可能执行完第一句，线程就切换了。 38、线程类的构造方法、静态块是被哪个线程调用的 这是一个非常刁钻和狡猾的问题。请记住：线程类的构造方法、静态块是被new这个线程类所在的线程所调用的，而run方法里面的代码才是被线程自身所调用的。 如果说上面的说法让你感到困惑，那么我举个例子，假设Thread2中new了Thread1，main函数中new了Thread2，那么： （1）Thread2的构造方法、静态块是main线程调用的，Thread2的run()方法是Thread2自己调用的 （2）Thread1的构造方法、静态块是Thread2调用的，Thread1的run()方法是Thread1自己调用的 39、同步方法和同步块，哪个是更好的选择 同步块，这意味着同步块之外的代码是异步执行的，这比同步整个方法更提升代码的效率。请知道一条原则：同步的范围越小越好。 借着这一条，我额外提一点，虽说同步的范围越少越好，但是在Java虚拟机中还是存在着一种叫做锁粗化的优化方法，这种方法就是把同步范围变大。这是有用的，比方说StringBuffer，它是一个线程安全的类，自然最常用的append()方法是一个同步方法，我们写代码的时候会反复append字符串，这意味着要进行反复的加锁-&gt;解锁，这对性能不利，因为这意味着Java虚拟机在这条线程上要反复地在内核态和用户态之间进行切换，因此Java虚拟机会将多次append方法调用的代码进行一个锁粗化的操作，将多次的append的操作扩展到append方法的头尾，变成一个大的同步块，这样就减少了加锁–&gt;解锁的次数，有效地提升了代码执行的效率。 40、高并发、任务执行时间短的业务怎样使用线程池？并发不高、任务执行时间长的业务怎样使用线程池？并发高、业务执行时间长的业务怎样使用线程池？ 这是我在并发编程网上看到的一个问题，把这个问题放在最后一个，希望每个人都能看到并且思考一下，因为这个问题非常好、非常实际、非常专业。关于这个问题，个人看法是： （1）高并发、任务执行时间短的业务，线程池线程数可以设置为CPU核数+1，减少线程上下文的切换 （2）并发不高、任务执行时间长的业务要区分开看： a）假如是业务时间长集中在IO操作上，也就是IO密集型的任务，因为IO操作并不占用CPU，所以不要让所有的CPU闲下来，可以加大线程池中的线程数目，让CPU处理更多的业务 b）假如是业务时间长集中在计算操作上，也就是计算密集型任务，这个就没办法了，和（1）一样吧，线程池中的线程数设置得少一些，减少线程上下文的切换 （3）并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计，看看这些业务里面某些数据是否能做缓存是第一步，增加服务器是第二步，至于线程池的设置，设置参考（2）。最后，业务执行时间长的问题，也可能需要分析一下，看看能不能使用中间件对任务进行拆分和解耦。 ================================================================================== 我不能保证写的每个地方都是对的，但是至少能保证不复制、不黏贴，保证每一句话、每一行代码都经过了认真的推敲、仔细的斟酌。每一篇文章的背后，希望都能看到自己对于技术、对于生活的态度。 我相信乔布斯说的，只有那些疯狂到认为自己可以改变世界的人才能真正地改变世界。面对压力，我可以挑灯夜战、不眠不休；面对困难，我愿意迎难而上、永不退缩。 其实我想说的是，我只是一个程序员，这就是我现在纯粹人生的全部。 ==================================================================================]]></content>
      <categories>
        <category>java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[前端各种赋值操作]]></title>
    <url>%2F2018%2F07%2F09%2F%E5%89%8D%E7%AB%AF%E5%90%84%E7%A7%8D%E8%B5%8B%E5%80%BC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[select选中 12345678910$('#id option\[value="' \+ option的value值 \+ '"\]').prop('selected', true);checkbox选中if(rtn.gradeTypes)&#123; gradeTypes = rtn.gradeTypes.split(",") for (var i=0;i&lt;gradeTypes.length;i++)&#123; $('input\[name="gradeType"\]\[value="'+gradeTypes\[i\]+'"\]').prop("checked",true); &#125;&#125; 操作数组 123var s = "525,626,485";s = s.split(","); //s会变成数组 \["525", "626", "485"\];s = s.join(",") //s 就会变成逗号隔开 "525,626,485";]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hql]]></title>
    <url>%2F2018%2F07%2F09%2Fhql-e5-86-99sql%2F</url>
    <content type="text"><![CDATA[一.hql默认Query是JPQL，不是SQL。执行SQL加nativeQuery=true 二.模糊查询范例： 根据gradeTypes模糊查询 @Query(“select distinct (mapping.course) from CourseAttributeMapping mapping where mapping.course.gradeTypes like CONCAT(‘%’,:gradeTypes,’%’) and mapping.deleted =0 and mapping.course.deleted =0 and mapping.detail.deleted =0 and mapping.attribute.deleted = 0 and mapping.detail.id = :detailId and mapping.course.customer.id = :customerId”)public List findCourseByDetailAndGradeTypes(@Param(“detailId”) String detailId, @Param(“gradeTypes”) String gradeTypes,@Param(“customerId”)String customerId); 三.不等于范例： content不等于null @Query(“select count(b.id) from BallotBatchMap b where b.content &lt;&gt; null and b.batch.id=:batchId and b.deleted = 0”)Integer findContentByBatchId(@Param(“batchId”) String batchId);]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[binlog日志详解释]]></title>
    <url>%2F2018%2F07%2F04%2Fbinlog%E6%97%A5%E5%BF%97%E8%AF%A6%E8%A7%A3%E9%87%8A%2F</url>
    <content type="text"><![CDATA[转自：https://blog.csdn.net/h348592532/article/details/78121962binlog 基本认识 MySQL的二进制日志可以说是MySQL最重要的日志了，它记录了所有的DDL和DML(除了数据查询语句)语句，以事件形式记录，还包含语句所执行的消耗的时间，MySQL的二进制日志是事务安全型的。 一般来说开启二进制日志大概会有1%的性能损耗(参见MySQL官方中文手册 5.1.24版)。二进制有两个最重要的使用场景: 其一：MySQL Replication在Master端开启binlog，Mster把它的二进制日志传递给slaves来达到master-slave数据一致的目的。 其二：自然就是数据恢复了，通过使用mysqlbinlog工具来使恢复数据。 二进制日志包括两类文件：二进制日志索引文件（文件名后缀为.index）用于记录所有的二进制文件，二进制日志文件（文件名后缀为.00000*）记录数据库所有的DDL和DML(除了数据查询语句)语句事件。 一、开启binlog日志： vi编辑打开mysql配置文件 # vi /usr/local/mysql/etc/my.cnf 在[mysqld] 区块 设置/添加 log-bin=mysql-bin 确认是打开状态(值 mysql-bin 是日志的基本名或前缀名)； 重启mysqld服务使配置生效 # pkill mysqld # /usr/local/mysql/bin/mysqld_safe --user=mysql &amp; 二、也可登录mysql服务器，通过mysql的变量配置表，查看二进制日志是否已开启 单词：variable[ˈvɛriəbəl] 变量 登录服务器 # /usr/local/mysql/bin/mysql -uroot -p123456 mysql\&gt; show variables like &apos;log_%&apos;; +----------------------------------------+---------------------------------------+ | Variable_name | Value | +----------------------------------------+---------------------------------------+ | log_bin | ON | ------&gt; ON表示已经开启binlog日志 | log\_bin\_basename | /usr/local/mysql/data/mysql-bin | | log\_bin\_index | /usr/local/mysql/data/mysql-bin.index | | log\_bin\_trust\_function\_creators | OFF | | log\_bin\_use\_v1\_row_events | OFF | | log_error | /usr/local/mysql/data/martin.err | | log_output | FILE | | log\_queries\_not\_using\_indexes | OFF | | log\_slave\_updates | OFF | | log\_slow\_admin_statements | OFF | | log\_slow\_slave_statements | OFF | | log\_throttle\_queries\_not\_using_indexes | 0 | | log_warnings | 1 | +----------------------------------------+---------------------------------------+ 三、常用binlog日志操作命令 1.查看所有binlog日志列表 mysql&gt; show master logs; 2.查看master状态，即最后(最新)一个binlog日志的编号名称，及其最后一个操作事件pos结束点(Position)值 mysql&gt; show master status; 3.刷新log日志，自此刻开始产生一个新编号的binlog日志文件 mysql\&gt; flush logs; 注：每当mysqld服务重启时，会自动执行此命令，刷新binlog日志；在mysqldump备份数据时加 -F 选项也会刷新binlog日志； 4.重置(清空)所有binlog日志 mysql\&gt; reset master;四、查看某个binlog日志内容，常用有两种方式： 1.使用mysqlbinlog自带查看命令法： 注: binlog是二进制文件，普通文件查看器cat more vi等都无法打开，必须使用自带的 mysqlbinlog 命令查看 binlog日志与数据库文件在同目录中(我的环境配置安装是选择在/usr/local/mysql/data中) 在MySQL5.5以下版本使用mysqlbinlog命令时如果报错，就加上 “–no-defaults”选项 # /usr/local/mysql/bin/mysqlbinlog /usr/local/mysql/data/mysql-bin.000013 下面截取一个片段分析： ……………………………………………………………………. # at 552 #131128 17:50:46 server id 1 end_log_pos 665 Query thread_id=11 exec_time=0 error_code=0 —-&gt;执行时间:17:50:46；pos点:665 SET TIMESTAMP=1385632246/!/; update zyyshop.stu set name=’李四’ where id=4 —-&gt;执行的SQL /!/; # at 665 #131128 17:50:46 server id 1 end_log_pos 692 Xid = 1454 —-&gt;执行时间:17:50:46；pos点:692 ……………………………………………………………………. 注: server id 1 数据库主机的服务号； end_log_pos 665 pos点 thread_id=11 线程号 2.上面这种办法读取出binlog日志的全文内容较多，不容易分辨查看pos点信息，这里介绍一种更为方便的查询命令： mysql&gt; show binlog events [IN ‘log_name’] [FROM pos] [LIMIT [offset,] row_count]; 选项解析： IN ‘log_name’ 指定要查询的binlog文件名(不指定就是第一个binlog文件) FROM pos 指定从哪个pos起始点开始查起(不指定就是从整个文件首个pos点开始算) LIMIT [offset,] 偏移量(不指定就是0) row_count 查询总条数(不指定就是所有行) 截取部分查询结果： *************************** 20. row *** Log_name: mysql-bin.000021 ———————————————-&gt; 查询的binlog日志文件名 Pos: 11197 ———————————————————-&gt; pos起始点: Event_type: Query ———————————————————-&gt; 事件类型：Query Server_id: 1 ————————————————————–&gt; 标识是由哪台服务器执行的 End_log_pos: 11308 ———————————————————-&gt; pos结束点:11308(即：下行的pos起始点) Info: use `zyyshop`; INSERT INTO `team2` VALUES (0,345,’asdf8er5’) —&gt; 执行的sql语句 *************************** 21. row *** Log_name: mysql-bin.000021 Pos: 11308 ———————————————————-&gt; pos起始点:11308(即：上行的pos结束点) Event_type: Query Server_id: 1 End_log_pos: 11417 Info: use `zyyshop`; /!40000 ALTER TABLE `team2` ENABLE KEYS / \*************************\ 22. row *** Log_name: mysql-bin.000021 Pos: 11417 Event_type: Query Server_id: 1 End_log_pos: 11510 Info: use `zyyshop`; DROP TABLE IF EXISTS `type` 这条语句可以将指定的binlog日志文件，分成有效事件行的方式返回，并可使用limit指定pos点的起始偏移，查询条数； A.查询第一个(最早)的binlog日志： mysql&gt; show binlog events\G; B.指定查询 mysql-bin.000021 这个文件： mysql&gt; show binlog events in ‘mysql-bin.000021’\G; C.指定查询 mysql-bin.000021 这个文件，从pos点:8224开始查起： mysql&gt; show binlog events in ‘mysql-bin.000021’ from 8224\G; D.指定查询 mysql-bin.000021 这个文件，从pos点:8224开始查起，查询10条 mysql&gt; show binlog events in ‘mysql-bin.000021’ from 8224 limit 10\G; E.指定查询 mysql-bin.000021 这个文件，从pos点:8224开始查起，偏移2行，查询10条 mysql&gt; show binlog events in ‘mysql-bin.000021’ from 8224 limit 2,10\G; 五、恢复binlog日志实验(zyyshop是数据库) 1.假设现在是凌晨4:00，我的计划任务开始执行一次完整的数据库备份： 将zyyshop数据库备份到 /root/BAK.zyyshop.sql 文件中： # /usr/local/mysql/bin/mysqldump -uroot -p123456 -lF --log-error=/root/myDump.err -B zyyshop &gt; /root/BAK.zyyshop.sql ...... 大约过了若干分钟，备份完成了，我不用担心数据丢失了，因为我有备份了，嘎嘎~~~ 由于我使用了-F选项，当备份工作刚开始时系统会刷新log日志，产生新的binlog日志来记录备份之后的数据库“增删改”操作，查看一下： mysql&gt; show master status; +------------------+----------+--------------+------------------+ | File | Position | Binlog\_Do\_DB | Binlog\_Ignore\_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000023 | 120 | | | +------------------+----------+--------------+------------------+ 也就是说， mysql-bin.000023 是用来记录4:00之后对数据库的所有“增删改”操作。 2.早9:00上班了，业务的需求会对数据库进行各种“增删改”操作~~~~~~~ @ 比如：创建一个学生表并插入、修改了数据等等： CREATE TABLE IF NOT EXISTS \`tt\` ( \`id\` int(10) unsigned NOT NULL AUTO_INCREMENT, \`name\` varchar(16) NOT NULL, \`sex\` enum(&apos;m&apos;,&apos;w&apos;) NOT NULL DEFAULT &apos;m&apos;, \`age\` tinyint(3) unsigned NOT NULL, \`classid\` char(6) DEFAULT NULL, PRIMARY KEY (\`id\`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; 导入实验数据 mysql\&gt; insert into zyyshop.tt(\`name\`,\`sex\`,\`age\`,\`classid\`) values(&apos;yiyi&apos;,&apos;w&apos;,20,&apos;cls1&apos;),(&apos;xiaoer&apos;,&apos;m&apos;,22,&apos;cls3&apos;),(&apos;zhangsan&apos;,&apos;w&apos;,21,&apos;cls5&apos;),(&apos;lisi&apos;,&apos;m&apos;,20,&apos;cls4&apos;),(&apos;wangwu&apos;,&apos;w&apos;,26,&apos;cls6&apos;); 查看数据 mysql\&gt; select * from zyyshop.tt; +----+----------+-----+-----+---------+ | id | name | sex | age | classid | +----+----------+-----+-----+---------+ | 1 | yiyi | w | 20 | cls1 | | 2 | xiaoer | m | 22 | cls3 | | 3 | zhangsan | w | 21 | cls5 | | 4 | lisi | m | 20 | cls4 | | 5 | wangwu | w | 26 | cls6 | +----+----------+-----+-----+---------+ 中午时分又执行了修改数据操作 mysql\&gt; update zyyshop.tt set name=&apos;李四&apos; where id=4; mysql\&gt; update zyyshop.tt set name=&apos;小二&apos; where id=2; 修改后的结果： mysql\&gt; select * from zyyshop.tt; +----+----------+-----+-----+---------+ | id | name | sex | age | classid | +----+----------+-----+-----+---------+ | 1 | yiyi | w | 20 | cls1 | | 2 | 小二 | m | 22 | cls3 | | 3 | zhangsan | w | 21 | cls5 | | 4 | 李四 | m | 20 | cls4 | | 5 | wangwu | w | 26 | cls6 | +----+----------+-----+-----+---------+ 假设此时是下午18:00，莫名地执行了一条悲催的SQL语句，整个数据库都没了： mysql&gt; drop database zyyshop; 3.此刻杯具了，别慌！先仔细查看最后一个binlog日志，并记录下关键的pos点，到底是哪个pos点的操作导致了数据库的破坏(通常在最后几步)； 备份一下最后一个binlog日志文件： # ll /usr/local/mysql/data | grep mysql-bin # cp -v /usr/local/mysql/data/mysql-bin.000023 /root/ 此时执行一次刷新日志索引操作，重新开始新的binlog日志记录文件，理论说 mysql-bin.000023 这个文件不会再有后续写入了(便于我们分析原因及查找pos点)，以后所有数据库操作都会写入到下一个日志文件； mysql&gt; flush logs; mysql&gt; show master status; 4.读取binlog日志，分析问题 方式一：使用mysqlbinlog读取binlog日志： # /usr/local/mysql/bin/mysqlbinlog /usr/local/mysql/data/mysql-bin.000023 方式二：登录服务器，并查看(推荐)： mysql&gt; show binlog events in ‘mysql-bin.000023’; 以下为末尾片段： +------------------+------+------------+-----------+-------------+------------------------------------------------------------+ | Log_name | Pos | Event\_type | Server\_id | End\_log\_pos | Info | +------------------+------+------------+-----------+-------------+------------------------------------------------------------+ | mysql-bin.000023 | 922 | Xid | 1 | 953 | COMMIT /* xid=3820 */ | | mysql-bin.000023 | 953 | Query | 1 | 1038 | BEGIN | | mysql-bin.000023 | 1038 | Query | 1 | 1164 | use \`zyyshop\`; update zyyshop.tt set name=&apos;李四&apos; where id=4| | mysql-bin.000023 | 1164 | Xid | 1 | 1195 | COMMIT /* xid=3822 */ | | mysql-bin.000023 | 1195 | Query | 1 | 1280 | BEGIN | | mysql-bin.000023 | 1280 | Query | 1 | 1406 | use \`zyyshop\`; update zyyshop.tt set name=&apos;小二&apos; where id=2| | mysql-bin.000023 | 1406 | Xid | 1 | 1437 | COMMIT /* xid=3823 */ | | mysql-bin.000023 | 1437 | Query | 1 | 1538 | drop database zyyshop | +------------------+------+------------+-----------+-------------+------------------------------------------------------------+ 通过分析，造成数据库破坏的pos点区间是介于 1437--1538 之间，只要恢复到1437前就可。 5.现在把凌晨备份的数据恢复： # /usr/local/mysql/bin/mysql -uroot -p123456 -v &lt; /root/BAK.zyyshop.sql; 注: 至此截至当日凌晨(4:00)前的备份数据都恢复了。 但今天一整天(4:00–18:00)的数据肿么办呢？就得从前文提到的 mysql-bin.000023 新日志做文章了…… 6.从binlog日志恢复数据 恢复语法格式： # mysqlbinlog mysql-bin.0000xx | mysql -u用户名 -p密码 数据库名 常用选项： –start-position=953 起始pos点 –stop-position=1437 结束pos点 –start-datetime=”2013-11-29 13:18:54” 起始时间点 –stop-datetime=”2013-11-29 13:21:53” 结束时间点 –database=zyyshop 指定只恢复zyyshop数据库(一台主机上往往有多个数据库，只限本地log日志) 不常用选项： -u --user=name Connect to the remote server as username.连接到远程主机的用户名 -p --password\[=name\] Password to connect to remote server.连接到远程主机的密码 -h --host=name Get the binlog from server.从远程主机上获取binlog日志 --read-from-remote-server Read binary logs from a MySQL server.从某个MySQL服务器上读取binlog日志 小结：实际是将读出的binlog日志内容，通过管道符传递给mysql命令。这些命令、文件尽量写成绝对路径； A.完全恢复(本例不靠谱，因为最后那条 drop database zyyshop 也在日志里，必须想办法把这条破坏语句排除掉，做部分恢复) # /usr/local/mysql/bin/mysqlbinlog /usr/local/mysql/data/mysql-bin.000021 | /usr/local/mysql/bin/mysql -uroot -p123456 -v zyyshop B.指定pos结束点恢复(部分恢复)： @ –stop-position=953 pos结束点 注：此pos结束点介于“导入实验数据”与更新“name=’李四’”之间，这样可以恢复到更改“name=’李四’”之前的“导入测试数据” # /usr/local/mysql/bin/mysqlbinlog –stop-position=953 –database=zyyshop /usr/local/mysql/data/mysql-bin.000023 | /usr/local/mysql/bin/mysql -uroot -p123456 -v zyyshop 在另一终端登录查看结果(成功恢复了)： mysql&gt; select * from zyyshop.tt; +—-+———-+—–+—–+———+ | id | name | sex | age | classid | +—-+———-+—–+—–+———+ | 1 | yiyi | w | 20 | cls1 | | 2 | xiaoer | m | 22 | cls3 | | 3 | zhangsan | w | 21 | cls5 | | 4 | lisi | m | 20 | cls4 | | 5 | wangwu | w | 26 | cls6 | +—-+———-+—–+—–+———+ C.指定pso点区间恢复(部分恢复)： 更新 name=’李四’ 这条数据，日志区间是Pos[1038] –&gt; End_log_pos[1164]，按事务区间是：Pos[953] –&gt; End_log_pos[1195]； 更新 name=&apos;小二&apos; 这条数据，日志区间是Pos\[1280\] --&gt; End\_log\_pos\[1406\]，按事务区间是：Pos\[1195\] --&gt; End\_log\_pos\[1437\]； c1.单独恢复 name=&apos;李四&apos; 这步操作，可这样： # /usr/local/mysql/bin/mysqlbinlog --start-position=1038 --stop-position=1164 --database=zyyshop /usr/local/mysql/data/mysql-bin.000023 | /usr/local/mysql/bin/mysql -uroot -p123456 -v zyyshop 也可以按事务区间单独恢复，如下： # /usr/local/mysql/bin/mysqlbinlog –start-position=953 –stop-position=1195 –database=zyyshop /usr/local/mysql/data/mysql-bin.000023 | /usr/local/mysql/bin/mysql -uroot -p123456 -v zyyshop c2.单独恢复 name=’小二’ 这步操作，可这样： # /usr/local/mysql/bin/mysqlbinlog –start-position=1280 –stop-position=1406 –database=zyyshop /usr/local/mysql/data/mysql-bin.000023 | /usr/local/mysql/bin/mysql -uroot -p123456 -v zyyshop 也可以按事务区间单独恢复，如下： # /usr/local/mysql/bin/mysqlbinlog –start-position=1195 –stop-position=1437 –database=zyyshop /usr/local/mysql/data/mysql-bin.000023 | /usr/local/mysql/bin/mysql -uroot -p123456 -v zyyshop c3.将 name=’李四’、name=’小二’ 多步操作一起恢复，需要按事务区间，可这样： # /usr/local/mysql/bin/mysqlbinlog –start-position=953 –stop-position=1437 –database=zyyshop /usr/local/mysql/data/mysql-bin.000023 | /usr/local/mysql/bin/mysql -uroot -p123456 -v zyyshop D.在另一终端登录查看目前结果(两名称也恢复了)： mysql&gt; select * from zyyshop.tt; +—-+———-+—–+—–+———+ | id | name | sex | age | classid | +—-+———-+—–+—–+———+ | 1 | yiyi | w | 20 | cls1 | | 2 | 小二 | m | 22 | cls3 | | 3 | zhangsan | w | 21 | cls5 | | 4 | 李四 | m | 20 | cls4 | | 5 | wangwu | w | 26 | cls6 | +—-+———-+—–+—–+———+ E.也可指定时间区间恢复(部分恢复)：除了用pos点的办法进行恢复，也可以通过指定时间区间进行恢复，按时间恢复需要用mysqlbinlog命令读取binlog日志内容，找时间节点。 比如，我把刚恢复的tt表删除掉，再用时间区间点恢复 mysql&gt; drop table tt; @ --start-datetime=&quot;2013-11-29 13:18:54&quot; 起始时间点 @ --stop-datetime=&quot;2013-11-29 13:21:53&quot; 结束时间点 # /usr/local/mysql/bin/mysqlbinlog --start-datetime=&quot;2013-11-29 13:18:54&quot; --stop-datetime=&quot;2013-11-29 13:21:53&quot; --database=zyyshop /usr/local/mysql/data/mysql-bin.000021 | /usr/local/mysql/bin/mysql -uroot -p123456 -v zyyshop]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[抽象工厂模式]]></title>
    <url>%2F2018%2F07%2F04%2F%E6%8A%BD%E8%B1%A1%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[为Animal创建接口类123public interface Animal &#123; void sounds();&#125; 实体类实现Animal接口123456public class Dog implements Animal &#123; @Override public void sounds() &#123; System.out.println("狗叫汪汪"); &#125;&#125; 123456public class Cat implements Animal &#123; @Override public void sounds() &#123; System.out.println("猫叫喵喵"); &#125;&#125; 为Food创建接口类123public interface Food &#123; void eat();&#125; 实体类实现Food接口123456public class Bone implements Food &#123; @Override public void eat() &#123; System.out.println("狗喜欢吃骨头"); &#125;&#125; 123456public class Fish implements Food &#123; @Override public void eat() &#123; System.out.println("猫喜欢吃鱼"); &#125;&#125; 为Animal和Food对象创建抽象类来获取工厂1234public abstract class AbstractFactory &#123; public abstract Animal getAnimal(String animal); public abstract Food getFood(String food);&#125; 为Animal和Food对象创建各自的工厂类继承AbstractFactory123456789101112131415161718public class AnimalFactory extends AbstractFactory&#123; @Override public Animal getAnimal(String animal) &#123; if("cat".equals(animal))&#123; return new Cat(); &#125; if ("dog".equals(animal)) &#123; return new Dog(); &#125; return null; &#125; @Override public Food getFood(String food) &#123; return null; &#125;&#125; 123456789101112131415161718public class FoodFactory extends AbstractFactory &#123; @Override public Animal getAnimal(String animal) &#123; return null; &#125; @Override public Food getFood(String food) &#123; if("bone".equals(food))&#123; return new Bone(); &#125; if ("fish".equals(food)) &#123; return new Fish(); &#125; return null; &#125;&#125; 创建一个工厂创造器/生成器类，通过传递参数来获取对应工厂1234567891011public class FactoryProducer &#123; public static AbstractFactory getFactory(String choice)&#123; if ("animal".equals(choice)) &#123; return new AnimalFactory(); &#125; if ("food".equals(choice)) &#123; return new FoodFactory(); &#125; return null; &#125;&#125; 测试类检验成果123456789101112131415public class AbstractFactoryDemo &#123; public static void main(String[] args) &#123; AbstractFactory animalFactory = FactoryProducer.getFactory("animal"); Animal dog = animalFactory.getAnimal("dog"); dog.sounds(); Animal cat = animalFactory.getAnimal("cat"); cat.sounds(); AbstractFactory foodFactory = FactoryProducer.getFactory("food"); Food bone = foodFactory.getFood("bone"); bone.eat(); Food fish = foodFactory.getFood("fish"); fish.eat(); &#125;&#125; 输出结果1234狗叫汪汪猫叫喵喵狗喜欢吃骨头猫喜欢吃鱼]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[工厂模式]]></title>
    <url>%2F2018%2F07%2F04%2F%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[工厂模式是我们最常用的实例化对象模式了，是用工厂方法代替new操作的一种模式。 创建接口类public interface Animal { void sounds(); }实体类实现接口public class Dog implements Animal { @Override public void sounds() { System.out.println(&quot;狗叫汪汪&quot;); } } public class Cat implements Animal { @Override public void sounds() { System.out.println(&quot;猫叫喵喵&quot;); } }创建一个工厂类，根据传入的参数生成对应的对象public class AnimalFactory { public Animal getAnimal(String animal) { if (&quot;cat&quot;.equals(animal)) { return new Cat(); } if (&quot;dog&quot;.equals(animal)) { return new Dog(); } return null; } }测试类检验成果public class FactoryPatternDemo { public static void main(String[] args) { AnimalFactory animalFactory = new AnimalFactory(); Animal animal = animalFactory.getAnimal(&quot;cat&quot;); animal.sounds(); Animal animal2 = animalFactory.getAnimal(&quot;dog&quot;); animal2.sounds(); } }输出结果猫叫喵喵 狗叫汪汪]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[单例模式]]></title>
    <url>%2F2018%2F07%2F02%2F%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[懒汉式静态工厂方法，线程不安全，推荐评级：★ 1234567891011121314151617public class Singleton &#123; private static Singleton singleton = null; //构造方法private避免了类在外部被实例化 private Singleton() &#123; &#125; //静态工厂方法 public static Singleton getInstance()&#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; return singleton; &#125;&#125; 线程安全，加锁synchronized，加锁会影响效率，推荐评级：★★ 1234567891011121314151617public class Singleton &#123; private static Singleton singleton = null; //构造方法private避免了类在外部被实例化 private Singleton() &#123; &#125; //加同步synchronized public static synchronized Singleton getSynInstance() &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; return singleton; &#125;&#125; 饿汉式线程安全，实际使用较多，推荐评级：★★★★★ 1234567public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return instance; &#125;&#125; 双重检查锁定线程安全，推荐评级：★★★★ 123456789101112131415161718192021public class Singleton &#123; private static Singleton singleton = null; //构造方法private避免了类在外部被实例化 private Singleton() &#123; &#125; //双重检查锁定 public static Singleton getDoubleCheckInstance()&#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125; 静态内部类线程安全，推荐评级：★★★★★ 1234567891011121314151617public class Singleton &#123; private static Singleton singleton = null; //构造方法private避免了类在外部被实例化 private Singleton() &#123; &#125; //★（推荐）静态内部类(既实现了线程安全，又避免了同步带来的性能影响) private static class LazyHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; public static final Singleton getInnerSingleton()&#123; return LazyHolder.INSTANCE; &#125;&#125; 枚举线程安全，推荐评级：★★★ 12345public enum Singleton &#123; INSTANCE; public void doSomeThing() &#123; &#125; &#125;]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[e.preventDefault(); //阻止 事件的默认操作]]></title>
    <url>%2F2018%2F07%2F02%2Fe.preventDefault()%3B%20%E9%98%BB%E6%AD%A2%20%E4%BA%8B%E4%BB%B6%E7%9A%84%E9%BB%98%E8%AE%A4%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[$(‘body’).on(‘click’,’#course-content’,function (e) { var url = $(‘#course-content’).children(‘a’).attr(‘href’); if(url){ e.preventDefault();//阻止 事件的默认操作 //iframe层 layer.open({ type: 2, title: ‘预览’, shadeClose: true,//点击蒙层关闭 shade: 0.5,//蒙层阴影 maxmin: true, //开启最大化最小化按钮 area: [‘893px’, ‘600px’], content: ‘https://view.officeapps.live.com/op/view.aspx?src=&#39;+encodeURI($(&quot;#course-content&quot;).children().attr(&quot;href&quot;)) }); }})]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JQuery中$.cookie()方法的使用]]></title>
    <url>%2F2018%2F06%2F29%2Fjquery-e4-b8-ad-cookie-e6-96-b9-e6-b3-95-e7-9a-84-e4-bd-bf-e7-94-a8%2F</url>
    <content type="text"><![CDATA[新增cookie如果不设置cookie的有效期，则cookie默认在浏览器关闭前都有效，故被称为”会话cookie”。 $.cookie(‘cookieName’, ‘cookieValue’); 创建一个cookie并设置有效时间为7天: $.cookie(‘cookieName’, ‘cookieValue’, { expires: 7 }); 创建一个cookie并设置cookie的有效路径： $.cookie(‘cookieName’, ‘cookieValue’, { expires: 7, path: ‘/‘ }); 读取cookie若cookie存在则返回值；若cookie不存在则返回null $.cookie(‘cookieName’); 删除cookie把cookie的值设为null即可 $.cookie(‘the_cookie’, null);]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设置mysql账户允许外网访问]]></title>
    <url>%2F2018%2F06%2F28%2F%E8%AE%BE%E7%BD%AEmysql%E8%B4%A6%E6%88%B7%E5%85%81%E8%AE%B8%E5%A4%96%E7%BD%91%E8%AE%BF%E9%97%AE%2F</url>
    <content type="text"><![CDATA[登录数据库 mysql -uroot -pPASSWORD 进入数据库名为mysql的数据库 mysql&gt;user mysql; 更新user表的用户名user为root的host为%（即为设置为没有访问限制，没修改之前是localhost） mysql&gt;update user set host = ‘%’ where user = ‘root’; 使修改生效 mysql&gt;flush privileges;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[单例模式有五种写法：懒汉、饿汉、双重检验锁、静态内部类、枚举。]]></title>
    <url>%2F2018%2F06%2F27%2F%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E6%9C%89%E4%BA%94%E7%A7%8D%E5%86%99%E6%B3%95%EF%BC%9A%E6%87%92%E6%B1%89%E3%80%81%E9%A5%BF%E6%B1%89%E3%80%81%E5%8F%8C%E9%87%8D%E6%A3%80%E9%AA%8C%E9%94%81%E3%80%81%E9%9D%99%E6%80%81%E5%86%85%E9%83%A8%E7%B1%BB%E3%80%81%E6%9E%9A%E4%B8%BE%E3%80%82%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/nsw911439370/article/details/50456231]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[会话保持是什么？]]></title>
    <url>%2F2018%2F06%2F25%2F%E4%BC%9A%E8%AF%9D%E4%BF%9D%E6%8C%81%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[以下内容基于CLB负载均衡器、F5 BIGIP设备的特性进行讨论 会话保持概述 会话保持是负载均衡最常见的问题之一，也是一个相对比较复杂的问题。会话保持有时候又叫做粘滞会话(Sticky Sessions) 在介绍会话保持技术之前，我们必须先花点时间弄清楚一些概念：什么是连接（Connection）、什么是会话（Session），以及这二者之间的区别。需要特别强调的是，如果我们仅仅是谈论负载均衡，会话和连接往往具有相同的含义 从简单的角度来看，如果用户需要登录，那么就可以简单的理解为会话；如果不需要登录，那么就是连接。 实际上，会话保持机制与负载均衡的基本功能是完全矛盾的。负载均衡希望将来自客户端的连接、请求均衡的转发至后端的多台服务器，以避免单台服务器负载过高；而会话保持机制却要求将某些请求转发至同一台服务器进行处理。因此，在实际的部署环境中，我们要根据应用环境的特点，选择适当的会话保持机制 原始负载均衡的基本原理 对于同一个连接中的数据包，负载均衡会将其进行NAT转换后，转发至后端固定的服务器进行处理，这是负载均衡最基本、最原始的功能。负载均衡系统内部会专门有一张表来记录这些连接的状况，包括：[源IP：端口]、[目的IP：端口]、[服务器IP：端口]、空闲超时时间（Idle Timeout）等等 由于负载均衡内部记录连接状态的这张表需要消耗系统的内存资源，因此这张表不可能无限大，所有传统厂商都会有一定的限制。这张表的大小一般称之为最大并发连接数，也就是系统同时能够容纳的连接数量。考虑到建立这些连接的客户端或服务器会发生一些异常状况，导致这些连接不能被正常终结掉，因此，负载均衡的当前连接状态表项中，设计了一个空闲超时时间的参数。这个参数定义为，当该连接在一定时间内无流量通过时，负载均衡会自动删除该连接条目，释放系统资源 有了会话之后，使用原始的负载均衡就会有问题，常见的异常场景包括： 因此，会话保持机制的意义就在于，确保将来自相同客户端的请求，转发至后端相同的服务器进行处理。换句话说，就是将客户端与服务器之间建立的多个连接，都发送到相同的服务器进行处理。如果在客户端和服务器之间部署了负载均衡设备，很有可能，这多个连接会被转发至不同的服务器进行处理。如果服务器之间没有会话信息的同步机制，会导致其他服务器无法识别用户身份，造成用户在和应用系统发生交互时出现异常。 在大多数电子商务的应用系统或者需要进行用户身份认证的在线系统中,一个客户与服务器经常经过好几次的交互过程才能完成一笔交易或者是一个请求的完成由于这几次交互过程是密切相关的,服务器在进行这些交互过程的某一个交互步骤时,往往需要了解上一次交互过程的处理结果,或者上几步的交互过程结果,服务器进行下一步操作时需要这就要求所有这些相关的交互过程都由一台服务器完成,而不能被负载均衡器分散到不同的服务器上 而这一系列的相关的交互过程可能是由客户到服务器的一个连接的多次会话完成,也可能是在客户与服务器之间的多个不同连接里的多次会话完成不同连接的多次会话,最典型的例子就是基于http的访问,一个客户完成一笔交易可能需多次点击,而一个新的点击产生的请求,可能会重用上一次点击建立起来的连接,也可能是一个新建的连接 会话保持就是指在负载均衡器上有这么一种机制,可以识别做客户与服务器之间交互过程的关连性,在作负载均衡的同时,还保证一系列相关连的访问请求会保持分配到一台服务器上 客户端输入了正确的用户名和口令，但却反复跳到登录页面； 用户输入了正确的验证码，但是总提示验证码错误； 客户端放入购物篮的物品丢失 … ![](http://seo-1255598498.file.myqcloud.com/full/4bead3c7a9185d76531c429486dbe2f2058fd0f5.jpg) 一、简单会话保持-基于ip 简单会话保持也被称为基于源地址的会话保持，也叫基于IP的会话保持,是指负载均衡器在作负载均衡时是根据访问请求的源地址作为判断关连会话的依据对来自同一IP地址的所有访问请求在作负载均时都会被保持到一台服务器上去负载均衡器可以为“同一IP地址”通过网络掩码进行区分,比如可以通过对IP地址 192.168.1.1进行255.255.255.0的网络掩码,这样只要是来自于192.168.1.0/24这个网段的流量BIGIP都可以认为他们是来自于同一个用户,这样就将把来自于192.168.1.0/24网段的流量会话保持到特定的一台服务器上 简单会话保持里另外一个很重要的参数就是连接超时值,负载均衡器会为每一个进行会话保持的会话设定一个时间值,当一个会话上一次完成到这个会话下次再来之前的间隔如果小于这个超时值,负载均衡器将会将新的连接进行会话保持,但如果这个间隔大于该超时值,负载均衡器将会将新来的连接认为是新的会话然后进行负载平衡简单会话保持实现起来简单,只需要根据数据包三四层的信息就可以实现,效率也比较高 简单会话保持存在的问题就在于当多个客户是通过代理或地址转换的方式来访问服务器时,由于都分配到同一台服务器上,会导致服务器之间的负载严重失衡另外一种情况上客户机数量很少,但每个客户机都会产生多个并发访问,对这些必发访问也要求通过负载均衡器分配到多个服器上,这时基于客户端源地址的会话保持方法也会导致负载均衡失效这个时候，就必须要考虑使用其他的会话保持方式，比如session等 F5 BigIP支持多种的会话保持方法,其中包括:简单会话保持(源地址会话保持)HTTP Header的会话保持,基于SSL Session ID的会话保持,I-Rules会话保持以及基于 HTTP Cookie的会话保持,此外还有基于SIP ID以及Cache设备的会话保持等,但常用的是简单会话保持,HTTP Header的会话保持以及 HTTP Cookie会话保持以及基于I-Rules的会话保持 NginX对简单会话保持的支持 ip_hash 每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session 的问题。 例如： upstream **nd { ip_hash; server 192.168.0.14:88; server 192.168.0.15:80; } 二、存session的会话保持方式 1、使用数据库存放session，Session信息存储到数据库表，这样实现不同应用服务器间Session信息的共享 适合数据库访问量不大的网站。优点：实现简单； 缺点：由于数据库服务器相对于应用服务器更难扩展且资源更为宝贵，在高并发的Web应用中，最大的性能瓶颈通常在于数据库服务器。因此如果将 Session存储到数据库表，频繁的数据库操作会影响业务。 2、使用文件系统存放session 通过文件系统（比如NFS方式）来实现各台服务器间的Session共享，各台服务器只需要mount共享服务器的存储Session的磁盘即可，实现较为简单。但NFS 对高并发读写的性能并不高，在硬盘I/O性能和网络带宽上存在较大瓶颈，尤其是对于Session这样的小文件的频繁读写操作，适合并发量不大的网站 3、基于Memcached 存储Session 利用Memcached来保存Session数据，直接通过内存的方式，效率自然能够提高不少。 在读写速度上会比 files 时快很多，而且在多个服务器需要共用 session 时会比较方便，将这些服务器都配置成使用同一组 memcached 服务器就可以，减少了额外的工作量。缺点： session 数据都保存在 memory 中，一旦宕机，数据将会丢失。但对 session 数据来说并不是严重的问题。如果网站访问量太大，session太多的时候，memcached会将不常用的部分删除，但是如果用户隔离了一段时间之后继续使用，已经全部乱套了。 三、基于Cookie的会话保持 在Cookie插入模式下，CLB将负责插入cookie，后端服务器无需作出任何修改.当客户进行第一次请求时，客户HTTP请求（不带cookie）进入CLB， CLB根据负载平衡算法策略选择后端一台服务器，并将请求发送至该服务器，后端服务器进行HTTP回复（不带cookie）被发回CLB，然后CLB插入cookie，将HTTP回复返回到客户端。 当客户请求再次发生时，客户HTTP请求（带有上次CLB插入的cookie）进入CLB，然后CLB读出cookie里的会话保持数值，将HTTP请求（带有与上面同样的cookie）发到指定的服务器，然后后端服务器进行请求回复，由于服务器并不写入cookie，HTTP回复将不带有cookie，恢复流量再次经过进入CLB时，CLB再次写入更新后的会话保持cookie。]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[调用微软的在线预览功能实现word、xls、ppt文件在线预览功能]]></title>
    <url>%2F2018%2F06%2F20%2F%E8%B0%83%E7%94%A8%E5%BE%AE%E8%BD%AF%E7%9A%84%E5%9C%A8%E7%BA%BF%E9%A2%84%E8%A7%88%E5%8A%9F%E8%83%BD%E5%AE%9E%E7%8E%B0word%E3%80%81xls%E3%80%81ppt%E6%96%87%E4%BB%B6%E5%9C%A8%E7%BA%BF%E9%A2%84%E8%A7%88%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[详情查看：[微软官方文档](https://www.microsoft.com/en-us/microsoft-365/blog/2013/04/10/office-web-viewer-view-office-documents-in-a-browser/?eu=true)]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用iText生成PDF文件]]></title>
    <url>%2F2018%2F06%2F11%2F%E4%BD%BF%E7%94%A8iText%E7%94%9F%E6%88%90PDF%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/h–d/p/6150320.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Cron表达式]]></title>
    <url>%2F2018%2F06%2F05%2FCron%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[转自：http://www.pppet.net Cron在线生成工具网址CronTriggerCronTriggers往往比SimpleTrigger更有用，如果您需要基于日历的概念，而非SimpleTrigger完全指定的时间间隔，复发的发射工作的时间表。 CronTrigger，你可以指定触发的时间表如“每星期五中午”，或“每个工作日9:30时”，甚至“每5分钟一班9:00和10:00逢星期一上午，星期三星期五“。 即便如此，SimpleTrigger一样，CronTrigger拥有的startTime指定的时间表时生效，指定的时间表时，应停止（可选）结束时间。 Cron表达式cron的表达式被用来配置CronTrigger实例。 cron的表达式是字符串，实际上是由七子表达式，描述个别细节的时间表。这些子表达式是分开的空白，代表： 1. Seconds 2. Minutes 3. Hours 4. Day-of-Month 5. Month 6. Day-of-Week 7. Year (可选字段) 例 “0 0 12 ? * WED” 在每星期三下午12:00 执行, 个别子表达式可以包含范围, 例如，在前面的例子里(“WED”)可以替换成 “MON-FRI”, “MON, WED, FRI”甚至”MON-WED,SAT”. “*” 代表整个时间段. 每一个字段都有一套可以指定有效值，如 Seconds (秒) ：可以用数字0－59 表示， Minutes(分) ：可以用数字0－59 表示， Hours(时) ：可以用数字0-23表示, Day-of-Month(天) ：可以用数字1-31 中的任一一个值，但要注意一些特别的月份 Month(月) ：可以用0-11 或用字符串 “JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV and DEC” 表示 Day-of-Week(每周)：可以用数字1-7表示（1 ＝ 星期日）或用字符口串“SUN, MON, TUE, WED, THU, FRI and SAT”表示 “/”：为特别单位，表示为“每”如“0/15”表示每隔15分钟执行一次,“0”表示为从“0”分开始, “3/20”表示表示每隔20分钟执行一次，“3”表示从第3分钟开始执行 “?”：表示每月的某一天，或第周的某一天 “L”：用于每月，或每周，表示为每月的最后一天，或每个月的最后星期几如“6L”表示“每月的最后一个星期五” “W”：表示为最近工作日，如“15W”放在每月（day-of-month）字段上表示为“到本月15日最近的工作日” ““#”：是用来指定“的”每月第n个工作日,例 在每周（day-of-week）这个字段中内容为”6#3” or “FRI#3” 则表示“每月第三个星期五” 常用Cron表达式0 15 10 * * ? * 每天10点15分触发 0 15 10 * * ? 2017 2017年每天10点15分触发 0 * 14 * * ? 每天下午的 2点到2点59分每分触发 0 0/5 14 * * ? 每天下午的 2点到2点59分(整点开始，每隔5分触发) 0 0/5 14,18 * * ? 每天下午的 2点到2点59分、18点到18点59分(整点开始，每隔5分触发) 0 0-5 14 * * ? 每天下午的 2点到2点05分每分触发 0 15 10 ? * 6L 每月最后一周的星期五的10点15分触发 0 15 10 ? * 6#3 每月的第三周的星期五开始触发]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Git常用命令速查表]]></title>
    <url>%2F2018%2F05%2F31%2FGit%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>Git</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tokenInput给页面元素动态传值用onResult示例]]></title>
    <url>%2F2018%2F05%2F30%2FtokenInput%E7%BB%99%E9%A1%B5%E9%9D%A2%E5%85%83%E7%B4%A0%E5%8A%A8%E6%80%81%E4%BC%A0%E5%80%BC%E7%94%A8onResult%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223var teacherIds = \[\];$("#input-teacher").tokenInput("/server/member/course/list.json?courseId=" \+ courseId, &#123; theme: "facebook", hintText: "请输入主讲教师名称", noResultsText: "没有主讲教师", searchingText: "搜索中...", propertyToSearch: "realName", preventDuplicates: true, onResult:function (results)&#123; results.forEach(function (item,index,array)&#123; item.id = item.memberId; &#125;); return results; &#125;, onAdd: function (item) &#123; teacherIds.push(item.memberId); $('#input-teacher-hidden').val(teacherIds.join(",")); &#125;, onDelete: function (item) &#123; teacherIds.splice(teacherIds.indexOf(item.memberId), 1); $('#input-teacher-hidden').val(teacherIds.join(",")); &#125; &#125;);]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql查询一个表的主键被哪些表所调用]]></title>
    <url>%2F2018%2F05%2F30%2Fmysql%E6%9F%A5%E8%AF%A2%E4%B8%80%E4%B8%AA%E8%A1%A8%E7%9A%84%E4%B8%BB%E9%94%AE%E8%A2%AB%E5%93%AA%E4%BA%9B%E8%A1%A8%E6%89%80%E8%B0%83%E7%94%A8%2F</url>
    <content type="text"><![CDATA[use information_schema; show tables; select * from KEY_COLUMN_USAGE where COLUMN_NAME=’xx_id’;]]></content>
      <categories>
        <category>mysql</category>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[（转载）Intellij IDEA神器居然还有这些小技巧]]></title>
    <url>%2F2018%2F05%2F24%2F%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89Intellij%20IDEA%E7%A5%9E%E5%99%A8%E5%B1%85%E7%84%B6%E8%BF%98%E6%9C%89%E8%BF%99%E4%BA%9B%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[作者：Sam哥哥 链接：https://blog.csdn.net/linsongbin1/article/details/80211919 概述 Intellij IDEA真是越用越觉得它强大，它总是在我们写代码的时候，不时给我们来个小惊喜。出于对Intellij IDEA的喜爱，我决定写一个与其相关的专栏或者系列，把一些好用的Intellij IDEA技巧分享给大家。本文是这个系列的第一篇，主要介绍一些你可能不知道的但是又实用的小技巧。 我最爱的【演出模式】 我们可以使用【Presentation Mode】，将IDEA弄到最大，可以让你只关注一个类里面的代码，进行毫无干扰的coding。 可以使用Alt+V快捷键，弹出View视图，然后选择Enter Presentation Mode。效果如下： 这个模式的好处就是，可以让你更加专注，因为你只能看到特定某个类的代码。可能读者会问，进入这个模式后，我想看其他类的代码怎么办？这个时候，就要考验你快捷键的熟练程度了。你可以使用CTRL+E弹出最近使用的文件。又或者使用CTRL+N和CTRL+SHIFT+N定位文件。 如何退出这个模式呢？很简单，使用ALT+V弹出view视图，然后选择Exit Presentation Mode即可。但是我强烈建议你不要这么做，因为你是可以在Enter Presentation Mode模式下在IDEA里面做任何事情的。当然前提是，你对IDEA足够熟练。 神奇的Inject language 如果你使用IDEA在编写JSON字符串的时候，然后要一个一个\去转义双引号的话，就实在太不应该了，又烦又容易出错。在IDEA可以使用Inject language帮我们自动转义双引号。 先将焦点定位到双引号里面，使用alt+enter快捷键弹出inject language视图，并选中 Inject language or reference。 选择后,切记，要直接按下enter回车键，才能弹出inject language列表。在列表中选择json组件。 选择完后。鼠标焦点自动会定位在双引号里面，这个时候你再次使用alt+enter就可以看到 选中Edit JSON Fragment并回车，就可以看到编辑JSON文件的视图了。 可以看到IDEA确实帮我们自动转义双引号了。如果要退出编辑JSON信息的视图，只需要使用ctrl+F4快捷键即可。 Inject language可以支持的语言和操作多到你难以想象，读者可以自行研究。 使用快捷键移动分割线 假设有下面的场景，某个类的名字在project视图里被挡住了某一部分。 你想完整的看到类的名字，该怎么做。一般都是使用鼠标来移动分割线，但是这样子效率太低了。可以使用alt+1把鼠标焦点定位到project视图里，然后直接使用ctrl+shift+左右箭头来移动分割线。 ctrl+shift+enter不只是用来行尾加分号的 ctrl+shift+enter其实是表示为您收尾的意思，不只是用来给代码加分号的。比如说： 这段代码，我们还需要为if语句加上大括号才能编译通过，这个时候你直接输入ctrl+shift+enter，IDEA会自动帮你收尾，加上大括号的。 不要动不动就使用IDEA的重构功能 IDEA的重构功能非常强大，但是也有时候，在单个类里面，如果只是想批量修改某个文本，大可不必使用到重构的功能。比如说： 上面的代码中，有5个地方用到了rabbitTemplate文本，如何批量修改呢？ 首先是使用ctrl+w选中rabbitTemplate这个文本,然后依次使用5次alt+j快捷键，逐个选中，这样五个文本就都被选中并且高亮起来了，这个时候就可以直接批量修改了。 去掉导航栏 去掉导航栏，因为平时用的不多。 可以把红色的导航栏去掉，让IDEA显得更加干净整洁一些。使用alt+v，然后去掉Navigation bar即可。去掉这个导航栏后，如果你偶尔还是要用的，直接用alt+home就可以临时把导航栏显示出来。 如果想让这个临时的导航栏消失的话，直接使用esc快捷键即可。 把鼠标定位到project视图里 当工程里的包和类非常多的时候，有时候我们想知道当前类在project视图里是处在哪个位置。 上面图中的DemoIDEA里，你如何知道它是在spring-cloud-config工程里的哪个位置呢？ 可以先使用alt+F1，弹出Select in视图，然后选择Project View中的Project，回车，就可以立刻定位到类的位置了。 那如何从project跳回代码里呢？可以直接使用esc退出project视图，或者直接使用F4,跳到代码里。 强大的symbol 如果你依稀记得某个方法名字几个字母，想在IDEA里面找出来，可以怎么做呢？ 直接使用ctrl+shift+alt+n，使用symbol来查找即可。 比如说： 你想找到checkUser方法。直接输入user即可。 如果你记得某个业务类里面有某个方法，那也可以使用首字母找到类,然后加个.，再输入方法名字也是可以的。 如何找目录 使用ctrl+shift+n后，使用/，然后输入目录名字即可. 自动生成not null判断语句 自动生成not null这种if判断，在IDEA里有很多种办法，其中一种办法你可能没想到。 当我们使用rabbitTemplate. 后，直接输入notnull并回车，IDEA就好自动生成if判断了。 按照模板找内容 这个也是我非常喜欢的一个功能，可以根据模板来找到与模板匹配的代码块。比如说： 想在整个工程里面找到所有的try catch语句,但是catch语句里面没有做异常处理的。 catch语句里没有处理异常，是极其危险的。我们可以IDEA里面方便找到所有这样的代码。 首先使用ctrl+shift+A快捷键弹出action框，然后输入Search Struct 选择Search Structurally后，回车，跳转到模板视图。 点击Existing Templates按钮，选择try模板。为了能找出catch里面没有处理异常的代码块，我们需要配置一下CatchStatement的Maximum count的值，将其设置为1。 点击Edit Variables按钮，在界面修改Maximum count的值。 最后点击find按钮，就可以找出catch里面没有处理异常的代码了。]]></content>
      <categories>
        <category>IDE</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ IDEA，代码行宽度超出限制时设置自动换行]]></title>
    <url>%2F2018%2F05%2F22%2FIntelliJ%20IDEA%EF%BC%8C%E4%BB%A3%E7%A0%81%E8%A1%8C%E5%AE%BD%E5%BA%A6%E8%B6%85%E5%87%BA%E9%99%90%E5%88%B6%E6%97%B6%E8%AE%BE%E7%BD%AE%E8%87%AA%E5%8A%A8%E6%8D%A2%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[当我们使用IDE写代码时，为了保证代码的可阅读性和优雅性，通常会借助IDE的代码风格设置功能，令IDE智能完成的代码部分或者格式化输入的代码，可以按照预期的格式输出。其中有一项设置就是限制一行代码的宽度，以IDEA举例，默认限制为120，如下图所示，图中所标注的垂直线即使代码行宽度的限制提示。 那么如何在IDEA中修改这个限制数值呢？在File-&gt;settings-&gt;Code Style-&gt;General中，修改“Right margin (columns)”的值即可改变代码行宽度的限制。 有人会问，如果输入的代码超出宽度界线时，如何让IDE自动将代码换行？有两种方式！第一种，在上述的“Right margin (columns)”的下方，有“Wrap when typing reaches right margin”选项，选中它，是什么效果呢？如下图所示，随着输入的字符的增加，当代码宽度到达界线时，IDEA会自动将代码换行。 第一种方式是在输入代码时触发，还有第二种方式，在File-&gt;settings-&gt;Code Style-&gt;Java中，选中“Wrapping and Braces”选项卡，在“Keep when reformatting”中有一个“Ensure rigth margin is not exceeded”，选中它，是什么效果呢？从配置项的字面意思很容易理解，在格式化Java代码时，确保代码没有超过宽度界线。 即输入的代码超出界线后， 不要紧，按下“Ctrl+Alt+L”格式化代码。 IDE能帮我们在超出代码宽度界线时自动换行自然是好，可上述两种方式依旧有不妥之处，就是IDE只会帮我们主动换行一次！当IDE自动换行后，代码长度依旧超出界线时，就需要手动处理了。而且不难看出IDE所做的换行处理简单粗暴，未必是我们想要的结果（有时我们期望可以字符串换行拼接可能更优雅些）。 所以我个人建议，IDE所给出的宽度界线是好的，但真正控制、优化代码宽度的格式需要我们编码时养成良好的习惯，避免单行代码过长，避免不了时自己手动找到合适的截点，进行换行处理，且更符合各自实际的编码格式需求。]]></content>
      <categories>
        <category>IDE</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在Centos7安装Git]]></title>
    <url>%2F2018%2F05%2F16%2F%E5%9C%A8Centos7%E5%AE%89%E8%A3%85Git%2F</url>
    <content type="text"><![CDATA[本机上可能自带git，先不管卸载之 yum remove git先安装所需要的依赖工具 yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel下载自己需要的版本：访问链接。此处这里下载的版本是2.22.0 wget https://github.com/git/git/archive/v2.22.0-rc0.tar.gz解压并进入解压目录 tar -zxf v2.22.0-rc0.tar.gz cd git-2.22.0-rc0/编译安装 make prefix=/usr/local all make prefix=/usr/local install如果是按照上面步骤编译安装在 /usr/local 目录了，教程到此结束，执行命令验证安装 git --version如果安装在其他目录需要添加环境变量，添加环境变量 vim /etc/profile添加下面这行（比如git安装在/home/git目录下） export PATH=$PATH:/home/git/bin刷新环境变量 source /etc/profile]]></content>
      <categories>
        <category>Git</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hql查找最大最小]]></title>
    <url>%2F2018%2F05%2F11%2Fhql%E6%9F%A5%E6%89%BE%E6%9C%80%E5%A4%A7%E6%9C%80%E5%B0%8F%2F</url>
    <content type="text"><![CDATA[@Query(“select COALESCE(min(h.beginDate),0) from ExamAnswersheetResult h where h.deleted = false and h.beginDate != 0 and h.exam.id = :examId and h.student.id = :studentId “)Long findMinBeginTime(@Param(“examId”) String examId, @Param(“studentId”) String studentId); @Query(“select COALESCE(min(ear.beginDate),0) from ExamAnswersheetResult ear where ear.deleted= false and ear.beginDate != 0 and ear.exam.id=:examId and ear.student.id=:studentId “)Long getMinBeginTime(@Param(“examId”) String examId, @Param(“studentId”) String studentId);]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[@Query中直接写sql语句]]></title>
    <url>%2F2018%2F05%2F10%2Fmark%2F</url>
    <content type="text"><![CDATA[@Query(nativeQuery=true,value = “”) value里写正常sql语句就可以 @Formula 了解一下 https://blog.csdn.net/kakolukiya/article/details/49175351]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[对比两个数组，删除其中一个数据的交集数据]]></title>
    <url>%2F2018%2F05%2F08%2F%E5%AF%B9%E6%AF%94%E4%B8%A4%E4%B8%AA%E6%95%B0%E7%BB%84%EF%BC%8C%E5%88%A0%E9%99%A4%E5%85%B6%E4%B8%AD%E4%B8%80%E4%B8%AA%E6%95%B0%E6%8D%AE%E7%9A%84%E4%BA%A4%E9%9B%86%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[测试代码：12345678910public static void main(String\[\] args) &#123; String\[\] s1 = &#123;"1","2","3","4"&#125;; String\[\] s2 = &#123;"3","4","5","6"&#125;; TreeSet ts1 = new TreeSet(Arrays.asList(s1)); ts1.removeAll(Arrays.asList(s2)); System.out.println(ts1);&#125; 输出：1[1, 2]]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[javascript 中this 的用法]]></title>
    <url>%2F2018%2F04%2F27%2Fjavascript%20%E4%B8%ADthis%20%E7%9A%84%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[this 是 Javascript语言的一个关键字. 它代表函数运行时,自动生成的一个内部对象,只能在函数内部使用.比如, function test(){ this.x = 1; }随着函数使用场合的不同,this 的值会发生变化.但是有一个总的原则就是,this 指的说是调用函数的那个对象. 1.纯粹函数调用 //这是函数的最通常的用法,术语全局性调用,因此this 就代表全局对象Global. function test(){ this.x =1; alert(this.x);//1 } var x = 1; function test(){ alert(this.x);//1 } 2.作为对象的方法调用 //函数可以作为某个对象的方法调用,这时候 this 指的就是这个上级对象. function test(){ alert(this.x); } var obj = { x: 1 }; obj.test = test; obj.test();//1 3.作为构造函数调用 //所谓构造函数,就是通过这个函数生成的一个新对象.这时,this就指的是这个新对象. var x = 2; function test(){ this.x = 1; } var o = new test(); alert(o.x);//1 4.apply 调用 apply() 是函数对象的一个方法, 它的作用是改变函数的调用对象,它的第一个参数就表示改变后的调用这个函数的对象,因此 this 指的就是 这个第一个参数. var x = 1; function test(){ alert(this.x); } var obj = { x :2 }; obj.test = test; obj.test.apply();//1 obj.test.apply(obj);//2 //apply 的第一个参数为空时候, 默认调用全局对象.call( ), apply( ), bind( ) call( ),方法给定第一个参数,绑定this 的指向对象,之后为函数调用所需参数,逗号隔开; apply( ),方法与call( )方法一致,第一个参数绑定this指向,第二个参数为一个数组,指定调用函数所需要参数. bind( )方法,可以绑定调用函数的this指向,与参数.后可更改(通过bind 方法改变). call( )与apply( ) 第一个参数 若为null 则this,指向全局对象.]]></content>
      <categories>
        <category>JavaScript</category>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[global]]></title>
    <url>%2F2018%2F04%2F27%2Fglobal%2F</url>
    <content type="text"><![CDATA[Node.js 全局对象Javascript中有一个特殊的对象，称为全局对象（Global Object）,它及其所有属性都可以在程序的任何地方访问。 在浏览器 Javascript中，通常window 是全局对象，而 Node.js 中的全局对象是 global，所有的全局变量除了global本身以外的都是 global 的属性。 在 Node.js 中可以直接访问到global 的属性，而不是要在应用中包含它。 全局对象与全局变量global 最根本的作用就是作为全局变量的宿主，按照ECMAScript的定义，满足以下条件 的变量是全局变量：* 在最外层定义的变量* 全局对象的属性* 隐式定义的变量（未定义直接赋值的变量） 当定义一个全局变量时，这个变量同时会成为全局对象的属性，反之亦然。需要注意的是，在Node.js中不可能在最外层定义变量，因为所有用户代码都是属于当前模块的，而模块本身并不是最外层上下文。可能在最外层定义变量，因为所有用户代码都是属于当前模块的，而模块本身不是最外层上下文。 注意： 永远使用var定义变量以避免引入全局变量，因为全局变量会污染命名空间，提高代码耦合风险。 __filename__filename 表示当前正在执行的脚本文件名。它将输出文件所在位置的绝对路径，且和命令行参数所指定的文件名不一定相同。如果在模块中，返回的是模块文件的路径。 __filename.jsconsole.log(__filename); // /Users/Day/learning/runoob.node/global/__filename.js__dirname__dirname.jsconsole.log(__dirname); // /Users/Day/learning/runoob.node/globalsetTimeout(cb,ms)clearTimeout(t)setInterval(cb,ms)clearInterval(t)console序号 方法 描述 1 console.log() 向标准输出流打印字符并以换行符结束。该方法接受若干个参数，如果只有一个参数，则输出这个参数的字符串形式。如果有多个参数，则以类似C语言printf()命令的格式输出。 2 console.info() 该命令的作用是返回信息性消息，会显示一个蓝色的惊叹号。 3 console.error() 该命令的作用是返回错误性信息，会显示一个红色的叉子。 4 console.warn() 该命令的作用是返回警告性信息，会显示一个黄色的惊叹号。 5 console.dir() 对一个对象进行检查（inspect）,并以易于阅读和打印的格式显示。 6 console.time() 输出时间，表示计时开始。 7 console.timeEnd() 结束时间，表示计时结束。 8 console.trace() 当前执行的代码在堆栈中调用的路径，这个测试函数运行很有帮助，只要给想测试的函数里面加入console.trace 就行。 9 console.assert() 用于判断某个表达式或变量是否为真，接受两个参数，第一个参数是表达式，第二个参数是字符串。只有当第一个参数为false，才会输出第二个参数，否则不会有任何结果。 console.log(&apos;print&apos;); console.info(&apos;info&apos;); console.error(&apos;error&apos;); console.warn(&apos;warn&apos;); console.dir({ a: 1, b: { c: 3, d: { e: 5 } } }); console.table({ a: 1, b: { c: 3, d: { e: 5 } } }); console.time(&apos;day&apos;); console.timeEnd(&apos;day&apos;); // console.clear(); console.assert(0===1,&apos;assert&apos;); function a(){ var b = 1; console.trace(); } a();processprocess 是一个全局变量，即global 对象的属性。它用于描述当前Node.js 进程状态的对象，提供了一个与操作系统的简单接口。通常在你写本地命令行程序的时候，少不了要它打交道。 序号 事件 描述 1 exit 当程序准备退出时触发。 2 beforeExit 当node清空事件循环，并且没有其他安排时触发这个事件。通常来说，当没有进程安排时node退出，但是’beforeExit’的监听器可以异步调用，这样node就可以继续执行。 3 uncaughtException 当一个异常冒泡回到事件循环，触发这个事件。如果给异常添加了监视器，默认的操作（打印堆栈跟踪信息并退出）就不会发生。 4 signal 当进程接收信号时就触发，信号列表详见标准的POSIX信号名，如SIGINT、SIGUSR1 退出状态码状态码 名称 描述 1 Uncaught Fatal Exception 有未捕获的异常，并且没有被域或者uncaughtExption处理函数处理 2 Unused 保留 3 Internal Javascript Parse Error Javascript 的源码启动Node进程时引起解析错误。非常罕见，仅会在开发Node 4 Internal Javascript Evalution Failure Javascript 的源码启动Node进程时引起解析错误。非常罕见，仅会在开发Node 5 Fatal Error V8 里致命的不可恢复的错误。通常会打印到stderr,内容为：FATAL ERROR 6 Non-function Internal Exception Handler 未捕获异常，内部异常处理函数不知为何设置为on-function，并且不能被调用。 7 Internal Exception Handler Run-Time Failure 未捕获的异常， 并且异常处理函数处理时自己抛出了异常。例如，如果 process.on(‘uncaughtException’) 或 domain.on(‘error’) 抛出了异常。 9 Invalid Argument 可能是给了未知的参数，或者给的参数没有值。 12 Invalid Debug Argument 设置了参数–debug 和/或 –debug-brk，但是选择了错误端口。 128 Signal Exits 如果 Node 接收到致命信号，比如SIGKILL 或 SIGHUP，那么退出代码就是128 加信号代码。这是标准的 Unix 做法，退出信号代码放在高位。 process属性序号 属性 描述 1 stdout 标准输出流 2 stderr 标准错误流 3 stdin 标准输入流 4 argv argv 属性返回一个数组，由命令行执行脚本时的各个参数组成。它的第一个成员总是node，第二个成员是脚本文件名，其余成员是脚本文件的参数。 5 execPath 返回执行当前脚本的 Node 二进制文件的绝对路径。 6 execArgv 返回一个数组，成员是命令行下执行脚本时，在Node可执行文件与脚本文件之间的命令行参数。 7 env 返回一个对象，成员为当前 shell 的环境变量 8 exitCode 进程退出时的代码，如果进程优通过 process.exit() 退出，不需要指定退出码。 9 version Node 的版本 10 versions 一个属性，包含了 node 的版本和依赖。 11 config 一个包含用来编译当前 node 执行文件的 javascript 配置选项的对象。它与运行 ./configure 脚本生成的 “config.gypi” 文件相同。 12 pid 当前进程的进程号。 13 title 进程名，默认值为”node”，可以自定义该值。 14 arch 当前 CPU 的架构：’arm’、’ia32’ 或者 ‘x64’。 15 platform 运行程序所在的平台系统 ‘darwin’, ‘freebsd’, ‘linux’, ‘sunos’ 或 ‘win32’ 16 mainModule require.main 的备选方法。不同点，如果主模块在运行时改变，require.main可能会继续返回老的模块。可以认为，这两者引用了同一个模块。 方法参考手册序号 方法 描述 1 abort() 这将导致 node 触发 abort 事件。会让 node 退出并生成一个核心文件。 2 chdir(directory) 改变当前工作进程的目录，如果操作失败抛出异常。 3 cwd() 返回当前进程的工作目录 4 exit([code]) 使用指定的 code 结束进程。如果忽略，将会使用 code 0。 5 kill(pid[, signal]) 发送信号给进程. pid 是进程id，并且 signal 是发送的信号的字符串描述。信号名是字符串，比如 ‘SIGINT’ 或 ‘SIGHUP’。如果忽略，信号会是 ‘SIGTERM’。 6 memoryUsage() memoryUsage() 7 nextTick(callback) nextTick(callback) 8 umask([mask]) 设置或读取进程文件的掩码。子进程从父进程继承掩码。如果mask 参数有效，返回旧的掩码。否则，返回当前掩码。 9 uptime() 返回 Node 已经运行的秒数。 10 Hrtime() 返回当前进程的高分辨时间，形式为 [seconds, nanoseconds]数组。它是相对于过去的任意事件。该值与日期无关，因此不受时钟漂移的影响。主要用途是可以通过精确的时间间隔，来衡量程序的性能。]]></content>
      <categories>
        <category>Node.js</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sql按照查询排名加上排名的名次字段]]></title>
    <url>%2F2018%2F04%2F25%2Fsql%E6%8C%89%E7%85%A7%E6%9F%A5%E8%AF%A2%E6%8E%92%E5%90%8D%E5%8A%A0%E4%B8%8A%E6%8E%92%E5%90%8D%E7%9A%84%E5%90%8D%E6%AC%A1%E5%AD%97%E6%AE%B5%2F</url>
    <content type="text"><![CDATA[格式 12345SELECT (@rowNO := @rowNo+1) AS rowno FROM (SELECT * FROM （自己的表名）) a,(SELECT @rowNO :=0) b 例子:根据分数排名并且加上排名名次字段rowno 12345SELECT (@rank := @rank+1) AS rank,a.name,a.score FROM (SELECT s.name,s.score FROM student s order by s.score) a,(SELECT @rank :=0) b]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Git主干合并分支、分支合并主干]]></title>
    <url>%2F2018%2F04%2F22%2FGit%E4%B8%BB%E5%B9%B2%E5%90%88%E5%B9%B6%E5%88%86%E6%94%AF%E3%80%81%E5%88%86%E6%94%AF%E5%90%88%E5%B9%B6%E4%B8%BB%E5%B9%B2%2F</url>
    <content type="text"><![CDATA[branch12138为本篇的分支名 (branch12138) 表示git命令在分支“branch12138”项目下执行 (master) 表示git命令在主干项目下执行 主干合并分支进入分支，更新分支代码 (branch12138) git pull；切换主干 (branch12138) git checkout master；在主干上合并分支branch12138 (master) git merge branch12138 --squash提交合并后的代码 (master) git commit -m &quot;提交的备注信息&quot;将代码推送到远程仓库 (master) git push分支合并主干进入主干，更新主干代码 (master) git pull；切换分支 (master) git checkout branch12138；在分支上合并主干 (branch12138) git merge master --squash提交合并后的代码 (branch12138) git commit -m &quot;提交的备注信息&quot;将代码推送到远程仓库 (branch12138) git push]]></content>
      <categories>
        <category>Git</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[repl]]></title>
    <url>%2F2018%2F04%2F18%2Frepl%2F</url>
    <content type="text"><![CDATA[Node.js REPL (交互式解释器)Node.js REPL (Read Eval Print Loop:交互式解释器)表示一个一个电脑的环境。类似Windows系统的终端或Unix/Linux shell，可以在终端中输入命令，并接收系统的响应。 读取 - 读取用户输入，解析输入了Javascript 数据结构并存储在内存中。 执行 - 执行输入的数据结构。 打印 - 输出结果。 循环 - 循环操作以上步骤直到用户两次按下ctrl-c 退出。 Node的交互式解释器可以很好的调试Javascript代码。 启动Node终端&gt; node简单表达式运算&gt; 1 + 4 5 &gt; 5 / 2 2.5 &gt; 3 * 6 18 &gt; 1 + ( 2 * 3 ) -4 3 &gt;使用变量可以将变量声明在内存中，并在需要的时候使用它。 变量声明需要使用var 关键字，否则会被直接打印出来。 使用var 关键字声明的变量可以使用console.log()来输出变量。 &gt; node &gt; x = 10 10 &gt; var y = 20 undefined &gt; x + y 30 &gt; console.log(&apos;Hello World&apos;) Hello World undefined多行表达式Node REPL 支持输入多行表达式，这就有点类似Javascript。 $ node &gt; var x = 0 undefined &gt; do { ... x++; ... console.log(&quot;x: &quot; + x); ... } while ( x &lt; 5 ); x: 1 x: 2 x: 3 x: 4 x: 5 undefined &gt;…三个点的符号是系统自动生成的，回车换行即可。Node 会自动检测是否为连续的表达式。 下划线变量可以使用下划线（_）获取上一个表达式的运算结果： &gt; 1 + 2 3 &gt; console.log(_) 3 undefined &gt;REPl 命令 ctrl+c 退出当前终端 ctrl+c 连续按下两次，退出Node REPL ctrl+d 退出Node REPL 向上/向下 查看历史输入命令 tab键 列出当前命令 .help 列出使用命令 .break 退出多行表达式 .clear 退出多行表达式 .exit 退出Node REPL .save filename 保存当前的 Node REPL 会话到指定文件 .load filename 载入当前Node RPEL 会话的文件内容 .help.break Sometimes you get stuck, this gets you out.clear Alias for .break.editor Enter editor mode.exit Exit the repl.help Print this help message.load Load JS from a file into the REPL session.save Save all evaluated commands in this REPL session to a file]]></content>
      <categories>
        <category>Node.js</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[npm]]></title>
    <url>%2F2018%2F04%2F18%2Fnpm%2F</url>
    <content type="text"><![CDATA[NPM 使用介绍NPM 是随同NodeJS一起安装的包管理工具，能够解决NodeJS 代码部署上的很多问题，常见的使用场景有以下几种：* 允许用户从NPM 服务器上下载别人编写的第三方包到本地使用。* 允许用户从NPM 服务器上下载并安装别人编写的命令行程序到本地使用。* 允许用户将自己编写的包或命令行程序上传到NPM 服务器上供别人使用。 查看NPM 版本&gt; npm -v旧版NPM 升级&gt; sudo npm install npm -g淘宝镜像安装cnpm install npm -g安装模块&gt; npm install express包安装在工程目录下的node_modules目录中，引用时候，无需指定路径。var express = require(&apos;express&apos;);全局安装 与 本地安装npm 的包安装分为本地安装（local）、全局安装（global）两种。 &gt; npm install express #局部安装 &gt; npm install express -g #全局安装 npm err! Error:connect ECONNREFUSED 127.0.0.1:8087 # npm config set proxy null本地安装 将安装包放在./node_modules下（运行npm命令时所在的目录），如果没有node_modules目录就会在当前执行npm命令的目录下生成node_modules 目录。 可以通过require（）来引入本地安装的包。 全局安装 将安装包放在 /usr/local 下或者node 的安装目录 可以直接在命令行里使用。 如果希望具备两者的功能，则需要在两个地方进行安装，或使用 npm link。 查看模块安装信息&gt; npm list #local &gt; npm list -g #global查看某个模块的安装信息&gt; npm list express -g #global &gt; npm list express #localpackage.json{ &quot;name&quot;: &quot;express&quot;, &quot;description&quot;: &quot;Fast, unopinionated, minimalist web framework&quot;, &quot;version&quot;: &quot;4.13.3&quot;, &quot;author&quot;: { &quot;name&quot;: &quot;TJ Holowaychuk&quot;, &quot;email&quot;: &quot;tj@vision-media.ca&quot; }, &quot;contributors&quot;: [ { &quot;name&quot;: &quot;Aaron Heckmann&quot;, &quot;email&quot;: &quot;aaron.heckmann+github@gmail.com&quot; }, { &quot;name&quot;: &quot;Ciaran Jessup&quot;, &quot;email&quot;: &quot;ciaranj@gmail.com&quot; }, { &quot;name&quot;: &quot;Douglas Christopher Wilson&quot;, &quot;email&quot;: &quot;doug@somethingdoug.com&quot; }, { &quot;name&quot;: &quot;Guillermo Rauch&quot;, &quot;email&quot;: &quot;rauchg@gmail.com&quot; }, { &quot;name&quot;: &quot;Jonathan Ong&quot;, &quot;email&quot;: &quot;me@jongleberry.com&quot; }, { &quot;name&quot;: &quot;Roman Shtylman&quot;, &quot;email&quot;: &quot;shtylman+expressjs@gmail.com&quot; }, { &quot;name&quot;: &quot;Young Jae Sim&quot;, &quot;email&quot;: &quot;hanul@hanul.me&quot; } ], &quot;license&quot;: &quot;MIT&quot;, &quot;repository&quot;: { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git+https://github.com/strongloop/express.git&quot; }, &quot;homepage&quot;: &quot;http://expressjs.com/&quot;, &quot;keywords&quot;: [ &quot;express&quot;, &quot;framework&quot;, &quot;sinatra&quot;, &quot;web&quot;, &quot;rest&quot;, &quot;restful&quot;, &quot;router&quot;, &quot;app&quot;, &quot;api&quot; ], &quot;dependencies&quot;: { &quot;accepts&quot;: &quot;~1.2.12&quot;, &quot;array-flatten&quot;: &quot;1.1.1&quot;, &quot;content-disposition&quot;: &quot;0.5.0&quot;, &quot;content-type&quot;: &quot;~1.0.1&quot;, &quot;cookie&quot;: &quot;0.1.3&quot;, &quot;cookie-signature&quot;: &quot;1.0.6&quot;, &quot;debug&quot;: &quot;~2.2.0&quot;, &quot;depd&quot;: &quot;~1.0.1&quot;, &quot;escape-html&quot;: &quot;1.0.2&quot;, &quot;etag&quot;: &quot;~1.7.0&quot;, &quot;finalhandler&quot;: &quot;0.4.0&quot;, &quot;fresh&quot;: &quot;0.3.0&quot;, &quot;merge-descriptors&quot;: &quot;1.0.0&quot;, &quot;methods&quot;: &quot;~1.1.1&quot;, &quot;on-finished&quot;: &quot;~2.3.0&quot;, &quot;parseurl&quot;: &quot;~1.3.0&quot;, &quot;path-to-regexp&quot;: &quot;0.1.7&quot;, &quot;proxy-addr&quot;: &quot;~1.0.8&quot;, &quot;qs&quot;: &quot;4.0.0&quot;, &quot;range-parser&quot;: &quot;~1.0.2&quot;, &quot;send&quot;: &quot;0.13.0&quot;, &quot;serve-static&quot;: &quot;~1.10.0&quot;, &quot;type-is&quot;: &quot;~1.6.6&quot;, &quot;utils-merge&quot;: &quot;1.0.0&quot;, &quot;vary&quot;: &quot;~1.0.1&quot; }, &quot;devDependencies&quot;: { &quot;after&quot;: &quot;0.8.1&quot;, &quot;ejs&quot;: &quot;2.3.3&quot;, &quot;istanbul&quot;: &quot;0.3.17&quot;, &quot;marked&quot;: &quot;0.3.5&quot;, &quot;mocha&quot;: &quot;2.2.5&quot;, &quot;should&quot;: &quot;7.0.2&quot;, &quot;supertest&quot;: &quot;1.0.1&quot;, &quot;body-parser&quot;: &quot;~1.13.3&quot;, &quot;connect-redis&quot;: &quot;~2.4.1&quot;, &quot;cookie-parser&quot;: &quot;~1.3.5&quot;, &quot;cookie-session&quot;: &quot;~1.2.0&quot;, &quot;express-session&quot;: &quot;~1.11.3&quot;, &quot;jade&quot;: &quot;~1.11.0&quot;, &quot;method-override&quot;: &quot;~2.3.5&quot;, &quot;morgan&quot;: &quot;~1.6.1&quot;, &quot;multiparty&quot;: &quot;~4.1.2&quot;, &quot;vhost&quot;: &quot;~3.0.1&quot; }, &quot;engines&quot;: { &quot;node&quot;: &quot;&gt;= 0.10.0&quot; }, &quot;files&quot;: [ &quot;LICENSE&quot;, &quot;History.md&quot;, &quot;Readme.md&quot;, &quot;index.js&quot;, &quot;lib/&quot; ], &quot;scripts&quot;: { &quot;test&quot;: &quot;mocha --require test/support/env --reporter spec --bail --check-leaks test/ test/acceptance/&quot;, &quot;test-ci&quot;: &quot;istanbul cover node_modules/mocha/bin/_mocha --report lcovonly -- --require test/support/env --reporter spec --check-leaks test/ test/acceptance/&quot;, &quot;test-cov&quot;: &quot;istanbul cover node_modules/mocha/bin/_mocha -- --require test/support/env --reporter dot --check-leaks test/ test/acceptance/&quot;, &quot;test-tap&quot;: &quot;mocha --require test/support/env --reporter tap --check-leaks test/ test/acceptance/&quot; }, &quot;gitHead&quot;: &quot;ef7ad681b245fba023843ce94f6bcb8e275bbb8e&quot;, &quot;bugs&quot;: { &quot;url&quot;: &quot;https://github.com/strongloop/express/issues&quot; }, &quot;_id&quot;: &quot;express@4.13.3&quot;, &quot;_shasum&quot;: &quot;ddb2f1fb4502bf33598d2b032b037960ca6c80a3&quot;, &quot;_from&quot;: &quot;express@*&quot;, &quot;_npmVersion&quot;: &quot;1.4.28&quot;, &quot;_npmUser&quot;: { &quot;name&quot;: &quot;dougwilson&quot;, &quot;email&quot;: &quot;doug@somethingdoug.com&quot; }, &quot;maintainers&quot;: [ { &quot;name&quot;: &quot;tjholowaychuk&quot;, &quot;email&quot;: &quot;tj@vision-media.ca&quot; }, { &quot;name&quot;: &quot;jongleberry&quot;, &quot;email&quot;: &quot;jonathanrichardong@gmail.com&quot; }, { &quot;name&quot;: &quot;dougwilson&quot;, &quot;email&quot;: &quot;doug@somethingdoug.com&quot; }, { &quot;name&quot;: &quot;rfeng&quot;, &quot;email&quot;: &quot;enjoyjava@gmail.com&quot; }, { &quot;name&quot;: &quot;aredridel&quot;, &quot;email&quot;: &quot;aredridel@dinhe.net&quot; }, { &quot;name&quot;: &quot;strongloop&quot;, &quot;email&quot;: &quot;callback@strongloop.com&quot; }, { &quot;name&quot;: &quot;defunctzombie&quot;, &quot;email&quot;: &quot;shtylman@gmail.com&quot; } ], &quot;dist&quot;: { &quot;shasum&quot;: &quot;ddb2f1fb4502bf33598d2b032b037960ca6c80a3&quot;, &quot;tarball&quot;: &quot;http://registry.npmjs.org/express/-/express-4.13.3.tgz&quot; }, &quot;directories&quot;: {}, &quot;_resolved&quot;: &quot;https://registry.npmjs.org/express/-/express-4.13.3.tgz&quot;, &quot;readme&quot;: &quot;ERROR: No README data found!&quot; }package.json 属性说明 name - 包名 version - 包的版本号 description - 包的描述 homepage - 包的官网 url author - 包的作者姓名 contributors - 包的其他贡献者姓名 dependencies - 依赖包列表 respository - 包代码存放地方的类型，可以是git或svn，git可以在 Github上。 main - main 字段制定了程序的主入口文件，require(‘package-name’)就会加载这个文件。字段默认值是模块根目录下面的index.js。 keywords - 关键字 卸载模块&gt; npm uninstall express查看模块&gt; npm list更新模块&gt; npm update express搜索模块&gt; npm search express创建模块&gt; npm init This utility will walk you through creating a package.json file. It only covers the most common items, and tries to guess sensible defaults. See `npm help json` for definitive documentation on these fields and exactly what they do. Use `npm install &lt;pkg&gt; --save` afterwards to install a package and save it as a dependency in the package.json file. Press ^C at any time to quit. name: (node_modules) fe.docs # 模块名 version: (1.0.0) description: 网页文档 # 描述 entry point: (index.js) test command: git repository: https://github.com/daysunx2/fe.docs.git # Github 地址 keywords: author: license: (ISC) About to write to ……/node_modules/package.json: # 生成地址 { &quot;name&quot;: &quot;fe.docs&quot;, &quot;version&quot;: &quot;1.0.0&quot;, &quot;description&quot;: &quot;网页文档&quot;, …… } Is this ok? (yes) yes &gt; npm adduser &gt; Username: day &gt; Password: ****** &gt; Email: （this IS public）xxx@qq.com发布项目&gt; npm publish版本号使用NPM 下载和发布代码时都会接触到版本号，NPM 使用语义版本号来管理代码。 语义版本号分为 x.y.z 三位，分别代表主版本号、次版本号、和补丁版本号。当代码变更时，版本号应按照以下原则进行更新。 如果只是修复bug，需要更新Z 位 如果是新增了功能，但是向下兼容，需要更新Y位。 如果有大的变动，向下不兼容，则需要更新X位。 NPM 常用命令 NPM 提供了很多的命令 如 install , publish ，使用 npm help 可以查看所有命令。 npm help &lt;commond&gt; 可查看某条命令的详细帮助，例如 npm help install。 在package.json 所在的目录下使用 npm install . -g可先在本地安装当前命令行程序，可用于发布前的本地测试。 使用npm update &lt;package&gt;可以把当前目录下的node_modules子目录里边的对应模块更新至最新版本。 npm update &lt;package&gt; -g 可以把全局安装的对应命令行程序更新至最新版。 npm cache clear 可以清空NPM本地缓存，用于对付使用相同版本号发布新版本代码的人。 npm unpublish &lt;package&gt;@&lt;version&gt; 可以撤销发布自己发布过的某个版本代码。 使用淘宝NPM 镜像使用cnpm（gzip压缩支持）命令行工具代替默认的npm：&gt; npm install -g cnpm --registry=https://registry.npm.taobao.org &gt; cnpm install [name]]]></content>
      <categories>
        <category>Node.js</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Webstorm NodeJs 提示]]></title>
    <url>%2F2018%2F04%2F17%2Fwebstorm-nodejs-ti-shi%2F</url>
    <content type="text"><![CDATA[￼￼￼]]></content>
      <categories>
        <category>IDE</category>
      </categories>
      <tags>
        <tag>rare</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[find命令]]></title>
    <url>%2F2018%2F04%2F12%2Ffind%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[find格式 1find pathname -options 选项（options） 含义 -name filename 查找名为filename的文件 -perm 按执行权限来查找 -user username 按文件属主来查找 -group groupname 按组来查找 -mtime -n +n 按文件更改时间来查找文件，-n指n天以内，+n指n天以前 -atime -n +n 按文件访问时间来查找文件，-n指n天以内，+n指n天以前 -ctime -n +n 按文件创建时间来查找文件，-n指n天以内，+n指n天以前 -nogroup 查无有效属组的文件，即文件的属组在/etc/groups中不存在 -nouser 查无有效属主的文件，即文件的属主在/etc/passwd中不存在 -newer f1 !f2 查更改时间比f1新但比f2旧的文件 -type b/d/c/p/l/f 查是块设备、目录、字符设备、管道、符号链接、普通文件 -size n[c] 查长度为n块[或n字节]的文件（1块=512byte，1024byte=1kb=2块） -depth 使查找在进入子目录前先行查找完本目录 -maxdepth n 查找包括当前目录加上当前目录子目录总共n层 -fstype 查位于某一类型文件系统中的文件，这些文件系统类型通常可 在/etc/fstab中找到 -mount 查文件时不跨越文件系统mount点 -follow 如果遇到符号链接文件，就跟踪链接所指的文件 -cpio 对匹配的文件使用cpio命令，将他们备份到磁带设备中 -prune 忽略某个目录，格式：find pathname -path ignorepath -prune -o [options] 举例说明： 查找/usr/local/目录下（包含子目录）所有以.txt结尾的文件 1find /usr/local/ -name "*.txt" 查找当前目录（包含子目录）以大写字母开头的文件 1find -name "[A-Z]*" 查找当前目录（包含子目录）以一个英文一个数字开头的txt文件 1find . -name "[a-z][0-9]*.txt" 查找当前目录（包含子目录）权限为755的文件 1find -perm -755 查找当前目录（包含一层子目录）所有txt文件；“2”表示当前目录和一层子目录两层目录 1find -maxdepth 2 -name "*.txt" 查找当前目录（不包含子目录）七天内创建的文件名为小写英文字母的所有文件 1find -maxdepth 1 -ctime -7 -name "[a-z]*" 查找当前目录（包含子目录）所有文件夹 1find -type d 查找当前目录（包含子目录）所有以小写字母t开头的文件夹 1find -type d -name "[t]*" 查找当前目录（包含子目录）所有以小写字母a或b或c开头的文件 1find -type f -name "[a-c]*" 查找当前目录（包含三层子目录）所有以小写字母e开头的文件 1find -maxdepth 4 -name "[e]*" -type d 查找当前目录（包含子目录）所有大于243字节的文件 1find -size +243c 查找当前目录（包含子目录）所有等于4096字节的 文件夹（-type d） 1find -type d -size 4096c 查找当前目录（不包含子目录）所有大于7块（1块=512字节）的文件 1find -maxdepth 1 -type f -size +7 查找当前目录（不包含子目录）所有小于5k的文件 1find -maxdepth 1 -type f -size -5k 查找当前目录（不包含子目录）所有大于1M的文件 1find -maxdepth 1 -type f -size +1M 查找当前目录（不包含子目录）所有大于1G的文件 1find -maxdepth 1 -type f -size +1G 查找当前目录（不包含子目录）所有大于1M且小于266M的文件 1find -maxdepth 1 -type f -size +1M -size -266M 查找 /usr 目录下除了 /usr/lib 目录所有的以小写字母e开头的txt文件 1find /usr/ -path /usr/lib -prune -o -name "[e]*.txt"]]></content>
      <categories>
        <category>Linux命令</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[阿里fastjson的基础使用]]></title>
    <url>%2F2018%2F04%2F10%2F%E9%98%BF%E9%87%8Cfastjson%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[实体类或集合转JSONString jsonString = JSONObject.toJSONString(实体类);JSON转JSONObjectJSONObject jsonObject = JSONObject.parseObject(jsonString);JSON转实体类实体类 javaBean = JSON.parseObject(json, 实体类.class);JSON转带泛型的List的集合List&lt;T&gt; list = JSON.parseObject(json, new TypeReference&lt;List&lt;T&gt;&gt;(){});]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[photoshop常规快捷键]]></title>
    <url>%2F2018%2F04%2F08%2Fphotoshop-e5-b8-b8-e8-a7-84-e5-bf-ab-e6-8d-b7-e9-94-ae%2F</url>
    <content type="text"><![CDATA[ctrl+d 取消选取； 合并所有图层，就按CTRL+SHIFT+E健，如果想合并其中两个或几个，就在图层的眼睛图标后面点一下，链接图层（合多少点多少）然后按下CTRL+E健；]]></content>
      <categories>
        <category>Photoshop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spring boot 启动方式]]></title>
    <url>%2F2018%2F04%2F06%2Fspring-boot-e5-90-af-e5-8a-a8-e6-96-b9-e5-bc-8f%2F</url>
    <content type="text"><![CDATA[1.编辑器内启动主方法； 2.命令行到项目目录，输入命令 mvn spring-boot:run 3.进入项目目录 mvn install打包在target目录生成jar包；然后到target目录输入命令 java -jar 包名.jar，也可以指定配置文件和端口号 比如测试环境用test配置文件，发布到12138端口 java -jar xxx.jar –spring.profiles.active=test –server.port=12138 比如生产环境用prod配置文件，发布到8848端口 java -jar xxx.jar –spring.profiles.active=prod –server.port=8848]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[node.js在windows环境安装]]></title>
    <url>%2F2018%2F04%2F03%2Fnode-js-e5-9c-a8windows-e7-8e-af-e5-a2-83-e5-ae-89-e8-a3-85%2F</url>
    <content type="text"><![CDATA[一．安装node.js 1.去官网下载安装包，安装； 2.Windows+R运行cmd输入 node -v和npm -v会出现版本信息，表示安装成功； 二．配置国内npm下载源（三种方式选其中一种） 方法一：通过config命令 npm config set registry https://registry.npm.taobao.org npm info underscore （如果上面配置正确这个命令会有字符串response） 方法二：命令行指定 npm –registry https://registry.npm.taobao.org info underscore 方法三：编辑 ~/.npmrc 加入下面内容（node安装目录node_modules/npm文件夹下） registry = https://registry.npm.taobao.org 三．下载项目 svn下载项目地址： https://svn.******.cn/svn/projects/*****/code/trunk/***** 四. 安装依赖 1.找到自己项目下载到的本地路径，输入npm i；安装依赖 2.输入npm run dev；运行脚本dev（配置写在package.json中） 3.通过http://localhost:8080访问项目，端口在config文件夹下index.js文件中配置的]]></content>
      <categories>
        <category>Node.js</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[intellij IDEA下maven项目无法导入依赖]]></title>
    <url>%2F2018%2F04%2F02%2Fintellij%20IDEA%E4%B8%8Bmaven%E9%A1%B9%E7%9B%AE%E6%97%A0%E6%B3%95%E5%AF%BC%E5%85%A5%E4%BE%9D%E8%B5%96%2F</url>
    <content type="text"><![CDATA[浪费了无法想象多的时间来解决这个非常蠢的问题！ 项目是用的官网下载到本地的maven软件，没有用IDEA自带的maven配置，默认配置只改了一个本地仓库位置。 之前从SVN下载下来maven项目，pom.xml右键Maven——Reimport不亦乐乎，屡试不爽，没遇到过任何问题，然后就因为在Maven默认配置中加了个阿里的镜像库地址………,项目中pom.xml是有配置标签的maven库用于下载自己公司的框架……]]></content>
      <categories>
        <category>IDE</category>
        <category>maven</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[intellij IDEA修改默认maven库等]]></title>
    <url>%2F2018%2F04%2F01%2Fintellij%20IDEA%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4maven%E5%BA%93%E7%AD%89%2F</url>
    <content type="text"><![CDATA[之前每次导入项目都会去改一遍所有的maven、svn，jdk等配置， 后来发现有个其他设置里面可以更改软件这些默认的配置·····，这样下次新建项目就不用一个一个修改了]]></content>
      <categories>
        <category>IDE</category>
        <category>maven</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot controller的使用]]></title>
    <url>%2F2018%2F03%2F31%2Fspring-boot-controller-e7-9a-84-e4-bd-bf-e7-94-a8%2F</url>
    <content type="text"><![CDATA[@Controller处理http请求 @RestControllerSpring4之后新加的注解，原来返回JSON需要@ResponseBody配合@Controller @RequestMapping配置URL映射 @PathVariable获取URL中的值 @RequestParam获取请求参数的值 @GetMapping组合注解@RequestMapping(value = “test”,method = RequestMethod.GET)等于@GetMapping(value = “test”)]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux下JVM调优]]></title>
    <url>%2F2018%2F03%2F30%2Flinux-e4-b8-8bjvm-e8-b0-83-e4-bc-98%2F</url>
    <content type="text"><![CDATA[##jvm参数调整,下面配置为系统2g内存下的推荐配置,如果系统内存有变化, ##-Xms1536m -Xmx1536m -Xmn768m这三项请安装比例调整 JAVA_OPTS=”-server -Xms1536m -Xmx1536m -Xmn768m -Xss256K -XX:MaxPermSize=128m -XX:MaxTenuringThreshold=7” ##jvm垃圾回收参数,下面配置为cpu=2时的推荐配置 JAVA_OPTS=”$JAVA_OPTS -XX:+UseParallelGC -XX:+UseParallelOldGC -XX:ParallelGCThreads=2”]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Intellij IDEA设置作者信息和时间]]></title>
    <url>%2F2018%2F03%2F29%2FIntellij%20IDEA%E8%AE%BE%E7%BD%AE%E4%BD%9C%E8%80%85%E4%BF%A1%E6%81%AF%E5%92%8C%E6%97%B6%E9%97%B4%2F</url>
    <content type="text"><![CDATA[配置如图： /** * @author Charlie * * @date ${YEAR}/${MONTH}/${DAY} ${HOUR}:${MINUTE} */]]></content>
      <categories>
        <category>IDE</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[df和du命令—查看磁盘和文件大小]]></title>
    <url>%2F2018%2F03%2F25%2Fdf%E5%92%8Cdu%E5%91%BD%E4%BB%A4%E2%80%94%E6%9F%A5%E7%9C%8B%E7%A3%81%E7%9B%98%E5%92%8C%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%2F</url>
    <content type="text"><![CDATA[df作用：报告文件系统磁盘空间使用情况，格式如下： df [OPTION]... [FILE]...选项 作用 -a 查看包括虚拟文件系统 –total 统计总量 -h 以一种友好的格式展示大小(例如：1K，234M，2G) -H 跟-h类似，只不过换算是用1000不是1024 -i 以inode模式显示磁盘使用情况 -t 显示指定类型磁盘。例如：df -ht ext4 常用举例： 查看硬盘大小 df -h查看当前所在目录硬盘大小 df -h .du作用：估计文件空间使用率，格式如下 du [OPTION]... [FILE]... du [OPTION]... --files0-from=F选项 作用 -a 列出所有文件的写入计数，而不仅仅是目录 -s 只显示每个参数的合计,不列出下级目录的文件，与-a冲突 -d, –max-depth=N 查看N级目录层级的数据。例如:“du -ahd2 /tmp/”表示列出tmp目录和他一层子目录的所有文件大小；“du -ahd2 /tmp/“等同于”du -ah –max-depth=2 /tmp/“ -h 以一种友好的格式展示大小(例如：1K，234M，2G) -c 统计总量 常用举例： 查看文件或者文件夹大小 du -sh file]]></content>
      <categories>
        <category>Linux命令</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux svn更新/恢复到指定版本]]></title>
    <url>%2F2018%2F03%2F19%2Flinux-e6-9b-b4-e6-96-b0-e5-88-b0-e6-9f-90-e4-b8-80-e7-89-88-e6-9c-ac%2F</url>
    <content type="text"><![CDATA[svn up //更新到最新版本； svn log -l 7 //查看历史提交记录7个版本，7是显示条数，随意修改； svn up -r 52197 //更新到版本号为52197的版本，可以通过 “svn log -l n” 命令查看版本号。]]></content>
      <categories>
        <category>SVN</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sql的having和where]]></title>
    <url>%2F2018%2F03%2F18%2Fsql-e7-9a-84having-e5-92-8cwhere%2F</url>
    <content type="text"><![CDATA[where： 是一个约束声明，使用Where来约束来之数据库的数据，Where是在结果返回之前起作用的，且Where中不能使用聚合函数（例如SUM, COUNT, MAX, AVG等） having：是一个过滤声明，是在查询返回结果集以后对查询结果进行的过滤操作，一定要和group by连用，在Having中可以使用聚合函数 例：查询总分大于250分的学生 如果不是用having使用where： SELECT * FROM (SELECT g.student_name,SUM(g.score) sumscore FROM grade g GROUP BY g.student_name) t WHERE t.sumscore &gt; 250 如果使用having SELECT g.student_name,SUM(g.score) FROM grade g GROUP BY g.student_name HAVING SUM(g.score) &gt; 250;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JPA criteria 查询:类型安全与面向对象]]></title>
    <url>%2F2018%2F03%2F16%2F'JPA%20criteria%20%E6%9F%A5%E8%AF%A2%E7%B1%BB%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1'%2F</url>
    <content type="text"><![CDATA[转自 https://my.oschina.net/zhaoqian/blog/133500#comment-list序言自工作以来,除了以前比较流量的hibernate,就是一直使用ORM 规范 JPA了.而这几天工作需要,研究了下JPA的标准查询,名为:JPA criteria查询.相比JPQL,其优势是类型安全,更加的面向对象. 使用标准查询,开发人员可在编译的时候就检查查询的正确与否.而以前也只是在Hibernate中听说有过.具体不详,没用过. 用的maven插件生成的.具体看这些把. Hibernate org.hibernate.jpamodelgen.JPAMetaModelEntityProcessor http://relation.to/Bloggers/HibernateStaticMetamodelGeneratorAnnotationProcessor OpenJPA org.apache.openjpa.persistence.meta.AnnotationProcessor6 http://openjpa.apache.org/builds/latest/docs/manual/manual.html#d0e11094 DataNucleus org.datanucleus.jpa.JPACriteriaProcessor http://www.datanucleus.org/products/accessplatform\_2\_1/jpa/jpql\_criteria\_metamodel.html 一.JPA元模型概念,及使用在JPA中,标准查询是以元模型的概念为基础的.元模型是为具体持久化单元的受管实体定义的.这些实体可以是实体类,嵌入类或者映射的父类.提供受管实体元信息的类就是元模型类. 描述受管类的状态和他们之间的关系的静态元模型类可以 1.从注解处理器产生 2.从程序产生 3.用EntityManager访问. 如下code,一个简单的实体类package com.demo.entities;下,实体类Employee ，假设该实体有诸如id，name和age的基本属性，还有与类Address的OneToMany关联: @Entity @Table public class Employee{ private int id; private String name; private int age; @OneToMany private List&lt;Address&gt; addresses; // Other code… }Employee类(com.demo.entities包中定义)的标准元模型类的名字将是使用 javax.persistence.StaticMetamodel注解的Employee_。元模型类的属性全部是static和public的。Employee的每一个属性都会使用在JPA2规范中描述的以下规则在相应的元模型类中映射： 诸如id，name和age的非集合类型，会定义静态属性SingularAttribute&lt;A, B&gt; b，这里b是定义在类A中的类型为B的一个对象。 对于Addess这样的集合类型，会定义静态属性ListAttribute&lt;A, B&gt; b，这里List对象b是定义在类A中类型B的对象。其它集合类型可以是SetAttribute, MapAttribute 或 CollectionAttribute 类型。 以下是用注解处理器产生的元模型类package com.demo.entities;下： import javax.annotation.Generated; import javax.persistence.metamodel.SingularAttribute; import javax.persistence.metamodel.ListAttribute; import javax.persistence.metamodel.StaticMetamodel; @Generated(&quot;org.hibernate.jpamodelgen.JPAMetaModelEntityProcesso&quot;) @StaticMetamodel(Employee.class) public class Employee_ { public static volatile SingularAttribute&lt;Employee, Integer&gt; id; public static volatile SingularAttribute&lt;Employee, Integer&gt; age; public static volatile SingularAttribute&lt;Employee, String&gt; name; public static volatile ListAttribute&lt;Employee, Address&gt; addresses; }就像它的名字表明的，注解处理器处理注解，帮助产生源代码。注解处理在编译时就能激活。元模型类遵循JPA2.0规范中为定义标准元模型类而描述的规则创建。 使用元模型类最大的优势是凭借其实例化可以在编译时访问实体的持久属性.该特性使得criteria 查询更加类型安全. 元模型API与Java中的标准反射API密切相关。主要不同在于使用标准反射API编译器无法验证其正确性。例如：下面的代码会通过编译测试： Class myClass = Class.forName(&quot;com.demo.Test&quot;); Field myField = myClass.getField(&quot;myName&quot;);编译器假定com.demo.Test中定义了属性myName，一旦该类并没有定义属性myName，编译器将抛出运行时异常。 元模型API会强制编译器检查适当的值是否分配给实体类的持久属性。例如：考虑Employee类的age属性，它是Integer变量。若该属性被赋值为String类型的值，编译器会抛出错误。该实现并不要求支持非标准特性。程序员编写的元模型类通常称为非标准元模型类。当EntityManagerFactory 创建时，持久化提供者会初始化元模型类的属性。 二.使用criteria 查询简单Demo为了更好的理解criteria 查询，考虑拥有Employee实例集合的Dept实体，Employee和Dept的元模型类的代码如下： //All Necessary Imports @StaticMetamodel(Dept.class) public class Dept_ { public static volatile SingularAttribute&lt;Dept, Integer&gt; id; public static volatile ListAttribute&lt;Dept, Employee&gt; employeeCollection; public static volatile SingularAttribute&lt;Dept, String&gt; name; } //All Necessary Imports @StaticMetamodel(Employee.class) public class Employee_ { public static volatile SingularAttribute&lt;Employee, Integer&gt; id; public static volatile SingularAttribute&lt;Employee, Integer&gt; age; public static volatile SingularAttribute&lt;Employee, String&gt; name; public static volatile SingularAttribute&lt;Employee, Dept&gt; deptId; }下面的代码片段展示了一个criteria 查询，它用于获取所有年龄大于24岁的员工： CriteriaBuilder criteriaBuilder = em.getCriteriaBuilder(); CriteriaQuery&lt;Employee&gt; criteriaQuery = criteriaBuilder.createQuery(Employee.class); Root&lt;Employee&gt; employee = criteriaQuery.from(Employee.class); Predicate condition = criteriaBuilder.gt(employee.get(Employee_.age), 24); criteriaQuery.where(condition); TypedQuery&lt;Employee&gt; typedQuery = em.createQuery(criteriaQuery); List&lt;Employee&gt; result = typedQuery.getResultList();对应的SQL: SELECT * FROM employee WHERE age &gt; 24 三.构建CriteriaQuery 实**例API说明**1.CriteriaBuilder 安全查询创建工厂,创建CriteriaQuery,创建查询具体具体条件Predicate 等CriteriaBuilder是一个工厂对象,安全查询的开始.用于构建JPA安全查询.可以从EntityManager 或 EntityManagerFactory类中获得CriteriaBuilder. 比如: CriteriaBuilder criteriaBuilder = em.getCriteriaBuilder(); 2.CriteriaQuery 安全查询主语句CriteriaQuery对象必须在实体类型或嵌入式类型上的Criteria 查询上起作用。 它通过调用 CriteriaBuilder, createQuery 或CriteriaBuilder.createTupleQuery 获得。 CriteriaBuilder就像CriteriaQuery 的工厂一样。 CriteriaBuilder工厂类是调用EntityManager.getCriteriaBuilder 或 EntityManagerFactory.getCriteriaBuilder而得。 Employee实体的 CriteriaQuery 对象以下面的方式创建： CriteriaBuilder criteriaBuilder = em.getCriteriaBuilder(); CriteriaQuery&lt;Employee&gt; criteriaQuery = criteriaBuilder.createQuery(Employee.class);3.Root 定义查询的From子句中能出现的类型AbstractQuery是CriteriaQuery 接口的父类。它提供得到查询根的方法。 Criteria查询的查询根定义了实体类型，能为将来导航获得想要的结果，它与SQL查询中的FROM子句类似。 Root实例也是类型化的，且定义了查询的FROM子句中能够出现的类型。 查询根实例能通过传入一个实体类型给 AbstractQuery.from方法获得。 Criteria查询，可以有多个查询根。 Employee实体的查询根对象可以用以下的语法获得： Root&lt;Employee&gt; employee = criteriaQuery.from(Employee.class);4.Predicate 过滤条件过滤条件应用到SQL语句的FROM子句中。 在criteria 查询中，查询条件通过Predicate 或Expression 实例应用到CriteriaQuery 对象上。 这些条件使用 CriteriaQuery .where 方法应用到CriteriaQuery 对象上。 CriteriaBuilder 也是作为Predicate 实例的工厂，Predicate 对象通过调用CriteriaBuilder 的条件方法（ equal，notEqual， gt， ge，lt， le，between，like等）创建。 Predicate 实例也可以用Expression 实例的 isNull， isNotNull 和 in方法获得，复合的Predicate 语句可以使用CriteriaBuilder的and, or andnot 方法构建。 下面的代码片段展示了Predicate 实例检查年龄大于24岁的员工实例: Predicate condition = criteriaBuilder.gt(employee.get(Employee_.age), 24); criteriaQuery.where(condition);过Employee_元模型类age属性，称之为路径表达式。若age属性与String文本比较，编译器会抛出错误，这在JPQL中是不可能的。 5.**Predicate[] 多个过滤条件**List predicatesList = new ArrayList(); predicatesList.add(…..Pridicate….) criteriaQuery.where(predicatesList.toArray(new Predicate[predicatesList.size()])); OR语句 predicatesList.add(criteriaBuilder.or(criteriaBuilder.equal(root.get(RepairOrder_.localRepairStatus), LocalRepairStatus.repairing),criteriaBuilder.equal(root.get(RepairOrder_.localRepairStatus), LocalRepairStatus.diagnos)));忽略大小写(全大写) predicatesList.add(criteriaBuilder.like(criteriaBuilder.upper(root.get(RepairShop_.shopName)), StringUtils.upperCase(StringUtils.trim(this.shopName)) + &quot;%&quot;));通过如上两句添加多个. 6._TypedQuery_执行查询与获取元模型实例注意，你使用EntityManager创建查询时，可以在输入中指定一个CriteriaQuery对象，它返回一个TypedQuery，它是JPA 2.0引入javax.persistence.Query接口的一个扩展，TypedQuery接口知道它返回的类型。 所以使用中,先创建查询得到TypedQuery,然后通过typeQuery得到结果. 当EntityManager.createQuery(CriteriaQuery)方法调用时，一个可执行的查询实例会创建，该方法返回指定从 criteria 查询返回的实际类型的TypedQuery 对象。 TypedQuery 接口是javax.persistence.Queryinterface.的子类型。在该片段中， TypedQuery 中指定的类型信息是Employee，调用getResultList时，查询就会得到执行 TypedQuery typedQuery = em.createQuery(criteriaQuery); List result = typedQuery.getResultList(); 元模型实例通过调用 EntityManager.getMetamodel 方法获得，EntityType的元模型实例通过调用Metamodel.entity(Employee.class)而获得，其被传入 CriteriaQuery.from 获得查询根。 Metamodel metamodel = em.getMetamodel();EntityType&lt;Employee&gt; Employee_ = metamodel.entity(Employee.class); Root&lt;Employee&gt; empRoot = criteriaQuery.from(Employee_);也有可能调用Root.getModel方法获得元模型信息。类型 EntityType的实例Dept_和name属性可以调用getSingularAttribute 方法获得，它与String文本进行比较： CriteriaQuery criteriaQuery = criteriaBuilder.createQuery(); Root&lt;Dept&gt; dept = criteriaQuery.from(Dept.class); EntityType&lt;Dept&gt; Dept_ = dept.getModel(); Predicate testCondition = criteriaBuilder.equal(dept.get(Dept_.getSingularAttribute(&quot;name&quot;, String.class)), &quot;Ecomm&quot;);7.Expression 用在查询语句的select，where和having子句中，该接口有 isNull, isNotNull 和 in方法Expression对象用在查询语句的select，where和having子句中，该接口有 isNull, isNotNull 和 in方法，下面的代码片段展示了Expression.in的用法，employye的年龄检查在20或24的。 CriteriaQuery&lt;Employee&gt; criteriaQuery = criteriaBuilder .createQuery(Employee.class); Root&lt;Employee&gt; employee = criteriaQuery.from(Employee.class); criteriaQuery.where(employee.get(Employee_.age).in(20, 24)); em.createQuery(criteriaQuery).getResultList();对应的SQL: SELECT * FROM employee WHERE age in (20, 24) 下面也是一个更贴切的例子: //定义一个Expression Expression&lt;String&gt; exp = root.get(Employee.id); // List&lt;String&gt; strList=new ArrayList&lt;&gt;(); strList.add(&quot;20&quot;); strList.add(&quot;24&quot;); predicatesList.add(exp.in(strList)); criteriaQuery.where(predicatesList.toArray(new Predicate[predicatesList.size()]));8.复合谓词Criteria Query也允许开发者编写复合谓词，通过该查询可以为多条件测试下面的查询检查两个条件。首先，name属性是否以M开头，其次，employee的age属性是否是25。逻辑操作符and执行获得结果记录。 criteriaQuery.where( criteriaBuilder.and( criteriaBuilder.like(employee.get(Employee_.name), &quot;M%&quot;), criteriaBuilder.equal(employee.get(Employee_.age), 25) )); em.createQuery(criteriaQuery).getResultList();连接查询在SQL中，连接跨多张表以获取查询结果，类似的实体连接通过调用 From.join 执行，连接帮助从一个实体导航到另一个实体以获得查询结果。 Root的join方法返回一个 Join&lt;Dept, Employee&gt;类型(也可以是SetJoin,，ListJoin，MapJoin 或者 CollectionJoin类型)。 默认情况下，连接操作使用内连接，而外连接可以通过在join方法中指定JoinType参数为LEFT或RIGHT来实现。 CriteriaQuery&lt;Dept&gt; cqDept = criteriaBuilder.createQuery(Dept.class); Root&lt;Dept&gt; deptRoot = cqDept.from(Dept.class); Join&lt;Dept, Employee&gt; employeeJoin = deptRoot.join(Dept_.employeeCollection); cqDept.where(criteriaBuilder.equal(employeeJoin.get(Employee_.deptId).get(Dept_.id), 1)); TypedQuery&lt;Dept&gt; resultDept = em.createQuery(cqDept);抓取连接当涉及到collection属性时，抓取连接对优化数据访问是非常有帮助的。这是通过预抓取关联对象和减少懒加载开销而达到的。 使用 criteria 查询，fetch方法用于指定关联属性 Fetch连接的语义与Join是一样的，因为Fetch操作不返回Path对象，所以它不能将来在查询中引用。 在以下例子中，查询Dept对象时employeeCollection对象被加载，这不会有第二次查询数据库，因为有懒加载。 CriteriaQuery&lt;Dept&gt; d = cb.createQuery(Dept.class); Root&lt;Dept&gt; deptRoot = d.from(Dept.class); deptRoot.fetch(&quot;employeeCollection&quot;, JoinType.LEFT); d.select(deptRoot); List&lt;Dept&gt; dList = em.createQuery(d).getResultList();对应SQL: SELECT * FROM dept d, employee e WHERE d.id = e.deptId 路径表达式Root实例，Join实例或者从另一个Path对象的get方法获得的对象使用get方法可以得到Path对象，当查询需要导航到实体的属性时，路径表达式是必要的。 Get方法接收的参数是在实体元模型类中指定的属性。 Path对象一般用于Criteria查询对象的select或where方法。例子如下： CriteriaQuery&lt;String&gt; criteriaQuery = criteriaBuilder.createQuery(String.class); Root&lt;Dept&gt; root = criteriaQuery.from(Dept.class); criteriaQuery.select(root.get(Dept_.name));&amp;nbsp;参数化表达式 在JPQL中，查询参数是在运行时通过使用命名参数语法(冒号加变量，如 :age)传入的。在Criteria查询中，查询参数是在运行时创建ParameterExpression对象并为在查询前调用TypeQuery,setParameter方法设置而传入的。下面代码片段展示了类型为Integer的ParameterExpression age，它被设置为24： ParameterExpression&lt;Integer&gt; age = criteriaBuilder.parameter(Integer.class); Predicate condition = criteriaBuilder.gt(testEmp.get(Employee_.age), age); criteriaQuery.where(condition); TypedQuery&lt;Employee&gt; testQuery = em.createQuery(criteriaQuery); List&lt;Employee&gt; result = testQuery.setParameter(age, 24).getResultList(); Corresponding SQL: SELECT * FROM Employee WHERE age = 24;排序结果 Criteria查询的结果能调用CriteriaQuery.orderBy方法排序，该方法接收一个Order对象做为参数。通过调用 CriteriaBuilder.asc 或 CriteriaBuilder.Desc，Order对象能被创建。以下代码片段中，Employee实例是基于age的升序排列。 CriteriaQuery&lt;Employee&gt; criteriaQuery = criteriaBuilder .createQuery(Employee.class); Root&lt;Employee&gt; employee = criteriaQuery.from(Employee.class); criteriaQuery.orderBy(criteriaBuilder.asc(employee.get(Employee_.age))); em.createQuery(criteriaQuery).getResultList();对应 SQL: SELECT * FROM Employee ORDER BY age ASC 分组CriteriaQuery 实例的groupBy 方法用于基于Expression的结果分组。查询通过设置额外表达式，以后调用having方法。下面代码片段中，查询按照Employee类的name属性分组，且结果以字母N开头： CriteriaQuery cq = criteriaBuilder.createQuery(Tuple.class); Root&lt;Employee&gt; employee = cq.from(Employee.class); cq.groupBy(employee.get(Employee_.name)); cq.having(criteriaBuilder.like(employee.get(Employee_.name), &quot;N%&quot;)); cq.select(criteriaBuilder.tuple(employee.get(Employee_.name),criteriaBuilder.count(employee))); TypedQuery&lt;Tuple&gt; q = em.createQuery(cq); List&lt;Tuple&gt; result = q.getResultList();对应 SQL: SELECT name, COUNT(*) FROM employeeGROUP BY name HAVING name like ‘N%’ 查询投影Criteria查询的结果与在Critiria查询创建中指定的一样。结果也能通过把查询根传入 CriteriaQuery.select中显式指定。Criteria查询也给开发者投影各种结果的能力。 使用construct()使用该方法，查询结果能由非实体类型组成。在下面的代码片段中，为EmployeeDetail类创建了一个Criteria查询对象，而EmployeeDetail类并不是实体类型。 CriteriaQuery&lt;EmployeeDetails&gt; criteriaQuery = criteriaBuilder.createQuery(EmployeeDetails.class); Root&lt;Employee&gt; employee = criteriaQuery.from(Employee.class); criteriaQuery.select(criteriaBuilder.construct(EmployeeDetails.class, employee.get(Employee_.name), employee.get(Employee_.age))); em.createQuery(criteriaQuery).getResultList(); Corresponding SQL: SELECT name, age FROM employee&lt;span style=&quot;white-space: normal;&quot;&gt;&amp;nbsp;&lt;/span&gt;返回Object[]的查询Criteria查询也能通过设置值给CriteriaBuilder.array方法返回Object[]的结果。下面的代码片段中，数组大小是2（由String和Integer组成）。 CriteriaQuery&lt;Object[]&gt; criteriaQuery = criteriaBuilder.createQuery(Object[].class); Root&lt;Employee&gt; employee = criteriaQuery.from(Employee.class); criteriaQuery.select(criteriaBuilder.array(employee.get(Employee_.name), employee.get(Employee_.age))); em.createQuery(criteriaQuery).getResultList();对应 SQL: SELECT name, age FROM employee 返回元组(Tuple)的查询数据库中的一行数据或单个记录通常称为元组。通过调用CriteriaBuilder.createTupleQuery()方法，查询可以用于元组上。CriteriaQuery.multiselect方法传入参数，它必须在查询中返回。 CriteriaQuery&lt;Tuple&gt; criteriaQuery = criteriaBuilder.createTupleQuery(); Root&lt;Employee&gt; employee = criteriaQuery.from(Employee.class); criteriaQuery.multiselect(employee.get(Employee_.name).alias(&quot;name&quot;), employee.get(Employee_.age).alias(&quot;age&quot;)); em.createQuery(criteriaQuery).getResultList();对应 SQL: SELECT name, age FROM employee 结论Criteria查询是一种以更加面向对象的方式查询数据库的方法、在本文中，我讨论了JPA2中类型安全的Criteria查询，以及对于理解Criteria查询非常重要的元模型的概念。也讨论了Criteria查询中的各种API。]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[判断当前时间在时间区间的逻辑关系，只有小时分钟秒钟]]></title>
    <url>%2F2018%2F03%2F16%2F%E5%88%A4%E6%96%AD%E5%BD%93%E5%89%8D%E6%97%B6%E9%97%B4%E5%9C%A8%E6%97%B6%E9%97%B4%E5%8C%BA%E9%97%B4%E7%9A%84%E9%80%BB%E8%BE%91%E5%85%B3%E7%B3%BB%EF%BC%8C%E5%8F%AA%E6%9C%89%E5%B0%8F%E6%97%B6%E5%88%86%E9%92%9F%E7%A7%92%E9%92%9F%2F</url>
    <content type="text"><![CDATA[需求 ：判断当前时间是否在水库的可用时间段（水库存储可用开始时间和可用结束时间两个字段）如果开始时间小于结束时间 1.可用: 开始时间&lt;当前时间&lt;结束时间 2.不可用: 当前时间 &lt; 开始时间 或者 当前时间 &gt; 结束时间如果开始时间大于结束时间 1.可用: 当前时间 &gt; 开始时间 或者 当前时间 &lt; 结束时间 2.不可用: 结束时间&lt;当前时间&lt;开始时间]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[报错]]></title>
    <url>%2F2018%2F03%2F13%2Fe6-8a-a5-e9-94-99%2F</url>
    <content type="text"><![CDATA[报错信息： Error starting ApplicationContext. To display the auto-configuration report re-run your application with ‘debug’ enabled. 2018-03-27 11:41:10.147 [main] ERROR org.springframework.boot.SpringApplication - Application startup failed org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘entityManagerFactory’ defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaAutoConfiguration.class]: Invocation of init method failed; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: default] Unable to build Hibernate SessionFactory at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1628) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:555) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:483) at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306) at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302) at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197) at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1078) at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:857) at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:543) at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:122) at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:693) at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:360) at org.springframework.boot.SpringApplication.run(SpringApplication.java:303) at org.springframework.boot.SpringApplication.run(SpringApplication.java:1118) at org.springframework.boot.SpringApplication.run(SpringApplication.java:1107) at com.kingsum.auxiliary.Startup.main(Startup.java:74) Caused by: javax.persistence.PersistenceException: [PersistenceUnit: default] Unable to build Hibernate SessionFactory at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.persistenceException(EntityManagerFactoryBuilderImpl.java:954) at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:882) at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:60) at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:353) at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:370) at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:359) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1687) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1624) … 16 common frames omitted Caused by: org.hibernate.tool.schema.spi.SchemaManagementException: Unable to execute schema management to JDBC target [alter table base_fire_company add constraint FK8vw215rygi32dc0rptb72a2wk foreign key (org_id) references base_org (id)] at org.hibernate.tool.schema.internal.TargetDatabaseImpl.accept(TargetDatabaseImpl.java:59) at org.hibernate.tool.schema.internal.SchemaMigratorImpl.applySqlString(SchemaMigratorImpl.java:431) at org.hibernate.tool.schema.internal.SchemaMigratorImpl.applySqlStrings(SchemaMigratorImpl.java:420) at org.hibernate.tool.schema.internal.SchemaMigratorImpl.applyForeignKeys(SchemaMigratorImpl.java:386) at org.hibernate.tool.schema.internal.SchemaMigratorImpl.doMigrationToTargets(SchemaMigratorImpl.java:214) at org.hibernate.tool.schema.internal.SchemaMigratorImpl.doMigration(SchemaMigratorImpl.java:60) at org.hibernate.tool.hbm2ddl.SchemaUpdate.execute(SchemaUpdate.java:134) at org.hibernate.tool.hbm2ddl.SchemaUpdate.execute(SchemaUpdate.java:101) at org.hibernate.internal.SessionFactoryImpl.(SessionFactoryImpl.java:472) at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:444) at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:879) … 22 common frames omitted Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`kingsum`.#sql-da8\_3f, CONSTRAINT `FK8vw215rygi32dc0rptb72a2wk` FOREIGN KEY (`org_id`) REFERENCES `base_org` (`id`)) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at com.mysql.jdbc.Util.handleNewInstance(Util.java:425) at com.mysql.jdbc.Util.getInstance(Util.java:408) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:935) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909) at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527) at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680) at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2483) at com.mysql.jdbc.StatementImpl.executeUpdateInternal(StatementImpl.java:1552) at com.mysql.jdbc.StatementImpl.executeLargeUpdate(StatementImpl.java:2607) at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1480) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.tomcat.jdbc.pool.StatementFacade$StatementProxy.invoke(StatementFacade.java:114) at com.sun.proxy.$Proxy104.executeUpdate(Unknown Source) at org.hibernate.tool.schema.internal.TargetDatabaseImpl.accept(TargetDatabaseImpl.java:56) … 32 common frames omitted 解决方法： 此处明确提示了 Caused by: org.hibernate.tool.schema.spi.SchemaManagementException: Unable to execute schema management to JDBC target [alter table base_fire_company add constraint FK8vw215rygi32dc0rptb72a2wk foreign key (org_id) references base_org (id)] 说明自动生成base_fire_company表的时候出现问题，删除数据库这张表重新生成即可； 也会经常遇到没有提示报错信息的情况，只有下面的提示： Application startup failed org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘entityManagerFactory’ defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaAutoConfiguration.class]: Invocation of init method failed; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: default] Unable to build Hibernate SessionFactory 试一下maven的clean然后重启，不行的话， 直接去项目路径 \target\classes下删除生成的所有class文件， 也可以尝试intellij IDEA清除缓存和索引； 个人理解： 在前一次编译时，生成的class编译文件，没有在修改代码后编译时重新生成！所以手动删除class文件，重新启动编译，让其生成新的class文件]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ IDEA 取消【import .;】星号导包]]></title>
    <url>%2F2018%2F03%2F09%2FIntelliJ%20IDEA%20%E5%8F%96%E6%B6%88%E3%80%90import%20.%3B%E3%80%91%E6%98%9F%E5%8F%B7%E5%AF%BC%E5%8C%85%2F</url>
    <content type="text"><![CDATA[Setting – Editor – Code Style – Java – Imports在【Class count to use import with ‘*’:】后填入500 在【Names count to use static import with ‘*’:】后填入500 具体数值自行填写，够大即可，截图如下：]]></content>
      <categories>
        <category>IDE</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[30秒创建带访问接口的Spring Boot项目]]></title>
    <url>%2F2018%2F03%2F04%2F30%E7%A7%92%E5%88%9B%E5%BB%BA%E5%B8%A6%E8%AE%BF%E9%97%AE%E6%8E%A5%E5%8F%A3%E7%9A%84Spring%20Boot%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[本篇使用IDEA快速创建一个最基础带访问接口的Spring Boot项目点击Create New Project按钮 选择Spring Initializr选项，Project SDK选择一个本地的，没有的可以自行配置 按自己的意愿选填各种参数 勾选上Web的依赖上面还能选择Spring boot的版本，这里默认的Spring Boot版本是2.1.5 给项目起名字和放到自己想放的目录 在启动文件中加个接口内容是package com.example.demo; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @SpringBootApplication @RestController @RequestMapping public class DemoApplication { @RequestMapping(&quot;test&quot;) public String test(){ return &quot;It&apos;s a demo&quot;; } public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } } 右键启动文件RUN启动项目 访问接口 http://127.0.0.1:8080/test]]></content>
      <categories>
        <category>java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[制作CSR文件]]></title>
    <url>%2F2018%2F02%2F27%2F%E5%88%B6%E4%BD%9CCSR%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[如何制作CSR文件?在申请数字证书之前，您必须先生成证书私钥和证书请求文件(Cerificate Signing Request，简称 CSR)。CSR 文件是您的公钥证书原始文件，包含了您的服务器信息和您的单位信息，需要提交给 CA 认证中心进行审核。 注意：建议您使用系统提供的系统创建 CSR 功能，避免出现内容不正确而导致的审核失败。关于审核失败详细信息，请参考审核失败 - 主域名不能为空。 手动生成CSR文件的同时会生成私钥文件，请务必妥善保管和备份您的私钥。 您手动生成CSR文件时，一般需要输入以下信息： 注意：输入的中文信息需要使用UTF8编码格式。 • Organization Name(O)： 申请单位名称法定名称，可以是中文或英文。• Organization Unit(OU)： 申请单位的所在部门，可以是中文或英文。• Country Code(C)： 申请单位所属国家，只能是两个字母的国家码。例如，中国只能是 CN。• State or Province(S)： 申请单位所在省名或州名，可以是中文或英文。• Locality(L)： 申请单位所在城市名，可以是中文或英文。• Common Name(CN)： 申请SSL证书的具体网站域名。 注意： 证书服务系统对 CSR 文件的密钥长度有严格要求，密钥长度必须是 2048 位，密钥类型必须为 RSA。如果申请证书是多域名或者通配子域名，在Common Name或What is your first and last name?字段只需要输入一个域名即可（通配子域名可以输入“*.example.com”等）。 使用 OpenSSL 工具生成 CSR 文件 安装OpenSSL工具。 执行命令openssl req -new -nodes -sha256 -newkey rsa:2048 -keyout myprivate.key -out mydomain.csr生成 CSR 文件。 其中， -new指定生成一个新的CSR。 -nodes指定私钥文件不被加密。 -sha256指定摘要算法。 -keyout生成私钥文件。 -newkey rsa:2048指定私钥类型和长度。 生成 CSR 文件 mydomain.csr。 需要输入的信息说明如下： 字段 说明 示例 Country Name ISO国家代码（两位字符） CN State or Province Name 所在省份 ZheJiang Locality Name 所在城市 HangZhou Organization Name 公司名称 HangZhou xxx Technologies, Inc. Organizational Unit Name 部门名称 IT Dept. Common Name 申请证书的域名 www.example.com Email Address 不需要输入 - A challenge password 不需要输入 - 完成命令提示的输入后，会在当前目录下生成 myprivate.key （私钥文件）和 mydomain.csr （CSR，证书请求文件）两个文件。 注意：在使用 OpenSSL 工具生成中文证书时需要注意中文编码格式必须使用 UTF8 编码格式。同时，需要在编译 OpenSSL 工具时指定支持 UTF8 编码格式。 如果您需要输入中文信息，建议您使用 Keytool 工具生成 CSR 文件。 使用 Keytool 工具生成 CSR 文件 安装 Keytool 工具，Keytool 工具一般包含在 Java Development Kit（JDK）工具包中。 使用 Keytool 工具生成 keystore 证书文件。 注意：Keystore 证书文件中包含密钥，导出密钥方式请参考主流数字证书都有哪些格式？ 执行命令keytool -genkey -alias mycert -keyalg RSA -keysize 2048 -keystore ./mydomain.jks生成 keystore 证书文件。其中， -keyalg指定密钥类型，必须是 RSA。 -keysize指定密钥长度为 2048。 -alias指定证书别名，可自定义。 -keystore指定证书文件保存路径。 输入证书保护密码，然后根据下表依次输入所需信息： 问题 说明 示例 What is your first and last name? 申请证书的域名 www.example.com What is the name of your organizational unit? 部门名称 IT Dept. What is the name of your organization? 公司名称 HangZhou xxx Technologies,Ltd. What is the name of your City or Locality? 所在城市 HangZhou What is the name of your State or Province? 所在省份 ZheJiang What is the two-letter country code for this unit? ISO 国家代码（两位字符） CN 输入完成后，确认输入内容是否正确，输入 Y 表示正确。 根据提示输入密钥密码。可以与证书密码一致，如果一致直接按回车键即可。 通过证书文件生成证书请求。 执行命令keytool -certreq -sigalg SHA256withRSA -alias mycert -keystore ./mydomain.jks -file ./mydomain.csr生成 CSR 文件。 其中， sigalg指定摘要算法，使用 SHA256withRSA。 alias指定别名，必须与 keystore 文件中的证书别名一致。 keystore指定证书文件。 file指定证书请求文件(CSR)。 根据提示输入证书密码即可以生成 mydomain.csr]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Windows10无法更新]]></title>
    <url>%2F2018%2F02%2F01%2FWindows10%E6%97%A0%E6%B3%95%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[更新时提示：你的设备已过期，并缺少重要的安全和质量更新，因此存在风险。让我们带你重回正轨，这样 Windows 可以更安全地运行。选择此按钮继续操作: 解决办法：右键此电脑选择管理，选择服务和应用程序，在服务项里找到Windows Update服务选择停止然后把手动项选择禁用，然后把C盘Windows目录下SoftwareDistribution文件夹里的东西都删除，重启系统然后再在服务里找到Windows Update服务启用再检查更新应该就可以了]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux各个目录的作用]]></title>
    <url>%2F2018%2F01%2F26%2FLinux%E5%90%84%E4%B8%AA%E7%9B%AE%E5%BD%95%E7%9A%84%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1、/bin ：获得最小的系统可操作性所需要的命令2、/boot ：内核和加载内核所需的文件3、/dev ：终端、磁盘、调制解调器等的设备项4、/etc ：关键的启动文件和配置文件5、/home ：用户的主目录6、/lib ：C编译器的库和部分C编译器7、/media ：可移动介质上文件系统的安装点8、/opt ：可选的应用安装包9、/proc ：所有正在运行进程的映像10、/root ：超级用户的主目录11、/sbin ：引导、修复或者恢复系统的命令12、/tmp ：每次重新引导就消失的临时文件13、/usr ：次要文件和命令的层次结构14、/usr/bin ：大多数命令和可执行文件15、/usr/include ：编译C程序的头文件16、/usr/lib ：库，供标准程序使用的支持文件17、/usr/local ：本地软件（用户所编写或者安装的软件）18、/usr/local/bin ：本地的可执行文件19、/usr/local/etc ：本地系统配置文件和命令20、/usr/local/lib ：本地的支持文件21、/usr/local/sbin ：静态链接的本地系统维护命令22、/usr/local/src ：/usr/local/*的源代码23、/usr/man ：联机用户手册24、/usr/sbin不太关键的系统管理命令和修复命令25、/usr/share ：多种系统共同的东西（只读）26、/usr/share/man ：练级用户手册27、/usr/src ：非本地软件包的源代码28、/var ：系统专用数据和配置文件29、/var/adm ：各种不同的东西30、/var/log ：各种系统日志文件31、/var/spool ：供打印机、邮件等使用的假脱机目录32、/var/tmp ：更多的临时空间（在重新引导之后，文件予以保留）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[69道Spring面试题和答案]]></title>
    <url>%2F2018%2F01%2F18%2F69%E9%81%93Spring%E9%9D%A2%E8%AF%95%E9%A2%98%E5%92%8C%E7%AD%94%E6%A1%88%2F</url>
    <content type="text"><![CDATA[69道Spring面试题和答案目录 Spring 概述 依赖注入 Spring beans Spring注解 Spring数据访问 Spring面向切面编程（AOP） Spring MVC Spring 概述1. 什么是spring?Spring 是个java企业级应用的开源开发框架。Spring主要用来开发Java应用，但是有些扩展是针对构建J2EE平台的web应用。Spring 框架目标是简化Java企业级应用开发，并通过POJO为基础的编程模型促进良好的编程习惯。 2. 使用Spring框架的好处是什么？ 轻量：Spring 是轻量的，基本的版本大约2MB。 控制反转：Spring通过控制反转实现了松散耦合，对象们给出它们的依赖，而不是创建或查找依赖的对象们。 面向切面的编程(AOP)：Spring支持面向切面的编程，并且把应用业务逻辑和系统服务分开。 容器：Spring 包含并管理应用中对象的生命周期和配置。 MVC框架：Spring的WEB框架是个精心设计的框架，是Web框架的一个很好的替代品。 事务管理：Spring 提供一个持续的事务管理接口，可以扩展到上至本地事务下至全局事务（JTA）。 异常处理：Spring 提供方便的API把具体技术相关的异常（比如由JDBC，Hibernate or JDO抛出的）转化为一致的unchecked 异常。 3. Spring由哪些模块组成?以下是Spring 框架的基本模块： Core module Bean module Context module Expression Language module JDBC module ORM module OXM module Java Messaging Service(JMS) module Transaction module Web module Web-Servlet module Web-Struts module Web-Portlet module 4. 核心容器（应用上下文) 模块。这是基本的Spring模块，提供spring 框架的基础功能，BeanFactory 是 任何以spring为基础的应用的核心。Spring 框架建立在此模块之上，它使Spring成为一个容器。 5. BeanFactory – BeanFactory 实现举例。Bean 工厂是工厂模式的一个实现，提供了控制反转功能，用来把应用的配置和依赖从正真的应用代码中分离。 最常用的BeanFactory 实现是XmlBeanFactory 类。 6. XMLBeanFactory最常用的就是org.springframework.beans.factory.xml.XmlBeanFactory ，它根据XML文件中的定义加载beans。该容器从XML 文件读取配置元数据并用它去创建一个完全配置的系统或应用。 7. 解释AOP模块AOP模块用于发给我们的Spring应用做面向切面的开发， 很多支持由AOP联盟提供，这样就确保了Spring和其他AOP框架的共通性。这个模块将元数据编程引入Spring。 8. 解释JDBC抽象和DAO模块。通过使用JDBC抽象和DAO模块，保证数据库代码的简洁，并能避免数据库资源错误关闭导致的问题，它在各种不同的数据库的错误信息之上，提供了一个统一的异常访问层。它还利用Spring的AOP 模块给Spring应用中的对象提供事务管理服务。 9. 解释对象/关系映射集成模块。Spring 通过提供ORM模块，支持我们在直接JDBC之上使用一个对象/关系映射映射(ORM)工具，Spring 支持集成主流的ORM框架，如Hiberate,JDO和 iBATIS SQL Maps。Spring的事务管理同样支持以上所有ORM框架及JDBC。 10. 解释WEB 模块。Spring的WEB模块是构建在application context 模块基础之上，提供一个适合web应用的上下文。这个模块也包括支持多种面向web的任务，如透明地处理多个文件上传请求和程序级请求参数的绑定到你的业务对象。它也有对Jakarta Struts的支持。 12. Spring配置文件Spring配置文件是个XML 文件，这个文件包含了类信息，描述了如何配置它们，以及如何相互调用。 13. 什么是Spring IOC 容器？Spring IOC 负责创建对象，管理对象（通过依赖注入（DI），装配对象，配置对象，并且管理这些对象的整个生命周期。 14. IOC的优点是什么？IOC 或 依赖注入把应用的代码量降到最低。它使应用容易测试，单元测试不再需要单例和JNDI查找机制。最小的代价和最小的侵入性使松散耦合得以实现。IOC容器支持加载服务时的饿汉式初始化和懒加载。 15. ApplicationContext通常的实现是什么? FileSystemXmlApplicationContext ：此容器从一个XML文件中加载beans的定义，XML Bean 配置文件的全路径名必须提供给它的构造函数。 ClassPathXmlApplicationContext：此容器也从一个XML文件中加载beans的定义，这里，你需要正确设置classpath因为这个容器将在classpath里找bean配置。 WebXmlApplicationContext：此容器加载一个XML文件，此文件定义了一个WEB应用的所有bean。 16. Bean 工厂和 Application contexts 有什么区别？Application contexts提供一种方法处理文本消息，一个通常的做法是加载文件资源（比如镜像），它们可以向注册为监听器的bean发布事件。另外，在容器或容器内的对象上执行的那些不得不由bean工厂以程序化方式处理的操作，可以在Application contexts中以声明的方式处理。Application contexts实现了MessageSource接口，该接口的实现以可插拔的方式提供获取本地化消息的方法。 17. 一个Spring的应用看起来象什么？ 一个定义了一些功能的接口。 这实现包括属性，它的Setter ， getter 方法和函数等。 Spring AOP。 Spring 的XML 配置文件。 使用以上功能的客户端程序。 依赖注入18. 什么是Spring的依赖注入？依赖注入，是IOC的一个方面，是个通常的概念，它有多种解释。这概念是说你不用创建对象，而只需要描述它如何被创建。你不在代码里直接组装你的组件和服务，但是要在配置文件里描述哪些组件需要哪些服务，之后一个容器（IOC容器）负责把他们组装起来。 19. 有哪些不同类型的IOC（依赖注入）方式？ 构造器依赖注入：构造器依赖注入通过容器触发一个类的构造器来实现的，该类有一系列参数，每个参数代表一个对其他类的依赖。 Setter方法注入：Setter方法注入是容器通过调用无参构造器或无参static工厂 方法实例化bean之后，调用该bean的setter方法，即实现了基于setter的依赖注入。 20. 哪种依赖注入方式你建议使用，构造器注入，还是 Setter方法注入？你两种依赖方式都可以使用，构造器注入和Setter方法注入。最好的解决方案是用构造器参数实现强制依赖，setter方法实现可选依赖。 Spring Beans21.什么是Spring beans?Spring beans 是那些形成Spring应用的主干的java对象。它们被Spring IOC容器初始化，装配，和管理。这些beans通过容器中配置的元数据创建。比如，以XML文件中 的形式定义。 Spring 框架定义的beans都是单件beans。在bean tag中有个属性”singleton”，如果它被赋为TRUE，bean 就是单件，否则就是一个 prototype bean。默认是TRUE，所以所有在Spring框架中的beans 缺省都是单件。 22. 一个 Spring Bean 定义 包含什么？一个Spring Bean 的定义包含容器必知的所有配置元数据，包括如何创建一个bean，它的生命周期详情及它的依赖。 23. 如何给Spring 容器提供配置元数据?这里有三种重要的方法给Spring 容器提供配置元数据。 XML配置文件。 基于注解的配置。 基于java的配置。 24. 你怎样定义类的作用域?当定义一个 在Spring里，我们还能给这个bean声明一个作用域。它可以通过bean 定义中的scope属性来定义。如，当Spring要在需要的时候每次生产一个新的bean实例，bean的scope属性被指定为prototype。另一方面，一个bean每次使用的时候必须返回同一个实例，这个bean的scope 属性 必须设为 singleton。 25. 解释Spring支持的几种bean的作用域。Spring框架支持以下五种bean的作用域： singleton :bean在每个Spring ioc 容器中只有一个实例。 prototype：一个bean的定义可以有多个实例。 request：每次http请求都会创建一个bean，该作用域仅在基于web的Spring ApplicationContext情形下有效。 session：在一个HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。 global-session：在一个全局的HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。 缺省的Spring bean 的作用域是Singleton. 26. Spring框架中的单例bean是线程安全的吗?不，Spring框架中的单例bean不是线程安全的。 27. 解释Spring框架中bean的生命周期。 Spring容器 从XML 文件中读取bean的定义，并实例化bean。 Spring根据bean的定义填充所有的属性。 如果bean实现了BeanNameAware 接口，Spring 传递bean 的ID 到 setBeanName方法。 如果Bean 实现了 BeanFactoryAware 接口， Spring传递beanfactory 给setBeanFactory 方法。 如果有任何与bean相关联的BeanPostProcessors，Spring会在postProcesserBeforeInitialization()方法内调用它们。 如果bean实现IntializingBean了，调用它的afterPropertySet方法，如果bean声明了初始化方法，调用此初始化方法。 如果有BeanPostProcessors 和bean 关联，这些bean的postProcessAfterInitialization() 方法将被调用。 如果bean实现了 DisposableBean，它将调用destroy()方法。 28. 哪些是重要的bean生命周期方法？ 你能重载它们吗？有两个重要的bean 生命周期方法，第一个是setup ， 它是在容器加载bean的时候被调用。第二个方法是 teardown 它是在容器卸载类的时候被调用。 The bean 标签有两个重要的属性（init-method和destroy-method）。用它们你可以自己定制初始化和注销方法。它们也有相应的注解（@PostConstruct和@PreDestroy）。 29. 什么是Spring的内部bean？当一个bean仅被用作另一个bean的属性时，它能被声明为一个内部bean，为了定义inner bean，在Spring 的 基于XML的 配置元数据中，可以在 或 元素内使用 元素，内部bean通常是匿名的，它们的Scope一般是prototype。 30. 在 Spring中如何注入一个java集合？Spring提供以下几种集合的配置元素： 类型用于注入一列值，允许有相同的值。 类型用于注入一组值，不允许有相同的值。 类型用于注入一组键值对，键和值都可以为任意类型。 类型用于注入一组键值对，键和值都只能为String类型。 31. 什么是bean装配?装配，或bean 装配是指在Spring 容器中把bean组装到一起，前提是容器需要知道bean的依赖关系，如何通过依赖注入来把它们装配到一起。 32. 什么是bean的自动装配？Spring 容器能够自动装配相互合作的bean，这意味着容器不需要和配置，能通过Bean工厂自动处理bean之间的协作。 33. 解释不同方式的自动装配 。有五种自动装配的方式，可以用来指导Spring容器用自动装配方式来进行依赖注入。 no：默认的方式是不进行自动装配，通过显式设置ref 属性来进行装配。 byName：通过参数名 自动装配，Spring容器在配置文件中发现bean的autowire属性被设置成byname，之后容器试图匹配、装配和该bean的属性具有相同名字的bean。 byType:：通过参数类型自动装配，Spring容器在配置文件中发现bean的autowire属性被设置成byType，之后容器试图匹配、装配和该bean的属性具有相同类型的bean。如果有多个bean符合条件，则抛出错误。 constructor：这个方式类似于byType， 但是要提供给构造器参数，如果没有确定的带参数的构造器参数类型，将会抛出异常。 autodetect：首先尝试使用constructor来自动装配，如果无法工作，则使用byType方式。 34.自动装配有哪些局限性 ?自动装配的局限性是： 重写： 你仍需用 和 配置来定义依赖，意味着总要重写自动装配。 基本数据类型：你不能自动装配简单的属性，如基本数据类型，String字符串，和类。 模糊特性：自动装配不如显式装配精确，如果有可能，建议使用显式装配。 35. 你可以在Spring中注入一个null 和一个空字符串吗？可以。 Spring注解36. 什么是基于Java的Spring注解配置? 给一些注解的例子.基于Java的配置，允许你在少量的Java注解的帮助下，进行你的大部分Spring配置而非通过XML文件。 以@Configuration 注解为例，它用来标记类可以当做一个bean的定义，被Spring IOC容器使用。另一个例子是@Bean注解，它表示此方法将要返回一个对象，作为一个bean注册进Spring应用上下文。 37. 什么是基于注解的容器配置?相对于XML文件，注解型的配置依赖于通过字节码元数据装配组件，而非尖括号的声明。 开发者通过在相应的类，方法或属性上使用注解的方式，直接组件类中进行配置，而不是使用xml表述bean的装配关系。 38. 怎样开启注解装配？注解装配在默认情况下是不开启的，为了使用注解装配，我们必须在Spring配置文件中配置 context:annotation-config/元素。 39. @Required 注解这个注解表明bean的属性必须在配置的时候设置，通过一个bean定义的显式的属性值或通过自动装配，若@Required注解的bean属性未被设置，容器将抛出BeanInitializationException。 40. @Autowired 注解@Autowired 注解提供了更细粒度的控制，包括在何处以及如何完成自动装配。它的用法和@Required一样，修饰setter方法、构造器、属性或者具有任意名称和/或多个参数的PN方法。 41. @Qualifier 注解当有多个相同类型的bean却只有一个需要自动装配时，将@Qualifier 注解和@Autowire 注解结合使用以消除这种混淆，指定需要装配的确切的bean。 Spring数据访问42.在Spring框架中如何更有效地使用JDBC?使用SpringJDBC 框架，资源管理和错误处理的代价都会被减轻。所以开发者只需写statements 和 queries从数据存取数据，JDBC也可以在Spring框架提供的模板类的帮助下更有效地被使用，这个模板叫JdbcTemplate。 43. JdbcTemplateJdbcTemplate 类提供了很多便利的方法解决诸如把数据库数据转变成基本数据类型或对象，执行写好的或可调用的数据库操作语句，提供自定义的数据错误处理。 44. Spring对DAO的支持Spring对数据访问对象（DAO）的支持旨在简化它和数据访问技术如JDBC，Hibernate or JDO 结合使用。这使我们可以方便切换持久层。编码时也不用担心会捕获每种技术特有的异常。 45. 使用Spring通过什么方式访问Hibernate?在Spring中有两种方式访问Hibernate： 控制反转 Hibernate Template和 Callback。 继承 HibernateDAOSupport提供一个AOP 拦截器。 46. Spring支持的ORMSpring支持以下ORM： Hibernate iBatis JPA (Java Persistence API) TopLink JDO (Java Data Objects) OJB 47.如何通过HibernateDaoSupport将Spring和Hibernate结合起来？用Spring的 SessionFactory 调用 LocalSessionFactory。集成过程分三步： 配置the Hibernate SessionFactory。 继承HibernateDaoSupport实现一个DAO。 在AOP支持的事务中装配。 48. Spring支持的事务管理类型Spring支持两种类型的事务管理： 编程式事务管理：这意味你通过编程的方式管理事务，给你带来极大的灵活性，但是难维护。 声明式事务管理：这意味着你可以将业务代码和事务管理分离，你只需用注解和XML配置来管理事务。 49. Spring框架的事务管理有哪些优点？ 它为不同的事务API 如 JTA，JDBC，Hibernate，JPA 和JDO，提供一个不变的编程模式。 它为编程式事务管理提供了一套简单的API而不是一些复杂的事务API如 它支持声明式事务管理。 它和Spring各种数据访问抽象层很好得集成。 50. 你更倾向用那种事务管理类型？大多数Spring框架的用户选择声明式事务管理，因为它对应用代码的影响最小，因此更符合一个无侵入的轻量级容器的思想。声明式事务管理要优于编程式事务管理，虽然比编程式事务管理（这种方式允许你通过代码控制事务）少了一点灵活性。 Spring面向切面编程（AOP）51. 解释AOP面向切面的编程，或AOP， 是一种编程技术，允许程序模块化横向切割关注点，或横切典型的责任划分，如日志和事务管理。 52. Aspect 切面AOP核心就是切面，它将多个类的通用行为封装成可重用的模块，该模块含有一组API提供横切功能。比如，一个日志模块可以被称作日志的AOP切面。根据需求的不同，一个应用程序可以有若干切面。在Spring AOP中，切面通过带有@Aspect注解的类实现。 53. 在Spring AOP 中，关注点和横切关注的区别是什么？关注点是应用中一个模块的行为，一个关注点可能会被定义成一个我们想实现的一个功能。 横切关注点是一个关注点，此关注点是整个应用都会使用的功能，并影响整个应用，比如日志，安全和数据传输，几乎应用的每个模块都需要的功能。因此这些都属于横切关注点。 54. 连接点连接点代表一个应用程序的某个位置，在这个位置我们可以插入一个AOP切面，它实际上是个应用程序执行Spring AOP的位置。 55. 通知通知是个在方法执行前或执行后要做的动作，实际上是程序执行时要通过SpringAOP框架触发的代码段。 Spring切面可以应用五种类型的通知： before：前置通知，在一个方法执行前被调用。 after:在方法执行之后调用的通知，无论方法执行是否成功。 after-returning:仅当方法成功完成后执行的通知。 after-throwing:在方法抛出异常退出时执行的通知。 around:在方法执行之前和之后调用的通知。 56. 切点切入点是一个或一组连接点，通知将在这些位置执行。可以通过表达式或匹配的方式指明切入点。 57. 什么是引入?引入允许我们在已存在的类中增加新的方法和属性。 58. 什么是目标对象?被一个或者多个切面所通知的对象。它通常是一个代理对象。也指被通知（advised）对象。 59. 什么是代理?代理是通知目标对象后创建的对象。从客户端的角度看，代理对象和目标对象是一样的。 60. 有几种不同类型的自动代理？BeanNameAutoProxyCreator DefaultAdvisorAutoProxyCreator Metadata autoproxying 61. 什么是织入。什么是织入应用的不同点？织入是将切面和到其他应用类型或对象连接或创建一个被通知对象的过程。 织入可以在编译时，加载时，或运行时完成。 62. 解释基于XML Schema方式的切面实现。在这种情况下，切面由常规类以及基于XML的配置实现。 63. 解释基于注解的切面实现在这种情况下(基于@AspectJ的实现)，涉及到的切面声明的风格与带有java5标注的普通java类一致。 Spring 的MVC64. 什么是Spring的MVC框架？Spring 配备构建Web 应用的全功能MVC框架。Spring可以很便捷地和其他MVC框架集成，如Struts，Spring 的MVC框架用控制反转把业务对象和控制逻辑清晰地隔离。它也允许以声明的方式把请求参数和业务对象绑定。 65. DispatcherServletSpring的MVC框架是围绕DispatcherServlet来设计的，它用来处理所有的HTTP请求和响应。 66. WebApplicationContextWebApplicationContext 继承了ApplicationContext 并增加了一些WEB应用必备的特有功能，它不同于一般的ApplicationContext ，因为它能处理主题，并找到被关联的servlet。 67. 什么是Spring MVC框架的控制器？控制器提供一个访问应用程序的行为，此行为通常通过服务接口实现。控制器解析用户输入并将其转换为一个由视图呈现给用户的模型。Spring用一个非常抽象的方式实现了一个控制层，允许用户创建多种用途的控制器。 68. @Controller 注解该注解表明该类扮演控制器的角色，Spring不需要你继承任何其他控制器基类或引用Servlet API。 69. @RequestMapping 注解该注解是用来映射一个URL到一个类或一个特定的方处理法上。 我有一个微信公众号，经常会分享一些Java技术相关的干货。如果你喜欢我的分享，可以用微信搜索“Java团长”或者“javatuanzhang”关注。 原作者姓名：深海 原出处：并发编程网 原文链接：69道Spring面试题和答案]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[let & const]]></title>
    <url>%2F2018%2F01%2F15%2Flet-const%2F</url>
    <content type="text"><![CDATA[let 块级作用域 { let a = 1; var b = 2; } a; //ReferenceError b; //2 不存在变量提升 console.log(child); //ReferenceError let child = &apos;child&apos;; 暂时性死区(TDZ:temporal dead zone) 只要块级作用域中存在let 命令,它所声明的变量就绑定这个区域,不再受外部影响. if(true){ // TDZ 开始; temp = &apos;abc&apos;; console.log(tmp);//ReferenceError; let tmp; //TDZ 结束; console.log(tmp);//undefined; temp = &apos;temp&apos;; console.log(temp);//temp; //上述代码块级作用域中在使用let命令声明变量temp之前,都属于tmp的死区. //暂时性死区意味着 typeof 也不再安全.只要变量未进行声明,那么使用的时候就会报错. //变量先声明,后使用. } 不允许重复声明 let 不允许在相同的作用域内重复声明同一个变量. const const 声明一个只读的常量.一旦声明,常量的值就不能改变.TypeError: Assignment to constant variable. const 只声明不赋值,就会报错.SyntaxError: Missing initializer in const declaration 只在所声明的块级作用域内有效 不存在变量提升 暂时性死区 不可重复声明 const 实际上保证的,并不是变量的值不得改动,而是变量指向的那个内存地址不得改动.对于简单的数据类型(数值,字符串,布尔值),值就保存在变量指向的那个内存地址中因此等同于常量.但是将一个复合类型的数据(主要是对象和数组)变量指向的内存地址,保存的只是一个指针,const 只能保证这个指针是固定的,至于它指向的数据结构是不是可变的,就完全不能控制了. const foo = {}; //为 foo 添加一个属性,可以成功. foo.prop = &apos;123&apos;; console.log(foo.prop);//123 //将foo 指向另一个对象,就会报错. foo = {};//TypeError: Assignment to constant variable. ES6 声明变量的六种方法 ES5 只有两种声明变量的方法: var 命令和 function 命令.ES6 除了let 和 const 命令. 还会有import 和class 命令. 顶层对象的属性 顶层对象,在浏览器环境中指的是 window对象,在Node 指的是global对象. 在ES5中,顶层对象的属性与全局变量是等价的. window.a = 1; a;//1 a=2; window.a;//2 顶层对象被设计来与全局变量挂钩,被认为是javascript语言最大的设计败笔之一.这样的设计带来几个问题. 首先是没法再编译时候就报出变量未声明的错误,只有运行时候才能够知道(因为全局变量可能是顶层对象的属性创造的,而属性的创造是动态的); 其次,容易创造出全局变量. 顶层对象的属性是到处可以读写的.这非常不利于模块化. ES6 为了改变这一点,一方面规定,为了保证兼容性,var命令和function命令 声明的全局变量,依旧是顶层对象的属性;另一方面规定,let命令,const命令,class命令生命的全局变量,不属于顶层的属性.也就是说,从ES6 开始,全局变量将逐步与顶层对象的属性脱钩. global对象 ES5 的顶层对象,本身也是一个问题. 因为它在各种实现里面是不统一的. 浏览器中,顶层对象是window,但Node和Web Worker 没有window. 浏览器和Web Worker里面,self 指向的是顶层对象,但是 Node 没有self. Node 里面,顶层对象是global ,但是其他环境都不支持. 同一段代码为了能够在各种环境中,都能取到顶层对象,现在一般使用this对象,但是在Node模块和 Es6 模块中,this返回的是当前模块.函数里面的this,如果函数不是作为对象的方法运行,而是单纯的作为函数运行,this 会指向顶层对象.但是严格模式下,this会返回undefined. 新提案,在语言标准层面,引入global作为顶层对象,也就是说在所有环境下,global都是存在的,都可以从它拿到顶层对象.(垫片库system.global模拟了这个提案,可以在所有环境下拿到global)]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[条件渲染]]></title>
    <url>%2F2018%2F01%2F12%2Ftiao-jian-xuan-ran%2F</url>
    <content type="text"><![CDATA[v-if 在字符串模板中，比如 Handlebards，条件块： { {#if ok}} &lt;h1&gt;Yes&lt;/h1&gt; { {/if}} 在Vue中，使用 v-if 指令实现同样的功能： Yes 也可以添加else块 Yes No 在元素上使用v-if 条件渲染分组 因为 v-if 是一个指令，所以必须将其添加到一个元素之上。但是 如果想要多个元素。可以将 元素 当做一个不可见的 包裹元素，并且在上面使用 v-if。 最终的渲染结果将不包含元素。v-else-if 可以使用v-else指令来表示 v-if 的”else-if 块”。v-else 可以使用v-else指令来表示 v-if 的”else 块”。 // 类似于 v-else，v-else-if 也必须紧跟在带 v-if 或者 v-else-if 的元素之后。 &lt;div v-if=&quot;Math.random()&gt; 0.5&quot;&gt; Now you see me &lt;/div&gt; &lt;div v-else-if=&quot;Math.random()== 0.5&quot;&gt; Now you don&apos;t &lt;/div&gt; &lt;div v-else&gt; Now you don&apos;t &lt;/div&gt;用key 管理可复用的元素 Vue 会尽可能高效地渲染元素，通常会复用已有元素而不是重头开始渲染。这么做除了会使得vue 变得很快以外。还有一些其它的好处。例如：如果你允许用户在不同的登录方式之间切换： &lt;div v-if=&quot;loginType===&apos;username&apos;&quot;&gt; &lt;label&gt;Username&lt;/label&gt; &lt;input placeholder=&quot;Enter your username&quot;&gt; &lt;/div&gt; &lt;div v-else&gt; &lt;label&gt;Username&lt;/label&gt; &lt;input placeholder=&quot;Enter your username&quot;&gt; &lt;/div&gt; 上述代码中 切换 loginType 将不会清除 用户已经输入的内容。因为两个模板使用了相同的元素， 不会被替换掉–仅仅是替换掉它的placeholder。这样也不总是符合实际需求，所以Vue 提供了一种方式来表达，这两个元素是独立的，不要复用它们。只需要添加一个具有唯一的值的 key 属性即可： &lt;div v-if=&quot;loginType===&apos;username&apos;&quot;&gt; &lt;label&gt;Username&lt;/label&gt; &lt;input placeholder=&quot;Enter your username&quot; key=&quot;username-input&quot;&gt; &lt;/div&gt; &lt;div v-else&gt; &lt;label&gt;Username&lt;/label&gt; &lt;input placeholder=&quot;Enter your username&quot;&gt; &lt;/div key=&quot;adminname-key&quot;&gt; // 注&lt;label&gt; 元素仍然会被高效的利用，因为其没有添加key属性。v-show 另一个用于根据条件展示元素的选项是v-show 指令。用法大致一样：&amp;lt；h1 v-show=”ok”&gt;Hello不同的是带有 v-show 的元素始终会被渲染并保留在DOM 中。 v-show 只是简单的切换元素的 css属性 display。v-if 是真正的条件渲染，因为他会确保在切换过程中条件快内的事件监听器和子组件适当的被销毁和重建。v-if 也是惰性的。如果在渲染时条件为假，则什么都不做–直到条件第一次变为真时，才会开始渲染条件块。相比之下， v-show 就简单的多–不管初始条件是什么，元素总是被渲染，只是简单的基于css 进行切换。一般来说， v-if 有更高的切换开销，而v-show 有更高的初始渲染开销。因此，如果需要非常频繁地切换，则使用v-show 较好； 如果在条件运行时候很少改变。则使用v-if较好。v-for 比 v-if 具有更高的优先级。]]></content>
      <categories>
        <category>JavaScript</category>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Class 与 Style 的绑定]]></title>
    <url>%2F2018%2F01%2F11%2FClass%20%E4%B8%8E%20Style%20%E7%9A%84%E7%BB%91%E5%AE%9A%2F</url>
    <content type="text"><![CDATA[操作元素的class 列表和内联样式是数据绑定的一个常见需求。因为他们都是属性，所以可以使用 v-bind 处理它们： 只需要通过表达式计算出字符串结果即可。不过字符串拼接麻烦且易错。因此，将v-bind 用于class 和 style时，Vue.js 做了一个专门的增强。表达式结果的类型除了是字符串之外，还可以是对象或者数组。 绑定 HTML Class 可以传递给v-bind:class 一个对象，以动态的切换class： &lt;style&gt; .active { color： red; } &lt;/style&gt; &lt;div class=&quot;dynamic-toggle-class&quot;&gt; &lt;div :class=&quot;{ active:isActive }&quot;&gt; active on/off &lt;/div&gt; &lt;a @click=&quot;toggleActive&quot;&gt;toggle&lt;/a&gt; &lt;/div&gt; var toggleClass = new Vue({ el:&apos;.dynamic-toggle-class&apos;, data:{ isActive:true }, methods:{ toggleActive: function () { this.isActive = !this.isActive; } } }); 上述语法表示 active 这个class 存在与否将区取决于数据属性 isActive 的 truthiness。可以再对象中传入更多的属性来动态切换多个class， 此外 v-bind 指令也可以与普通的class 属性共存。 &lt;div class=&quot;static&quot; :class=&quot;{ active: isActive, &apos;.text-danger&apos;: hasError }&quot;&gt; ... &lt;/div&gt; // data { isActive: true, hasError: false } // result &lt;div class=&quot;static active&quot;&gt;...&lt;/div&gt; 当isActive 或者 hasError 变化时，class 列表将会更新。例如，如果 hasError 的值为true，class 列表将变为 “static active text-danger”绑定的数据对象不一定需要内联定义在模板里： &lt;div class=&quot;classObject&quot;&gt;...&lt;/div&gt; data:{ classObject:{ active:true, &apos;text-danger&apos;:false } } // 渲染结果与上面一样， 可以在这里绑定一个返回对象的计算属性。这是一个常用且强大的模式： &lt;div class=&quot;classObject&quot;&gt;...&lt;/div&gt; data:{ isActive:true, error: null }, computed: { classObject:function(){ return { active: this.isActive &amp;&amp; !this.error, &apos;text-danger&apos;: this.error &amp;&amp; this.error.type ===&apos;fatal&apos; } } }数组语法 可以将一个数组传给 v-bind:class, 以应用一个class列表。 &lt;div :class=&quot;[activeClass,errorClass]&quot;&gt; ... &lt;/div&gt; data:{ activeClass:&apos;active&apos;, errorClass:&apos;text-danger&apos; } // 渲染效果为： &lt;div class=&quot;active text-danger&quot;&gt;...&lt;/div&gt; // 这样将始终添加errorClass，但是只有在 isActive 是 truthy 时候才添加activeClass。 // 不过，当有多个条件class 的时候，为避免繁琐，可以再数组语法中使用对象语法。 &lt;div v-bind:class=&quot;[{ active: isActive }, errorClass]&quot;&gt;...&lt;/div&gt;用在组件上 当在一个自定义组件上适应class 属性的时候，这些类将被添加到该组件的根元素上面。这个元素上面已经存在的类不被覆盖。e.g. Vue.component(&apos;my-component&apos;,function(){ template:&apos;&lt;p class=&quot;foo bar&quot;&gt;SayHi&lt;/p&gt;&apos; }); &lt;my-componetnt class=&quot;baz boo&quot;&gt;&lt;/my-component&gt; // 最终渲染结果： &lt;p class=&quot;foo bar baz boo&quot;&gt;&lt;/p&gt; // 对于带数据绑定的class 同样适用 &lt;my-component :class=&quot;{ active: isActive}&quot;&gt;&lt;/my-component&gt; // 当 isActive 为 truthy 时，Html 将被渲染成为： &lt;p class=&quot;foo bar active&quot;&gt;&lt;/div&gt;绑定内联样式 对象语法 v-bind:style 的对象语法十分直观–看着试分析像是CSS， 但是其实是一个javascript对象。css 属性名可以用驼峰式(camelCase) 或 短横线分割（kebab-case,记得用单引号括起来）来命名： &lt;div v-bind:style=&quot;{ color:activeColor, fontSize: fontSize + &apos;px&apos;}&quot;&gt; &lt;/div&gt; data:{ activeColor: &apos;red&apos;, fontSize: 30 } // 直接绑定到一个样式对象通常更好，让模板更加清晰 &lt;div v-bind:style=&quot;styleObject&quot;&gt;&lt;/div&gt; data:{ styleObject：{ color：&apos;red&apos;, font-size: 13px } } // 同样的 对象语法常常结合返回对象的计算属性使用。数组语法v-bind:style 的数组语法可以将多个样式对象应用到同一个元素上： &lt;div v-bind:style=&quot;[baseStyle, overridingStyles]&quot;&gt;&lt;/div&gt;自动添加前缀 v-bind:style 使用需要添加浏览器引擎前缀的css 属性时， 如transform，vue.js 会自动侦测并添加相应的前缀。 多重值 从2.3.0 起，可以为style 绑定中的属性提供一个包含多个值的数组，常用于提供多个带前缀的值 &lt;div :style=&quot;{ display: [&apos;-webkit-box&apos;, &apos;-ms-flexbox&apos;, &apos;flex&apos;] }&quot;&gt;&lt;/div&gt; 这样写只会渲染数组中最后一个被浏览器支持的值。在本例中，如果浏览器支持不带浏览器前缀的 flexbox，那么就只会渲染 display: flex。]]></content>
      <categories>
        <category>JavaScript</category>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算属性和观察者]]></title>
    <url>%2F2018%2F01%2F10%2Fji-suan-shu-xing-he-guan-cha-zhe%2F</url>
    <content type="text"><![CDATA[计算属性 模板内的表达式非常便利，但是设计的初衷是用于简单运算的，在模板中放入太多的逻辑会让模板过重且难以维护。 例如： &lt;div id=&quot;example&quot;&gt; { { message.split(&apos;&apos;).reverse().join(&apos;&apos;) }} &lt;/div&gt; 上述模板步子啊是简单的声明式逻辑。 对于任何复杂逻辑，都应当使用计算属性。 基础例子&lt;div id=&quot;example&quot;&gt; &lt;p&gt;Original message: &quot;{ {message}}&quot;&lt;/p&gt; &lt;p&gt;Computed reversed message: &quot;{ { reversesMessage }}&quot; &lt;/div&gt; var vm = new Vue({ el:&apos;#example&apos;, data:{ message:&apos;Hello&apos; }, computed:{ //计算属性的getter reverseMessage: function(){ // this 指向 vm 实例 return this。message.split(&apos;&apos;).reverse().join(&apos;&apos;); } } }); Original message: “Hello”Computed reversed message: “olleH” 上述声明了一个计算属性 reversedMessage. 提供的函数将用作属性 vm.reversedMessage 的getter 函数： console.log(vm.reversedMessage); // =&gt;&apos;olleH&apos; vm.message = &apos;Goodbye&apos;; console.log(vm.reversedMessage); // =&gt;&apos;eybdooG&apos; 可以打开浏览器的控制台，自行修改例子中的 vm。 vm。reversedMessage的值始终取决于 vm.message 的值。可以像绑定普通属性一样在模板中绑定计算属性。Vue 知道 vm.reverseMessage 依赖于 vm.message， 因此当 vm.message 发生改变时，所有依赖vm.reversedMessage 的绑定也会更新。而且最妙的是已经以声明式的方式创建了这种依赖关系： 计算属性的getter 函数是没有副作用（side effect）的，这使得他更易于测试和理解。 计算属性缓存 vs 方法 可以在表达式中调用方法来达到相同的效果。 &lt;p&gt; Reversed messaged: &quot;{ { reversedMessage }}&quot;&lt;/p&gt; // 在组件中 methods：{ reverseMessage: function(){ return this.message.split(&apos;&apos;).reverse().join(&apos;&apos;); } } 可以将同一函数定义为一个方法而不是一个计算属性。两种方式的最终结果是完全相同的。 然而，不同的是计算属性是基于它们的依赖进行缓存的。计算属性只有在它的相关依赖发生改变时候才会重新求值。 这意味着只要message还没有发生改变，多次访问 reversedMessage 计算属性会立即返回之前的计算结果，而不必再次执行函数。这也意味着下面的计算属性将不再更新， 因为 Date.now( ) 不是响应式依赖： computed:{ now: function(){ return Date.now(); } } 相比之下，每当触发重新渲染的时候，调用方法总会再次执行函数。 计算属性 vs 侦听属性 vue 提供了一种更通用的方式来观察和响应 Vue实例上的数据变动： 侦听属性。 当有一些数据需要随着其他数据变动而变动的时候，很容易滥用watch 。通常更好的做法是使用计算属性而不是命令式的watch 回调。 &lt;div id=&quot;demo&quot;&gt;{ { fullName }}&lt;/div&gt; //watch 版本 var vm = new Vue({ el:&apos;#demo&apos;, data:{ firstName:&apos;Foo&apos;, lastName:&apos;Bar&apos;, fullName:&apos;Foo Bar&apos; }， watch:{ firstName:function(val){ this.fullName = val +&apos; &apos;+ this.lastName; }, lastName:function(val){ this.fullName = this .firstName + &apos; &apos; + val; } } }); // 计算属性版本 var vm = new Vue({ el:&apos;#demo&apos;, data:{ firstName:&apos;Foo&apos;, lastName:&apos;Bar&apos;, fullName:&apos;Foo Bar&apos; }， computed: { fullName: function(){ return this.firstName + &apos; &apos; + this.lastName; } } });计算属性的setter 计算属性默认只有getter ，不过在需要的时候可以提供一个setter。 computed:{ fullName:{ //getter get:function(){ return this.firstName + &apos; &apos; + this.lastName； }， //setter set:function(newVal){ var names = newValue.split(&apos;&apos;); this.firstName = names[0]; this.lastName = names[names.length - 1]; } } }// 现在再运行 vm.fullName = ‘John Doe’ 时，setter 会被调用， vm.firstName 和 vm.lastName 也相应地被更新。 侦听器 虽然计算属性在大多数情况下更合适，但有事也需要一个自定义的侦听器。Vue 通过watch 选项提供了一个更通用的方法，来响应数据的变化。当需要在数据变化时执行异步或者开销比较大的操作的时，此操作最为有效果。 &lt;div class=&quot;watch-example&quot;&gt; &lt;p&gt; Ask a yes/no question: &lt;input type=&quot;text&quot; v-model=&quot;question&quot;&gt; &lt;/p&gt; &lt;p&gt;{ { answer }}&lt;/p&gt; &lt;/div&gt; var watchExampleVM = new Vue({ el:&apos;.watch-example&apos;, data:{ question:&apos;&apos;, answer:&apos;I cannot give you an answer untill you ask a question!&apos; }, watch:{ question:function (newVal,oldVal) { this.answer = &apos;Waiting for you to stop typing...&apos;; this.getAnswer(); } }, methods:{ getAnswer:_.debounce( function () { if(this.question.indexOf(&apos;?&apos;)===-1){ this.answer=&apos;Questions usually contain a question mark. ;-)&apos;; return; } this.answer = &apos;Thanking...&apos;; var vm = this; axios.get(&apos;http://yesno.wtf/api&apos;) .then(function (response) { }).catch(function (reason) { vm.answer = &apos;Error! Could not reach the API.&apos;+ reason; }); },500 ) } }); 示例中， 使用watch 选项允许执行一步操作（访问API），限制执行该操作的频率，并在得到最终结果之前，设置中间状态，这些事计算属性无法做到的。除了使用 watch 选项之外，还可以使用命令式的vm.$watch API。]]></content>
      <categories>
        <category>JavaScript</category>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模板语法]]></title>
    <url>%2F2018%2F01%2F10%2Fmo-ban-yu-fa%2F</url>
    <content type="text"><![CDATA[Vue.js 使用了基于HTML的模板语法，允许开发者声明式的将DOM 绑定至底层Vue 实例的数据。 所有Vue.js 的模板都是合法的HTML。所以能够被遵循规范的浏览器和HTML解析器解析。 在底层的实现上，Vue将模板编译成虚拟DOM渲染函数。结合响应系统，在应用状态改变时候，Vue 能够智能的计算出重新渲染组件的最小代价并应用到DOM操作上。 若足够熟悉虚拟DOM，并且偏爱javascript的原始力量，可以不使用模板，直接写渲染函数（render），使用可选的JSX语法。 插值文本 数据绑定最常见的形式就是使用”Mustache”(双大括号)的文本插值： &lt;span&gt; Message: { { msg }} &lt;/span&gt; Mustache 标签将会被替换为对应数据上的msg 属性的值。 无论何时， 绑定的数据对象上msg 属性发生改变时，插值处的内容也会进行更新。 通过使用 v-once 指令也能够实现一次性地插值。当数据改变时候，插值处的内容并不会更新。会影响到该节点上所有数据的绑定。 原始HTML 双大括号会将数据解释为普通文本，而非HTML代码。为了输出原始的HTML代码，需要使用 v-html 指令： &lt;p&gt; Using mustaches: { { rawHtml }} &lt;/p&gt; &lt;p&gt; Using v-html directive: &lt;span v-html=&quot;rawHtml&quot;&gt;&lt;/span&gt; &lt;/p&gt; span 中的内容将会被替换为属性值 rawHtml，直接作为HTML – 会忽略解析属性值中的数据绑定。注意不能使用v-html来复合局部模板。因为 Vue不是基于字符串的模板引擎。反之对于用户界面，组件更适合作为可重用和可组合的基本单位。 Mustache 语法不能作用在HTML 特性上，遇到这种情况应该使用 v-bind 指令： &lt;div v-bind:id=&quot;dynamicId&quot;&gt;&lt;/div&gt; 如果 isButtonDisabled 的值是 null、undefined 或 false,则disabled特性甚至不会被包含在渲染出来的btn中。 &lt;button v-bind:disabled=&quot;isButtonDisabled&quot;&gt;&lt;/button&gt;使用javascript表达式 迄今为止，在模板中，一直使用的都只是绑定简单的属性键值。但实际上，对于所有的数据绑定，Vue.js 都提供了完全的javascript表达式支持。 { { number+1 }} { { ok ? &apos;YES&apos; : &apos;NO&apos; }} { { message.split(&apos;&apos;).reverse().join(&apos;&apos;) }} &lt;div v-bind:id=&quot;&apos;list&apos;+id&quot;&gt;&lt;/div&gt; 这些表达式会在所属Vue实例的数据作用域下作为javascript被解析。有个限制就是，每个绑定都只能包含单个表达式。下列例子不会生效： { { var a = 1;}}{ { if(ok){ return message } }} 指令 指令(Directive)是带有 v- 前缀的特殊属性，指令属性的值预期是 单个javascript表达式 （v-for 是例外的情况）。指令的职责是，当表达是改变时候，将会产生连带影响，响应式地作用于DOM。 &lt;p v-if=&quot;seen&quot;&gt;...&lt;/p&gt; 这里，v-if 指令将根据表达式 seen 的值的真假来插入/移除 元素。 参数 一些指令可以接受参数，在指令名称后面加以冒号表示。例如, v-bind 指令可以用于响应式地更新HTML 属性： &lt;a v-bind:href=&quot;url&quot;&gt;...&lt;/a&gt; 在这里 href 是参数，告知 v-bind 指令将钙元素的href 属性与 表达式url 的值绑定。另一个为 v-on 指令，它用于监听DOM事件： &lt;a v-on:click=&quot;doSomething&quot;&gt;...&lt;/a&gt; 在这里参数是监听的事件名。 修饰符 修饰符（Modifiers）是以半角句号. 指明的特殊后缀，用于只出一个指令应该以特殊的方式进行绑定。 例如 .prevent 修饰符 告诉 v-on 指令对于触发的事件调用 event.preventDefalt(): &lt;form v-on:submit.prevent=&quot;onSubmit&quot;&gt;...&lt;/form&gt; 缩写 v- 前缀作为一种视觉提示，用来识别模板中Vue 特定的特性。当使用Vue.js 为现有标签添加动态行为时(dynamic behavior)时， v- 前缀很有帮助，但是对于一些频繁用到的指令俩说，就会使用繁琐。 同时，在构建由Vue.js 管理所有模板的 单页面应用程序 SPA（single page application）时， v-前缀也变得不再那么重要，因此Vue.js为v-bind 和v-on 这两个最常用指令提供了特定简写：v-bind:attributeName =&gt; :attributeName &lt;!-- 完整写法 --&gt; &lt;a v-bind:href=&quot;url&quot;&gt;...&lt;/a&gt; &lt;!-- 简写 --&gt; &lt;a :href=&quot;url&quot;&gt;...&lt;/a&gt; v-on:eventName =&gt; @eventName &lt;!-- 完整写法 --&gt; &lt;a v-on:click=&quot;show&quot;&gt;...&lt;/a&gt; &lt;!-- 简写 --&gt; &lt;a @click=&quot;show&quot;&gt;...&lt;/a&gt; 上述看起来可能与普通的HTML 略有不同，但：与 @对于特姓名来说都是合法字符，在所有支持Vue.js 的浏览器都能够被正确的解析。并且，不会出现在最终渲染的标记中。缩写语法为可选。]]></content>
      <categories>
        <category>JavaScript</category>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vue.js 安装]]></title>
    <url>%2F2018%2F01%2F09%2Fvuejs-an-zhuang%2F</url>
    <content type="text"><![CDATA[兼容性 vue.js 不支持 IE8及其以下版本，因为vue使用了IE8无法模拟的ECMAScript 5 特性。但是支持所有兼容 ECMAScript 5 的浏览器。 Vue Devtools 在使用vue.js时候，浏览器可以安装 Vue Devtools。可以帮助在更好的界面中审查和调试 Vue 应用。 直接使用 标签引入⚠️ 在开发环境中不要使用压缩版本，会失去所有常见错误的相关警告。 直接下载并用 标签引入， Vue 会被注册为一个全局变量。 CDN https://cdn.jsdelivr.net/npm/vue 与npm 发布的最新版本一致。https://cdn.jsdelivr.net/npm/vue/ 浏览 npm 包资源。 命令行工具（CLI）⚠️ 熟悉 Vue 本身之后在使用CLI。 # 全局安装 vue-clinpm install -g vue-cli# 创建一个基于webpack 模板的新项目vue init webpack# 安装依赖cdnpm installnpm run dev 对不同构建版本的解释在npm 包中的 dist/ 目录中存在多种不同的Vue.js构建版本。 UMD CommonJS ESModule 完整版 vue.js vue.common.js vue.esm.js 只包含运行时 vue.runtime.js vue.runtime.common.js vue.runtime.esm.js 完整版（生产环境） vue.min.js - - 只包含运行时（生产环境） vue.runtime.min.js - - 术语： 完整版：同事包含编译期和运行时的版本。编译器：用来将模板字符串编译成Javascript 渲染函数的代码。运行时：用来创建 Vue实例、渲染并处理虚拟DOM等的代码。基本就是除去编译器的其它一切。 UMD:UMD版本可以通过 标签直接用在浏览器中。 jsDelivr CDN的https://cdn.jsdelivr.net/npm/vue 默认文件就是运行时 + 编译器的 UMD 版本（vue.js）。 CommonJS: CommonJS版本用来配合 老的打包工具比如 Browserify 或者 webpack1。这些打包工具的默认文件（pkg.main）是只包含运行时的CommonJS版本（vue.runtime.common.js）; ESModule: ES module版本用来配合现代打包工具 比如 webpack2 或Rollup。这些打包工具的默认文件（pkg.module）是只包含运行时的 ES Module 版本（vue.runtime.esm.js）; 运行时+编译器 vs.只包含运行时 若需要在客户端编译模板（比如传递一个字符串给template选项，或者挂载到一个元素上并以其DOM内部的HTM作为模板），就需要加上编译器，即： new Vue({ template:&quot;&lt;div&gt;{ {hi}}&lt;/div&gt;&quot; }); new Vue({ render(h){ return h(&apos;div&apos;,this.hi); } }); 当使用 vue-loader 或 vueify 的时候， *.vue 文件内部的模板会在构建时预编译成javascript。在最终打包好的包里实际上是不需要编译器的，所以只用运行时版本即可。 运行时版本相比于完整版本体积要小大约30%，所以应该尽可能使用这个版本，如果仍然希望使用完整版本，则需要在打包工具里配置一个别名。]]></content>
      <categories>
        <category>JavaScript</category>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue 实例]]></title>
    <url>%2F2018%2F01%2F09%2Fvue-shi-li%2F</url>
    <content type="text"><![CDATA[创建一个实例 每一个Vue 应用都是通过 Vue 函数创建一个新的Vue 实例开始的： var vm = new Vue({ //选项 }); Vue 虽然未完全按照MVVM 模型，但是Vue 的设计也受到了它的启发。因此文档中经常会使用vm（viewModel 的缩写）这个变量名表示Vue实例。当创建一个Vue 实例时，你可以传入一个选项对象。 一个Vue应用由一个通过 new Vue 创建的 根Vue实例， 以及可选的嵌套的、可复用的组件树组成。eg. 根实例–TodoList TodoItem DeleteTodoButtonEditTodoButton TodoListFooter ClearTodosButtonTodoListStatistics ⚠️ 所有的Vue组件都是Vue实例，并且接受相同的选项对象（一些根实例特有的选项除外） 数据与方法 当一个 Vue 实例被创建时，它向Vue的响应式系统中加入了其data对象中所能找到的所有的属性。当这些属性的值发生改时，视图将会产生响应，即匹配更新为新的值。 //数据对象 var data = { a: 1 }; //该对象加入到Vue实例之中 var vm = new Vue({ data: data }); //对象引用是否一致 vm.a === data.a; //true //设置属性也会影响到原始值 vm.a = 2; data.a // =&gt;2 //反之亦然 data.a = 3; vm.a // =&gt;3 当这些数据改变时，视图会重新进行渲染。 值得注意的是只有当实例被创建时 data 中存在的属性才是响应式的。也就是说新添加的属性，eg. vm.b = &apos;hi&apos;; 那么对 b 的改动将不会触发任何视图的更新。如果预知晚些时候会用到一个新的属性，但是一开始的时候它为空或者不存在，那么你仅需要设置一些初始值。 eg. data: { newTodoText:&apos;&apos;, visitCount: 0, hideCompletedTodos:false, todos:[], error:null } 这里唯一的例外是使用 Object.freeze(),这会组织修改现有属性，也意味着响应式系统无法再追踪变化。vue.js:3271 Uncaught TypeError: Cannot assign to read only property ‘init’ of object ‘#‘。除了数据属性，Vue 实例还暴露了一些有用的实力属性与方法。它们都有前缀 $,以便与用户定义的属性区分开来。 var exampleData = { a: 1 }; var example = new Vue({ el:&apos;.example&apos;, data: { a: 1 }, methods: { changeA: function () { this.a = 2; console.log(this.a); } } }); console.log(example.$data === exampleData); console.log(example.$el === document.getElementsByClassName(&apos;example&apos;)[0]); example.$watch(&apos;a&apos;,function (newVal,oldVal) { console.log(&apos;新值：&apos;+newVal); console.log(&apos;旧值：&apos;+oldVal); });实例生命周期钩子 每个Vue 实例在被创建时都要经过一系列的初始化过程–例如，需要设置数据监听、编译模板、将实例挂载到DOM 并在数据变化时更新DOM等。 同时在这个过程中也会运行一些叫做生命周期钩子 的函数，这给了用户在不同阶段添加自己的代码的机会。 比如created 钩子可以用来在一个实例被创建之后执行代码： new Vue({ data:{ a: 1 }, created: function(){ // &apos;this&apos; 指向 vm 实例 console.log(&apos;a is:&apos; + this.a); } }); 也有其它的一些生命周期钩子， 在实例生命周期的不同阶段被调用，如 mounted、updated 和 destoryed。 声明周期钩子 this 上下文指向调用它的 Vue实例。⚠️ 不要在选项属性或回调上使用箭头函数，比如created:() =&gt; console.log(this.a); 或 vm.$watch(‘a’,newValue =&gt; this.myMethod());。 因为箭头函数是和父级上下文绑定在一起的， this 不会是预估的Vue实例，经常会导致 Uncaught TypeError: Cannot read property of undefined 或 Uncaught TypeError: this.myMethod is not a function 之类的错误。 生命周期示意图st=&gt;start: new Vue( ) e=&gt;end: Destroyed opInitBeforeCreate=&gt;operation: Init（Events &amp; Lifecycle） opInitCreated=&gt;operation: Init ( Injections &amp; Reactivity) opX=&gt;operation: ? condHasEl=&gt;condition: Has &quot;el&quot; option ？ condHasTemp=&gt;condition: Has &quot;template&quot; option ？ opVm.$mountedCall=&gt;operation: vm.$mounted(el) st-&gt;opInitBeforeCreate-&gt;opInitCreated-&gt;condHasEl condHasEl(yes)-&gt;condHasTemp condHasEl(no)-&gt;opVm.$mountedCall-&gt;condHasTemp opCompileTempIntoRender=&gt;operation: Compile template into render function* opCompileElOutHtmlAsTemp=&gt;operation: Compile el&apos;s outerHTML as template* opCreateVm.$elAndReplace=&gt;operation: Create vm.$el and replace &apos;el&apos; with it opMounted=&gt;operation: Mounted condDataChange=&gt;condition: when data changes opUpdate=&gt;operation: Virtual DOM re-render and patch condDestroy=&gt;condition: when vm.$destroy() is called opDestroy=&gt;operation: Teardown watchers,child components and event listeners condHasTemp(yes)-&gt;opCompileTempIntoRender-&gt;opCreateVm.$elAndReplace condHasTemp(no)-&gt;opCompileElOutHtmlAsTemp-&gt;opCreateVm.$elAndReplace opCreateVm.$elAndReplace-&gt;opMounted-&gt;condDestroy condDestroy(yes)-&gt;opDestroy-&gt;e condDestroy(no)-&gt;condDataChange condDataChange(yes)-&gt;opUpdate condDataChange(no)-&gt;opMounted]]></content>
      <categories>
        <category>JavaScript</category>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vue.js 介绍]]></title>
    <url>%2F2018%2F01%2F09%2Fvuejs-jie-shao%2F</url>
    <content type="text"><![CDATA[vue.js是什么 Vue (读音 /vju:/, 类似view) 是一套用于构建用户界面的渐进式框架。与其他框架不同，vue 被设计为可以自底向上逐层应用。Vue的核心库只关心视图层，不仅易于上手，还便于与第三方库或既有项目整合。另一方面，当与现代化的工具链以及各种支持类库结合使用时，Vue 也完全能够为复杂的单页应用提供驱动。 起步 创建一个 .html 文件，然后通过如下方式引入 Vue： ](https://cdn.jsdelivr.net/npm/vue%22%3E) 声明式渲染vue.js 的核心是一个允许采用简洁的模板语法来声明式的将数据渲染进DOM系统： &lt;div id=&quot;app&quot;&gt; { {message}} &lt;/div&gt;]]></content>
      <categories>
        <category>JavaScript</category>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ztree异步加载otherParam中的参数无法传值]]></title>
    <url>%2F2017%2F12%2F29%2FZtree%E5%BC%82%E6%AD%A5%E5%8A%A0%E8%BD%BDotherParam%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0%E6%97%A0%E6%B3%95%E4%BC%A0%E5%80%BC%2F</url>
    <content type="text"><![CDATA[下面version是动态生成的,但是像下面那样写是取不到值的 1otherParam ：&#123;“version”，version&#125; 写成这样就可以取到了 1otherParam ：&#123;“version”，function()&#123;return version&#125;&#125;]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux进程管理]]></title>
    <url>%2F2017%2F12%2F28%2Flinux-e8-bf-9b-e7-a8-8b-e7-ae-a1-e7-90-86%2F</url>
    <content type="text"><![CDATA[一、进程的查看1、ps命令 具体可以通过ps –help查看，自带的讲解很全面； 2、top命令，与ps命令不同，ps只是输出瞬间运行的进程，top提供实时监控； 下图为top命令每一列的含义： 在top显示页面中还有一些快捷键可以使用，比如按字母P键表示按照CPU的使用率排序，按字母M键表示按照Memory的使用率排序，按字母N键表示以PID排序，按字母T键表示按照CPU使用时间排序，按字母K键则表示kill进程，按字母R键表示可以renice一个进程等。注意快捷键是区分大小写的。更多可用的方式可以按问号（?）键进入帮助模式。 二、进程的终止kill [信号代码] 进程ID；通过kill -l命令查看可用的信号代码： 常用的一般有三个，即HUP（1）、KILL（9）、TERM（15），分别代表重启、强行杀掉、正常结束。 例如通过 ps -ef查到一个进程的PID为2233， kill -1 2233 表示重启PID为2233进程的程序； kill -9 2233 表示强行停止，慎用； kill -15 2233 表示正常关闭，是默认的，可以不加同kill 2233效果相同。 killall 程序名 这样相对于直接kill PID的方式方便且安全，试想如果看错了PID结束了一个很重要的程序，后果不可估量； 三、查看进程打开的文件lsof 查看所有已经打开的文件，lsof –help可以查看具体有哪些命令； 四、进程优先级的调整top命令查看进程时有个NI列表示程序的nice优先级，数值越低表示优先级越高，取值范围是-20～19，如果启动时没有设置nice优先级则默认为0,用户可以给进程设定nice优先级，取值范围为0～19。在top命令中还有个PR列，表示进程的优先级； 进程的最终优先级 ＝ 优先级+nice优先级； nice 修改未启动进程的优先级，例如 nice -n -10 ./hello.sh，在运行hello.sh脚本的同时设置nice优先级； renice 修改运行中进程的优先级，例如 renice -10 -p 2233 表示修改PID为2233进程的nice优先级为-10。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux命令]]></title>
    <url>%2F2017%2F12%2F21%2Flinux-e5-91-bd-e4-bb-a4%2F</url>
    <content type="text"><![CDATA[tail -f 文件名 动态查看日志文件 for example:[root@localhost~]#tail -f /var/log/messages 查看系统日志文件 -p可以一次性创建dir3和dir4两个文件夹[root@localhost dir2]# mkdir -p dir3/dir4 rm删除文件夹remove directory的简写如果只是这个命令是不可以删除非空文件夹的，但是rm -r就可以删除，但是每次删除之前都会提醒你确认删除的文件；rm -rf删除非空文件夹并且没有提示 cp复制文件 在复制文件夹是需要加上-r cp * ../dir/ 备份当前文件夹下所有文件到dir，可以创建一个时间戳文件比较发生变化的文件 touch time_stamp 每次备份之后都更新一下这个时间戳，用于下次备份比较 文件权限：每组都是rwx的组合，如果拥有读权限，则该组的第一个字符显示r，否则显示一个小横线；如果拥有写权限，则该组的第二个字符显示w，否则显示一个小横线；如果拥有执行权限，则第三个字符显示x，否则显示一个小横线。 我们定义r=4，w=2，x=1，如果权限是rwx，则数字表示为7，如果权限是r-x，则数字表示为5。假设想设置一个文件的权限是：拥有者的权限是读、写、执行（rwx），拥有组的权限是读、执行（r-x），其他人的权限是只读（r–），那么可以使用命令chmod 754 somefile来设置。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[不同主板BIOS进入方法]]></title>
    <url>%2F2017%2F12%2F21%2F%E4%B8%8D%E5%90%8C%E4%B8%BB%E6%9D%BFBIOS%E8%BF%9B%E5%85%A5%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[AWARD BIOS：按【Delete】键；AMI BIOS：按【Delete】键； IBM BIOS：按【F2】键； MR BIOS：按【Esc】键； Quadtel BIOS：按【F2】键； AST BIOS：按【Ctrl+Alt+Esc】组合键； COMPAQ BIOS：待屏幕右上角出现光标时按【F10】键]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[两位数速算]]></title>
    <url>%2F2017%2F12%2F21%2F%E4%B8%A4%E4%BD%8D%E6%95%B0%E9%80%9F%E7%AE%97%2F</url>
    <content type="text"><![CDATA[1、十位相同，个位数和为10：例如74x76 这，7x(7+1)=56作为千位和百位，4x6=24作为十位和个位得出 74x76=5624; 2、十位和为10，个位相同：例如48x68 这，4x6+8=32作为千位和百位，8x8=64作为十位和个位得出 48x68=3264; 3、个位数是1的两位数相乘：例如61x41，60x40=2400，60+40=100，1x1=1，几个数字相加2400+100+1=2501； ………………方法很多不作赘述]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ IDEA中的main函数、for循环和System.out.println()快捷键]]></title>
    <url>%2F2017%2F12%2F20%2FIntelliJ%20IDEA%E4%B8%AD%E7%9A%84main%E5%87%BD%E6%95%B0%E3%80%81for%E5%BE%AA%E7%8E%AF%E5%92%8CSystem.out.println()%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[1、在Eclipse中输入main然后Alt+/就可以补全main函数。IntelliJ IDEA中是输入psvm； 2、for循环，在IntelliJ IDEA中是输入fori； 3、在IntelliJ IDEA中是输入sout就是System.out.println();]]></content>
      <categories>
        <category>IDE</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[@RequestBody和@RequestParam区别]]></title>
    <url>%2F2017%2F12%2F15%2Frequestbody-e5-92-8crequestparam-e5-8c-ba-e5-88-ab%2F</url>
    <content type="text"><![CDATA[@RequestParam用来处理Content-Type: 为 application/x-www-form-urlencoded编码的内容。（Http协议中，如果不指定Content-Type，则默认传递的参数就是application/x-www-form-urlencoded类型） RequestParam可以接受简单类型的属性，也可以接受对象类型。 实质是将Request.getParameter() 中的Key-Value参数Map利用Spring的转化机制ConversionService配置，转化成参数接收对象或字段。 tip在Content-Type: application/x-www-form-urlencoded的请求中， get 方式中queryString的值，和post方式中 body data的值都会被Servlet接受到并转化到Request.getParameter()参数集中，所以@RequestParam可以获取的到。 @RequestBody处理HttpEntity传递过来的数据，一般用来处理非Content-Type: application/x-www-form-urlencoded编码格式的数据。 GET请求中，因为没有HttpEntity，所以@RequestBody并不适用。 POST请求中，通过HttpEntity传递的参数，必须要在请求头中声明数据的类型Content-Type，SpringMVC通过使用HandlerAdapter 配置的HttpMessageConverters来解析HttpEntity中的数据，然后绑定到相应的bean上。 总结 在GET请求中，不能使用@RequestBody。 在POST请求，可以使用@RequestBody和@RequestParam，但是如果使用@RequestBody，对于参数转化的配置必须统一。 举个例子，在SpringMVC配置了HttpMessageConverters处理栈中，指定json转化的格式，如Date转成‘yyyy-MM-dd’,则参数接收对象包含的字段如果是Date类型，就只能让客户端传递年月日的格式，不能传时分秒。因为不同的接口，它的参数可能对时间参数有不同的格式要求，所以这样做会让客户端调用同事对参数的格式有点困惑，所以说扩展性不高。 如果使用@RequestParam来接受参数，可以在接受参数的model中设置@DateFormat指定所需要接受时间参数的格式。 另外，使用@RequestBody接受的参数是不会被Servlet转化统一放在request对象的Param参数集中，@RequestParam是可以的。 综上所述，一般情况下，推荐使用@RequestParam注解来接受Http请求参数。]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL的语句执行顺序]]></title>
    <url>%2F2017%2F12%2F15%2Fmysql-e7-9a-84-e8-af-ad-e5-8f-a5-e6-89-a7-e8-a1-8c-e9-a1-ba-e5-ba-8f%2F</url>
    <content type="text"><![CDATA[(8)SELECT (9)DISTINCT (11)&lt;TOP_specification&gt; &lt;select_list&gt; (1)FROM (3) &lt;join_type&gt; JOIN &lt;right_table&gt; (2) ON (4)WHERE (5)GROUP BY &lt;group_by_list&gt; (6)WITH {CUBE | ROLLUP} (7)HAVING (10)ORDER BY &lt;order_by_list&gt; (11)LIMIT 查询处理的每一个阶段 FORM: 对FROM的左边的表和右边的表计算笛卡尔积。产生虚表VT1 ON: 对虚表VT1进行ON筛选，只有那些符合的行才会被记录在虚表VT2中。 JOIN： 如果指定了OUTER JOIN（比如left join、 right join），那么保留表中未匹配的行就会作为外部行添加到虚拟表VT2中，产生虚拟表VT3, 如果from子句中包含两个以上的表的话，那么就会对上一个join连接产生的结果VT3和下一个表重复执行步骤1~3这三个步骤，一直到处理完所有的表为止。 WHERE： 对虚拟表VT3进行WHERE条件过滤。只有符合的记录才会被插入到虚拟表VT4中。 GROUP BY: 根据group by子句中的列，对VT4中的记录进行分组操作，产生VT5. CUBE | ROLLUP: 对表VT5进行cube或者rollup操作，产生表VT6. HAVING： 对虚拟表VT6应用having过滤，只有符合的记录才会被 插入到虚拟表VT7中。 SELECT： 执行select操作，选择指定的列，插入到虚拟表VT8中。 DISTINCT： 对VT8中的记录进行去重。产生虚拟表VT9. ORDER BY: 将虚拟表VT9中的记录按照&lt;order_by_list&gt;进行排序操作，产生虚拟表VT10. LIMIT：取出指定行的记录，产生虚拟表VT11, 并将结果返回]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sql优化]]></title>
    <url>%2F2017%2F12%2F15%2Fsql-e4-bc-98-e5-8c-96%2F</url>
    <content type="text"><![CDATA[1. 越小的列会越快 对于大多数的数据库引擎来说，硬盘操作可能是最重大的瓶颈。所以，把你的数据变得紧凑会对这种情况非常有帮助，因为这减少了对硬盘的访问。 参看 MySQL 的文档 Storage Requirements 查看所有的数据类型。 如果一个表只会有几列罢了(比如说字典表，配置表)，那么，我们就没有理由使用 INT 来做主键，使用 MEDIUMINT, SMALLINT 或是更小的 TINYINT 会更经济一些。如果你不需要记录时间，使用 DATE 要比 DATETIME 好得多。 当然，你也需要留够足够的扩展空间，不然，你日后来干这个事，你会死的很难看，参看Slashdot的例子(2009年11月06日)，一个简单的ALTER TABLE语句花了3个多小时，因为里面有一千六百万条数据。 2. 垂直分割 “垂直分割”是一种把数据库中的表按列变成几张表的方法，这样可以降低表的复杂度和字段的数目，从而达到优化的目的。(以前，在银行做过项目，见过一张表有100多个字段，很恐怖) 示例一：在Users表中有一个字段是家庭地址，这个字段是可选字段，相比起，而且你在数据库操作的时候除了个人信息外，你并不需要经常读取或是改 写这个字段。那么，为什么不把他放到另外一张表中呢? 这样会让你的表有更好的性能，大家想想是不是，大量的时候，我对于用户表来说，只有用户ID，用户名，口令，用户角色等会被经常使用。小一点的表总是会有 好的性能。 示例二： 你有一个叫 “last_login” 的字段，它会在每次用户登录时被更新。但是，每次更新时会导致该表的查询缓存被清空。所以，你可以把这个字段放到另一个表中，这样就不会影响你对用户 ID，用户名，用户角色的不停地读取了，因为查询缓存会帮你增加很多性能。 另外，你需要注意的是，这些被分出去的字段所形成的表，你不会经常性地去Join他们，不然的话，这样的性能会比不分割时还要差，而且，会是极数级的下降。 3. 把IP地址存成 UNSIGNED INT 很多程序员都会创建一个 VARCHAR(15) 字段来存放字符串形式的IP而不是整形的IP。如果你用整形来存放，只需要4个字节，并且你可以有定长的字段。而且，这会为你带来查询上的优势，尤其是当 你需要使用这样的WHERE条件：IP between ip1 and ip2。 我们必需要使用UNSIGNED INT，因为 IP地址会使用整个32位的无符号整形。 而你的查询，你可以使用 INET_ATON() 来把一个字符串IP转成一个整形，并使用 INET_NTOA() 把一个整形转成一个字符串IP。在PHP中，也有这样的函数 ip2long() 和 long2ip()。 4. 尽可能的使用 NOT NULL 除非你有一个很特别的原因去使用 NULL 值，你应该总是让你的字段保持 NOT NULL。这看起来好像有点争议，请往下看。 首先，问问你自己“Empty”和“NULL”有多大的区别(如果是INT，那就是0和NULL)?如果你觉得它们之间没有什么区别，那么你就不要使用NULL。(你知道吗?在 Oracle 里，NULL 和 Empty 的字符串是一样的!) 不要以为 NULL 不需要空间，其需要额外的空间，并且，在你进行比较的时候，你的程序会更复杂。 当然，这里并不是说你就不能使用NULL了，现实情况是很复杂的，依然会有些情况下，你需要使用NULL值。 5. 使用 ENUM 而不是 VARCHAR ENUM 类型是非常快和紧凑的。在实际上，其保存的是 TINYINT，但其外表上显示为字符串。这样一来，用这个字段来做一些选项列表变得相当的完美。 如果你有一个字段，比如“性别”，“国家”，“民族”，“状态”或“部门”，你知道这些字段的取值是有限而且固定的，那么，你应该使用 ENUM 而不是 VARCHAR。 MySQL也有一个“建议”(见第十条)告诉你怎么去重新组织你的表结构。当你有一个 VARCHAR 字段时，这个建议会告诉你把其改成 ENUM 类型。使用 PROCEDURE ANALYSE() 你可以得到相关的建议。 避免 SELECT * 从数据库里读出越多的数据，那么查询就会变得越慢。并且，如果你的数据库服务器和WEB服务器是两台独立的服务器的话，这还会增加网络传输的负载。 所以，你应该养成一个需要什么就取什么的好的习惯。 6. 当只要一行数据时使用 LIMIT 1 当你查询表的有些时候，你已经知道结果只会有一条结果，但因为你可能需要去fetch游标，或是你也许会去检查返回的记录数。 在这种情况下，加上 LIMIT 1 可以增加性能。这样一样，MySQL数据库引擎会在找到一条数据后停止搜索，而不是继续往后查少下一条符合记录的数据。 下面的示例，只是为了找一下是否有“中国”的用户，很明显，后面的会比前面的更有效率。(请注意，第一条中是Select ，第二条是Select 1) 7. 为查询缓存优化你的查询 大多数的MySQL服务器都开启了查询缓存。这是提高性最有效的方法之一，而且这是被MySQL的数据库引擎处理的。当有很多相同的查询被执行了多次的时候，这些查询结果会被放到一个缓存中，这样，后续的相同的查询就不用操作表而直接访问缓存结果了。 这里最主要的问题是，对于程序员来说，这个事情是很容易被忽略的。因为，我们某些查询语句会让MySQL不使用缓存。请看下面的示例： MySQL性能优化的21个最佳实践 和 mysql使用索引 上面两条SQL语句的差别就是 CURDATE() ，MySQL的查询缓存对这个函数不起作用。所以，像 NOW() 和 RAND() 或是其它的诸如此类的SQL函数都不会开启查询缓存，因为这些函数的返回是会不定的易变的。所以，你所需要的就是用一个变量来代替MySQL的函数，从而 开启缓存。 8. 任何情况下SELECT COUNT() FROM tablename是最优选择 如果你的数据表没有主键，那么count(1)比count()快 如果有主键的话，那主键（联合主键）作为count的条件也比count()要快 如果你的表只有一个字段的话那count()就是最快的啦 count() count(1) 两者比较。主要还是要count(1)所相对应的数据字段。 如果count(1)是聚索引,id,那肯定是count(1)快。但是差的很小的。 因为count(),自动会优化指定到那一个字段。所以没必要去count(?)，用count(),sql会帮你完成优化的 9. 并不是所有索引对查询都有效 SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[memcached高性能的分布式内存对象缓存系统]]></title>
    <url>%2F2017%2F12%2F15%2Fmemcached%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%86%85%E5%AD%98%E5%AF%B9%E8%B1%A1%E7%BC%93%E5%AD%98%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[Memcached 是一个高性能的分布式内存对象缓存系统，用于动态Web应用以减轻数据库负载。它通过在内存中缓存数据和对象来减少读取数据库的次数，从而提高动态、数据库驱动网站的速度。Memcached基于一个存储键/值对的hashmap。其守护进程（daemon ）是用C写的，但是客户端可以用任何语言来编写，并通过memcached协议与守护进程通信。 安装 1.依赖包安装libevent 从官网下载最新的http://libevent.org/ 解压 tar zxvf libevent-2.1.8-stable.tar.gz 编译 ./configure –prefix=/usr/libevent 安装 make &amp;&amp; make install 将lib包放进标准lib中 ln -s /usr/libevent/lib/* /usr/lib/ 2.安装memcached 从官网下载最新的http://memcached.org/ 解压 tar zxvf memcached-1.4.36.tar.gz 编译 ./configure –with-libevent=/usr/libevent 安装 make &amp;&amp; make install 使用 关于memcache启动的一些参数说明： memcached命令参数解释: -p 监听的端口 -l 连接的IP地址, 默认是本机 -d start 启动memcached 服务 -d restart 重起memcached 服务 -d stop|shutdown 关闭正在运行的memcached 服务 -d install 安装memcached 服务 -d uninstall 卸载memcached 服务 -u 以的身份运行 (仅在以root运行的时候有效) -m 最大内存使用，单位MB。默认64MB -M 内存耗尽时返回错误，而不是删除项 -c 最大同时连接数，默认是1024 -f 块大小增长因子，默认是1.25 -n 最小分配空间，key+value+flags默认是48 -h 显示帮助 1.启动 /usr/local/bin/memcached -d start -m 1024 -u root -p 11211 -P /opt/memcached.pid]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[emmet使用手册]]></title>
    <url>%2F2017%2F12%2F13%2Femmet%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[截取自emmet官网：https://docs.emmet.io/abbreviations/syntax/详情可去[官网](https://docs.emmet.io/abbreviations/syntax/)查看 Child: &gt;You can use 1div&gt;ul&gt;li …will produce 12345&lt;div&gt; &lt;ul&gt; &lt;li&gt;li&gt; ul&gt;div&gt; Sibling: +Use 1div+p+bq …will output 123&lt;div&gt;div&gt;&lt;p&gt;p&gt;&lt;blockquote&gt;blockquote&gt; Climb-up: ^With 1div+div&gt;p&gt;span+em …will be expanded to 1234&lt;div&gt;div&gt;&lt;div&gt; &lt;p&gt;&lt;span&gt;span&gt;&lt;em&gt;em&gt;p&gt;div&gt; With 1div+div&gt;p&gt;span+em^bq …outputs to 12345&lt;div&gt;div&gt;&lt;div&gt; &lt;p&gt;&lt;span&gt;span&gt;&lt;em&gt;em&gt;p&gt; &lt;blockquote&gt;blockquote&gt;div&gt; You can use as many 1div+div&gt;p&gt;span+em^^^bq …will output to 12345&lt;div&gt;div&gt;&lt;div&gt; &lt;p&gt;&lt;span&gt;span&gt;&lt;em&gt;em&gt;p&gt;div&gt;&lt;blockquote&gt;blockquote&gt; Multiplication: *With 1ul&gt;li*5 …outputs to 1234567&lt;ul&gt; &lt;li&gt;li&gt; &lt;li&gt;li&gt; &lt;li&gt;li&gt; &lt;li&gt;li&gt; &lt;li&gt;li&gt;ul&gt; Grouping: ()Parenthesises are used by Emmets’ power users for grouping subtrees in complex abbreviations: 1div&gt;(header&gt;ul&gt;li*2&gt;a)+footer&gt;p …expands to 1234567891011&lt;div&gt; &lt;header&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;&quot;&gt;a&gt;li&gt; &lt;li&gt;&lt;a href=&quot;&quot;&gt;a&gt;li&gt; ul&gt; header&gt; &lt;footer&gt; &lt;p&gt;p&gt; footer&gt;div&gt; If you’re working with browser’s DOM, you may think of groups as Document Fragments: each group contains abbreviation subtree and all the following elements are inserted at the same level as the first element of group. You can nest groups inside each other and combine them with multiplication * operator: 1(div&gt;dl&gt;(dt+dd)*3)+footer&gt;p …produces 12345678910111213&lt;div&gt; &lt;dl&gt; &lt;dt&gt;dt&gt; &lt;dd&gt;dd&gt; &lt;dt&gt;dt&gt; &lt;dd&gt;dd&gt; &lt;dt&gt;dt&gt; &lt;dd&gt;dd&gt; dl&gt;div&gt;&lt;footer&gt; &lt;p&gt;p&gt;footer&gt; With groups, you can literally write full page mark-up with a single abbreviation, but please don’t do that. Attribute operatorsAttribute operators are used to modify attributes of outputted elements. For example, in HTML and XML you can quickly add ID and CLASSIn CSS, you use 1div#header+div.page+div#footer.class1.class2.class3 …will output 123&lt;div id=&quot;header&quot;&gt;div&gt;&lt;div class=&quot;page&quot;&gt;div&gt;&lt;div id=&quot;footer&quot; class=&quot;class1 class2 class3&quot;&gt;div&gt; Custom attributesYou can use 1td[title=&quot;Hello world!&quot; colspan=3] …outputs 1&lt;td title=&quot;Hello world!&quot; colspan=&quot;3&quot;&gt;td&gt; You can place as many attributes as you like inside square brackets. You don’t have to specify attribute values: td[colspan title] will produce ``with tabstops inside each empty attribute (if your editor supports them). You can use single or double quotes for quoting attribute values. You don’t need to quote values if they don’t contain spaces: td[title=hello colspan=3] will work. Item numbering: $With multiplication 1ul&gt;li.item$*5 …outputs to 1234567&lt;ul&gt; &lt;li class=&quot;item1&quot;&gt;li&gt; &lt;li class=&quot;item2&quot;&gt;li&gt; &lt;li class=&quot;item3&quot;&gt;li&gt; &lt;li class=&quot;item4&quot;&gt;li&gt; &lt;li class=&quot;item5&quot;&gt;li&gt;ul&gt; You can use multiple 1ul&gt;li.item$$$*5 …outputs to 1234567&lt;ul&gt; &lt;li class=&quot;item001&quot;&gt;li&gt; &lt;li class=&quot;item002&quot;&gt;li&gt; &lt;li class=&quot;item003&quot;&gt;li&gt; &lt;li class=&quot;item004&quot;&gt;li&gt; &lt;li class=&quot;item005&quot;&gt;li&gt;ul&gt; Changing numbering base and directionWith For example, to change direction, add @- after $: 1ul&gt;li.item$@-*5 …outputs to 1234567&lt;ul&gt; &lt;li class=&quot;item5&quot;&gt;li&gt; &lt;li class=&quot;item4&quot;&gt;li&gt; &lt;li class=&quot;item3&quot;&gt;li&gt; &lt;li class=&quot;item2&quot;&gt;li&gt; &lt;li class=&quot;item1&quot;&gt;li&gt;ul&gt; To change counter base value, add @N modifier to $: 1ul&gt;li.item$@3*5 …transforms to 1234567&lt;ul&gt; &lt;li class=&quot;item3&quot;&gt;li&gt; &lt;li class=&quot;item4&quot;&gt;li&gt; &lt;li class=&quot;item5&quot;&gt;li&gt; &lt;li class=&quot;item6&quot;&gt;li&gt; &lt;li class=&quot;item7&quot;&gt;li&gt;ul&gt; You can use these modifiers together: 1ul&gt;li.item$@-3*5 …is transformed to 1234567&lt;ul&gt; &lt;li class=&quot;item7&quot;&gt;li&gt; &lt;li class=&quot;item6&quot;&gt;li&gt; &lt;li class=&quot;item5&quot;&gt;li&gt; &lt;li class=&quot;item4&quot;&gt;li&gt; &lt;li class=&quot;item3&quot;&gt;li&gt;ul&gt; Text: {}You can use curly braces to add text to element: 1a&#123;Click me&#125; …will produce 1&lt;a href=&quot;&quot;&gt;Click mea&gt; Note that ,petc.) but has a special meaning when written right after element. For example,a{click}anda&gt;{click}will produce the same output, buta{click}+b{here}anda&gt;{click}+b{here}won’t: element is placed inside element. And that’s the difference: when {text} is written right after element, it doesn’t change parent context. Here’s more complex example showing why it is important:p&gt;{Click }+a{here}+{ to continue} …producesClick herea&gt; to continuep&gt; In this example, to write Click here to continue inside element we have explicitly move down the tree with &gt; operator after p, but in case of aelement we don’t have to, since we need element with here word only, without changing parent context.For comparison, here’s the same abbreviation written without child &gt; operator:p{Click }+a{here}+{ to continue} …producesClick p&gt; herea&gt; to continue Notes on abbreviation formattingWhen you get familiar with Emmet’s abbreviations syntax, you may want to use some formatting to make your abbreviations more readable. For example, use spaces between elements and operators, like this:(header &gt; ul.nav &gt; li*5) + footer But it won’t work, because space is a stop symbol where Emmet stops abbreviation parsing.]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2017%2F12%2F12%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[. 这个英文句号“.”匹配不包括换行符的一个字符，例如a.b可以匹配abc、acc、a2c、a c,空格也可以匹配； * 可以匹配任意数量的字符，例如ab可以匹配abc、abcd、abdkxkdk，只要是ab开头的后面不论多少字符都可以匹配，经常有“.”表示匹配非换行符任意长度的字符； \{n,m\} 跟*不同的是这个可以控制字符出现的次数，n,m是要输入的数字， 例如 grep ‘ab\{2\}c’ file1 这句查询中file1是文件，文件中如果有abbc这样的字段就会被匹配出来，因为b出现了两次； 例如 grep ‘ab\{2,\}c’ file1 这句查询中会匹配file1文件中字符b出现最少两次的所有字段，abbc、abbbbc、abbbbbc 这样的b出现了最少两次的字段都会匹配出来； 例如 grep ‘ab\{2,4\}c’ file1 这句查询中会匹配file1文件中字符b出现2～4次的所有字段，abbc、abbbc、abbbbc 这样的b出现了2～4次的字段都会被匹配出来； ^ 用来匹配以什么开头的行，例如 grep ‘^ab’ file1 用来匹配开头为ab字符的行； $ 用来匹配以什么结尾的行，例如 grep ‘abc$’ file1 用来匹配结尾为abc字符的行； ^$连用的话表示查询的该行为空，因为他们开头到结尾中间什么都没有； [ ] 方括号用于匹配方括号中出现的任一字符，例如 grep ‘[a]‘ file1 表示查询file1中出现过a字符的所有行；[A-Z]用于查询所有大写字母;[A-Za-z]用于查询所有大小写字母；在方括号中^符号表示取反；[^A-D]用于查询非大写字母ABCD中的所有字母； 根据以上学到的正则知识就可以写一个匹配手机号的正则表达式，首先分析一下，手机号十一位其中第一位一定是1第二位可能会是3、4、5、7、8都有可能，后面九位数随机排列所以写出如下正则表达式：”^1[3,4,5,7,8][0-9]\{9\}”； \ 反斜杠表示转义字符； \&lt;word\&gt; 表示精确匹配，例如\&lt;hello\&gt;就只会匹配hello字段；]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据库加索引解决慢查询问题]]></title>
    <url>%2F2017%2F12%2F12%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8A%A0%E7%B4%A2%E5%BC%95%E8%A7%A3%E5%86%B3%E6%85%A2%E6%9F%A5%E8%AF%A2%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[待续http://mp.weixin.qq.com/s/A5z937EXmZoSGt1EJtd2PA]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[vi编辑器的使用]]></title>
    <url>%2F2017%2F12%2F11%2Fvi-e7-bc-96-e8-be-91-e5-99-a8-e5-bf-ab-e6-8d-b7-e9-94-ae-e5-be-85-e7-bb-ad%2F</url>
    <content type="text"><![CDATA[先来张图感受一下 没找到上面这张图前自己的记录 vi newfile 打开文件名为newfile的文件；如果没有这个文件会直接创建一个名为newfile的文件； :f或者:file 查看正在编辑的文件信息，如果是多文件编辑会显示是编辑的第几个文件； yy 连续按两次y键是复制； nyy 复制包含光标所在行往下的n行，例如复制光标以下的三行 3yy； p 粘贴； dd 连续按两次d键是删除整行； ndd 删除包含光标所在行往下的n行，例如删除光标以下的三行 3dd； dw 组合键是删除单词； d$ 删除光标到文末的所有字符； x 往后删除一个字符； X 向前删除一个字符； :%d 全部删除 u 键是撤销； i insert插入进入编辑模式，在当前光标位置； I 进入编辑模式，在光标位置当前行的第一个飞空字符出编辑； o 当前光标下一行插入新行编辑； O 当前光标上一行插入新行编辑； a 在当前光标后一个位置进行编辑； A 在当前光标所在行的最后进行编辑； h 光标左移动； j 光标下移； k 光标下移； l 光标右移； 0 移动到本行的开头； $ 移动到本行的末尾； G 移动到整个文件的末尾； :n然后回车键 移动到某一行，n为数字行数，例如要移动到第五行就输入 :5+Enter； n然后回车键 往下移动多少行，n为数字，例如往下移动两行 2+Enter； Ctrl+f 往下移动一页； Ctrl+b 往上移动一页； Ctrl+d 往下移动半页； Ctrl+u 往上移动半页； /关键词 查找文中的关键词，并把光标停留在关键词位置，如果有多个关键词，优先查找光标往下位置最接近光标的一个；按n键查找下一个，N查找上一个； ？关键词 与/关键词功能完全相反，查找光标往上位置最接近光标的一个；按n键查找上一个，N查找下一个； 有时候我们编辑一个文件需要将某个词替换掉，或者整个文章中出现的某个词替换。 :n1,n2s/关键词1/关键词2/g 将n1到n2行中的所有关键词1替换成关键词2； :1,$s/关键词1/关键词2/g 将文中从第一行开始所有的关键词1替换成关键词2； :s/关键词1/关键词2/g 将光标所在行中所有关键词1换成关键词2； :s/关键词1/关键词2 将光标所在行第一次出现的关键词1替换成关键词2； 虽然有nyy,ndd这样的快捷键，但是心里想的并没有眼睛看的来的直接： v 按v键会出现“–VISUAL–”字样，这时候进入行编辑，上下左右调整选中的字段进行操作； Ctrl+v 按Ctrl+v会出现“–VISUAL BLOCK–”字样这时候进入的是列编辑，然后上下左右键操作光标选中多列进行操作； V 大写V会出现“–VISUAL LINE–”字样，这样会选中当前光标所在行，每次都选中光标所在的一整行； 多文件编辑，就像你在Windows系统中一样，可以打开多个txt文本文件进行修改，互相复制粘贴数据。 vi file1 file2 file3 同时打开file1、file2、file3三个文件，如果没有这的文件，通过vi命令也会自动创建文件，默认会进入file1文件中进行编辑； :n 此处n是next下一个文件，在多文件编辑中，会切换到下一个文件，我们这里就会切换到file2文件中，在输入:n会切换到file3文件中； :N 在多文件编辑中，会切换上一个文件，如果你在file3文件中编辑，输入:N会切换回file2文件中，在输入:N会切换到file1中； 如果是在vim编辑器中，多文件编辑时输入:files可以查看所有正在编辑的文件，vi编辑器输入:f查看正在编辑的文件信息； vim编辑器不做赘述，操作跟vi编辑器基本相同； vimtutor 命令查看vim编辑器的说明文档，是最好的学习手册。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[vi编辑器的三种模式]]></title>
    <url>%2F2017%2F12%2F08%2Fvi%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[vi编辑器的三种模式 一般模式 使用vi打开某个文件的时候默认进入的模式就是一般模式。在这种模式中最基础的功能就是“移动光标”——使用上下左右键来移动光标块。还可使用按键组合的方式来执行复制、粘贴、删除的功能。 编辑模式 在一般模式中，按i键可以进入编辑模式（这是最简单的进入方式，底部会出现“–INSERT–”字样，还有其他的进入方式后面介绍）。在编辑模式中，依然可以使用上下左右键来移动光标，同时还可以输入文字到文件中。从编辑模式回到一般模式需要按Esc键。 末行指令模式 在一般模式中，按冒号键（：）或斜杠键（/）或问号键（?）就会在当前视图的最后一行出现相应的符号，这就代表进入了相应的末行指令模式。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[阿里Java开发手册摘录]]></title>
    <url>%2F2017%2F12%2F07%2F%E9%98%BF%E9%87%8CJava%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C%E6%91%98%E5%BD%95%2F</url>
    <content type="text"><![CDATA[循环体内，字符串的连接方式，使用 StringBuilder 的 append 方法进行扩展。 说明：反编译出的字节码文件显示每次循环都会 new 出一个 StringBuilder 对象，然后进行 append 操作，最后通过 toString 方法返回 String 对象，造成内存资源浪费。 反例： String str = “start”; for (int i = 0; i &lt; 100; i++) { str = str + “hello”; }]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ios微信扫描二维码跳转App Store]]></title>
    <url>%2F2017%2F12%2F05%2Fios%E5%BE%AE%E4%BF%A1%E6%89%AB%E6%8F%8F%E4%BA%8C%E7%BB%B4%E7%A0%81%E8%B7%B3%E8%BD%ACApp%20Store%2F</url>
    <content type="text"><![CDATA[需求：做一个下载页，用扫一扫进入页面，根据设备判断是Android端还是iOS端，显示不同的下载按钮。这个下载页面的链接地址是 “http://a.website.com/downclient/download.html”。 遇到的问题：按钮是a标签，起初自己的Android机器测试如丝般顺滑，微信浏览器扫一扫可以点击按钮跳转浏览器执行下载app，没有任何问题。发布项目之后，同事的苹果手机同样扫码，点击立即安装按钮不跳转，一片死寂···，没有任何反应。 原因：这个生成二维码的下载页面链接有问题·····域名解析“a”换成了“www”就解决了·····；原因不明，不想细究。 代码如下： 下载页 #weixin-tip{display:none; position: fixed; left:0; top:0; background: rgba(0,0,0,0.8); filter:alpha(opacity=80); width: 100%; height:100%; z-index: 100;} #weixin-tip p{text-align: center; margin-top: 15%; padding:0 5%; position: relative; font-size:16px;color: orange;} &nbsp; &nbsp; 请点击右上角选择在浏览器中打开本页面 var pla=ismobile();//0为ios，1为android或者其他 function ismobile(test){ var u = navigator.userAgent, app = navigator.appVersion; if(/AppleWebKit.*Mobile/i.test(navigator.userAgent) || (/MIDP|SymbianOS|NOKIA|SAMSUNG|LG|NEC|TCL|Alcatel|BIRD|DBTEL|Dopod|PHILIPS|HAIER|LENOVO|MOT-|Nokia|SonyEricsson|SIE-|Amoi|ZTE/.test(navigator.userAgent))){ if(window.location.href.indexOf("?mobile") -1){ return '0'; }else{ return '1'; } }; if(pla=="0"){ document.getElementById('btn-android').style.display="none"; }else{ document.getElementById('btn-ios').style.display="none"; } //监测微信下载 var is_weixin = (function() { var ua = navigator.userAgent.toLowerCase(); if (ua.match(/MicroMessenger/i) == “micromessenger”) { return true; } else { return false; } })(); function isWeixin(){ if(is_weixin&amp;&amp;pla!=&quot;0&quot;) { //ios微信会弹出前往appStore,不需要此提示 var winHeight = typeof window.innerHeight != ‘undefined’ ? window.innerHeight : document.documentElement.clientHeight; var tip = document.getElementById(‘weixin-tip’); tip.style.height = winHeight + ‘px’; tip.style.display = ‘block’; } } //点击蒙层关闭 var close = document.getElementById(‘weixin-tip’); close.onclick = function(){ var tip = document.getElementById(‘weixin-tip’); tip.style.display = ‘none’; } 起初将ios需要的链接直接生成二维码给ios设备扫描“https://itunes.apple.com/cn/app/id128****846”；会直接弹打开“App Store”；证明链接没有任何问题。 网上也有说是onclick方法不能执行，还是试试吧。删除按钮中的onclick方法，引入jQuery写了个 $(‘body’).on(‘click’,’#btn-android,#btn-ios’,isWeixin)； 测试的时候发现那个大兄弟可能跟我遇到的不是一个问题。 还有种说法是要用腾讯应用宝的外链才可以跳转，应该是针对直接点击链接跳转，与我遇到的问题应该是不同的。 后来我在我本地用自己的域名层层虚拟服务器转发解析到我工作的主机上，”http://www.doeat.cn:82/downclient/download.html&quot;,用这个链接生成了二维码，让同事IOS来扫一扫，结果神奇的可以跳转App Store······；一下子有点懵······不知如何是好。 经过反复的对比唯一的差别那只有域名了额····· 还好“www”没有解析到别处服务器。用 “http://www.website.com/downclient/download.html”生成了新的二维码给同事扫描，问题解决。 非常无奈，因为这种问题耽误了大把时间·····]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Unexpected end of JSON input]]></title>
    <url>%2F2017%2F12%2F01%2Funexpected-end-of-json-input%2F</url>
    <content type="text"><![CDATA[因为接口返回的参数不是JSON对象，有的看似JSON对象的，但是用引号引起来的，是字符串。例如：data=”{name:”day”,sex:”male”}”； 我们需要将这个字符串外部包裹的双引号去掉转为JSON对象，data.JSON.parse() ={name:”day”,sex:”male”};这样就转为JSON对象了。 与JSON.parse相反的一个方法是JSON.stringify，用法是将JSON对象转为字符串。]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java程序员们最常犯的10个错误]]></title>
    <url>%2F2017%2F12%2F01%2FJava%E7%A8%8B%E5%BA%8F%E5%91%98%E4%BB%AC%E6%9C%80%E5%B8%B8%E7%8A%AF%E7%9A%8410%E4%B8%AA%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[1.将数组转化为列表将数组转化为一个列表时，程序员们经常这样做： List&lt;String&gt; list = Arrays.asList(arr);Arrays.asList()会返回一个ArrayList对象，ArrayList类是Arrays的一个私有静态类，而不是java.util.ArrayList类，java.util.Arrays.ArrayList类有set()、get()、contains()方法，但是没有增加元素的方法，所以它的大小是固定的，想要创建一个真正的ArrayList类，你应该这样做: ArrayList&lt;String&gt; arrayList = new ArrayList&lt;String&gt;(Arrays.asList(arr));ArrayList的构造方法可以接受一个集合类型，刚好它也是java.util.Arrays.ArrayList的超类。 2.判断一个数组是否包含一个值程序员们经常这样做: Set&lt;String&gt; set = new HashSet&lt;String&gt;(Arrays.asList(arr)); return set.contains(targetValue);这段代码起作用，但是没有必要把一个数组转化成列表，转化为列表需要额外的时间。它可以像下面那样简单: Arrays.asList(arr).contains(targetValue);或者是: for(String s:arr){ if(s.equals(targetValue)){ return true; } } return false;第一种方法比第二种更容易读 3.在一个循环中删除一个列表中的元素思考下面这一段在循环中删除多个元素的的代码 ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(Arrays.asList(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;)); for(int i=0;i&lt;list.size();i++){ list.remove(i); } System.out.println(list);输出结果是: [b,d]在这个方法中有一个严重的错误。当一个元素被删除时，列表的大小缩小并且下标变化，所以当你想要在一个循环中用下标删除多个元素的时候，它并不会正常的生效。 你也许知道在循环中正确的删除多个元素的方法是使用迭代，并且你知道java中的foreach循环看起来像一个迭代器，但实际上并不是。考虑一下下面的代码: ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(Arrays.asList(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;)); for(String s:list){ if(s.equals(&quot;a&quot;)){ list.remove(s); } }它会抛出一个ConcurrentModificationException异常。 相反下面的显示正常： ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(Arrays.asList(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;)); Iterator&lt;String&gt; iter = list.iterator(); while(iter.hasNext()){ String s = iter.next(); if(s.equals(&quot;a&quot;)){ iter.remove(); } }.next()必须在.remove()之前调用。在一个foreach循环中，编译器会使.next()在删除元素之后被调用，因此就会抛出ConcurrentModificationException异常，你也许希望看一下ArrayList.iterator()的源代码。 4.Hashtable与HashMap的对比就算法而言，哈希表是数据结构的一个名字。但是在java中，这个数据结构的名字是HashMap。Hashtable与HashMap的一个重要不同点是Hashtable是同步的。所以你经常不需要Hashtable,相反HashMap经常会用到。 5.在集合中使用原始类型在Java中原始类型与无界通配符类型很容易混合在一起，拿Set来说，Set是一个原始类型，而Set&lt;?&gt;是无界的通配符类型。 考虑下面使用原始类型List作为参数的代码: public static void add(List list,Object o){ list.add(o); } pulbic static void main(String[] args){ List&lt;String&gt; list = new ArrayList&lt;String&gt;(); add(list,10); String s = list.get(0);这段代码会抛出一个异常： Exception in thread &quot;main&quot; java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String at ...使用原生类型集合是危险的，这是因为原生类型集合跳过了泛型类型检查，并且不是安全的，在Set、Set&lt;?&gt;和Set&lt;Object&gt;中有很大的不同。 6.访问级别程序员们经常使用public作为类字段的修饰符，可以很简单的通过引用得到值，但这是一个坏的设计，按照经验，分配给成员变量的访问级别应该尽可能的低。 7.ArrayList与LinkedList的对比当程序员们不知道ArrayList与LinkedList的区别时，他们经常使用ArrayList，因为它看起来比较熟悉。然而，它们之前有巨大的性能差别。简而言之，如果有大量的增加删除操作并且没有很多的随机访问元素的操作，应该首先LinkedList。 8.可变与不可变不可变对象有许多的优点，比如简单，安全等等。但是对于每一个不同的值都需要一个独立的对象，太多的对象可能会造成大量的垃圾回收。当选择可变与不可变时应该有一个平衡。 一般的，可变对象用来避免产生大量的中间对象。一个典型的例子是连接大量的字符串。如果你用一个不可变的字符串，你会产生很多需要进行垃圾回收的对象。这很浪费CPU的时间，使用可变对象是正确的解决方案(比如StringBuilder)。 String result=&quot;&quot;; for(String s: arr){ result = result + s; }有时在某些情况下也是需要可变对象的，比如将可变对象作为参数传入方法，你不用使用很多语句便可以得到多个结果。另外一个例子是排序和过滤：当然，你可以写一个方法来接收原始的集合，并且返回一个排好序的集合，但是那样对于大的集合就太浪费了。 9.父类与子类的构造函数 这个编译期错误的出现是父类默认的构造方法未定义，在java中，如果一个类没有定义构造方法，编译器会默认的为这个类添加一个无参的构造方法。如果在父类中定义了构造方法，在这个例子中是Super(String s),编译器就不会添加默认的无参构造方法，这就是上面这个父类的情形。 子类的构造器，不管是无参还有有参，都会调用父类的无参构造器。因为编译器试图在子类的两个构造方法中添加super()方法。但是父类默认的构造方法未定义，编译器就会报出这个错误信息。 想要修复这个问题，可以简单的通过1)在父类中添加一个Super()构造方法，像下面这样： public Super(){ System.out.println(&quot;Super&quot;); }或者2)移除父类自定义的构造方法，或者3)在子类的构造方法中调用父类的super(value)方法。 10.””还是构造器有两种方式可以创建字符串 //1.使用字符串 String x = &quot;abc&quot;; //2.使用构造器 String y = new String(&quot;abc&quot;);有什么区别？ 下面的例子会给出一个快速的答案: String a = &quot;abc&quot;; String b = &quot;abc&quot;; System.out.println(a==b);//true System.out.println(a.equals(b));//true String c = new String(&quot;abc&quot;); String d = new String(&quot;abc&quot;); System.out.println(c==d);//false System.out.println(c.equals(d));//true]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[成为Java顶尖程序员，先过了下面问题！]]></title>
    <url>%2F2017%2F12%2F01%2F%E6%88%90%E4%B8%BAJava%E9%A1%B6%E5%B0%96%E7%A8%8B%E5%BA%8F%E5%91%98%EF%BC%8C%E5%85%88%E8%BF%87%E4%BA%86%E4%B8%8B%E9%9D%A2%E9%97%AE%E9%A2%98%EF%BC%81%2F</url>
    <content type="text"><![CDATA[作者：冰封 链接：https://zhuanlan.zhihu.com/p/31552882 来源：知乎 一、数据结构与算法基础 说一下几种常见的排序算法和分别的复杂度。 用Java写一个冒泡排序算法 描述一下链式存储结构。 如何遍历一棵二叉树？ 倒排一个LinkedList。 用Java写一个递归遍历目录下面的所有文件。 二、Java基础 接口与抽象类的区别？ Java中的异常有哪几类？分别怎么使用？ 常用的集合类有哪些？比如List如何排序？ ArrayList和LinkedList内部的实现大致是怎样的？他们之间的区别和优缺点？ 内存溢出是怎么回事？请举一个例子？ ==和equals的区别？ hashCode方法的作用？ NIO是什么？适用于何种场景？ HashMap实现原理，如何保证HashMap的线程安全？ JVM内存结构，为什么需要GC？ NIO模型，select/epoll的区别，多路复用的原理 Java中一个字符占多少个字节，扩展再问int, long, double占多少字节 创建一个类的实例都有哪些办法？ final/finally/finalize的区别？ Session/Cookie的区别？ String/StringBuffer/StringBuilder的区别，扩展再问他们的实现？ Servlet的生命周期？ 如何用Java分配一段连续的1G的内存空间？需要注意些什么？ Java有自己的内存回收机制，但为什么还存在内存泄露的问题呢？ 什么是java序列化，如何实现java序列化?(写一个实例)？ String s = new String(“abc”);创建了几个 String Object? 三、JVM JVM堆的基本结构。 JVM的垃圾算法有哪几种？CMS垃圾回收的基本流程？ JVM有哪些常用启动参数可以调整，描述几个？ 如何查看JVM的内存使用情况？ Java程序是否会内存溢出，内存泄露情况发生？举几个例子。 你常用的JVM配置和调优参数都有哪些？分别什么作用？ JVM的内存结构？ 常用的GC策略，什么时候会触发YGC，什么时候触发FGC？ 四、多线程/并发 如何创建线程？如何保证线程安全？ 如何实现一个线程安全的数据结构 如何避免死锁 Volatile关键字的作用？ HashMap在多线程环境下使用需要注意什么？为什么？ Java程序中启动一个线程是用run()还是start()？ 什么是守护线程？有什么用？ 什么是死锁？如何避免 线程和进程的差别是什么？ Java里面的Threadlocal是怎样实现的？ ConcurrentHashMap的实现原理是？ sleep和wait区别 notify和notifyAll区别 volatile关键字的作 ThreadLocal的作用与实现 两个线程如何串行执行 上下文切换是什么含义 可以运行时kill掉一个线程吗？ 什么是条件锁、读写锁、自旋锁、可重入锁？ 线程池ThreadPoolExecutor的实现原理？ 五、Linux使用与问题分析排查 使用两种命令创建一个文件？ 硬链接和软链接的区别？ Linux常用命令有哪些？ 怎么看一个Java线程的资源耗用？ Load过高的可能性有哪些？ /etc/hosts文件什么做用？ 如何快速的将一个文本中所有“abc”替换为“xyz”？ 如何在log文件中搜索找出error的日志？ 发现磁盘空间不够，如何快速找出占用空间最大的文件？ Java服务端问题排查（OOM，CPU高，Load高，类冲突） Java常用问题排查工具及用法（top, iostat, vmstat, sar, tcpdump, jvisualvm, jmap, jconsole） Thread dump文件如何分析（Runnable，锁，代码栈，操作系统线程ID关联） 如何查看Java应用的线程信息？ 六、框架使用 描述一下Hibernate的三个状态？ Spring中Bean的生命周期。 SpringMVC或Struts处理请求的流程。 Spring AOP解决了什么问题？怎么实现的？ Spring事务的传播属性是怎么回事？它会影响什么？ Spring中BeanFactory和FactoryBean有什么区别？ Spring框架中IOC的原理是什么？ spring的依赖注入有哪几种方式 struts工作流程 用Spring如何实现一个切面？ Spring 如何实现数据库事务？ Hibernate对一二级缓存的使用，Lazy-Load的理解； mybatis如何实现批量提交？ 七、数据库相关 MySQL InnoDB、Mysaim的特点？ 乐观锁和悲观锁的区别？ 数据库隔离级别是什么？有什么作用？ MySQL主备同步的基本原理。 select * from table t where size &gt; 10 group by size order by size的sql语句执行顺序？ 如何优化数据库性能（索引、分库分表、批量操作、分页算法、升级硬盘SSD、业务优化、主从部署） SQL什么情况下不会使用索引（不包含，不等于，函数） 一般在什么字段上建索引（过滤数据最多的字段） 如何从一张表中查出name字段不包含“XYZ”的所有行？ MySQL，B+索引实现，行锁实现，SQL优化 Redis，RDB和AOF，如何做高可用、集群 如何解决高并发减库存问题 mysql存储引擎中索引的实现机制； 数据库事务的几种粒度； 行锁，表锁；乐观锁，悲观锁 八、网络协议和网络编程 TCP建立连接的过程。 TCP断开连接的过程。 浏览器发生302跳转背后的逻辑？ HTTP协议的交互流程。HTTP和HTTPS的差异，SSL的交互流程？ Rest和Http什么关系？ 大家都说Rest很轻量，你对Rest风格如何理解？ TCP的滑动窗口协议有什么用？讲讲原理。 HTTP协议都有哪些方法？ 交换机和路由器的区别？ Socket交互的基本流程？ http协议（报文结构，断点续传，多线程下载，什么是长连接） tcp协议（建连过程，慢启动，滑动窗口，七层模型） webservice协议（wsdl/soap格式，与rest协议的区别） NIO的好处，Netty线程模型，什么是零拷贝 九、Redis等缓存系统/中间件/NoSQL/一致性Hash等 列举一个常用的Redis客户端的并发模型。 HBase如何实现模糊查询？ 列举一个常用的消息中间件，如果消息要保序如何实现？ 如何实现一个Hashtable？你的设计如何考虑Hash冲突？如何优化？ 分布式缓存，一致性hash LRU算法，slab分配，如何减少内存碎片 如何解决缓存单机热点问题 什么是布隆过滤器，其实现原理是？ False positive指的是？ memcache与redis的区别 zookeeper有什么功能，选举算法如何进行 map/reduce过程，如何用map/reduce实现两个数据源的联合统计 十、设计模式与重构 你能举例几个常见的设计模式 你在设计一个工厂的包的时候会遵循哪些原则？ 你能列举一个使用了Visitor/Decorator模式的开源项目/库吗？ 你在编码时最常用的设计模式有哪些？在什么场景下用？ 如何实现一个单例？ 代理模式（动态代理） 单例模式（懒汉模式，恶汉模式，并发初始化如何解决，volatile与lock的使用） JDK源码里面都有些什么让你印象深刻的设计模式使用，举例看看？]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[flushdns命令]]></title>
    <url>%2F2017%2F12%2F01%2Fflushdns%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[问题描述：Windows系统电脑突然上不了网，或者有些网站能打开，有些网站打不开。 可能原因：很有可能是DNS缓存延时导致的 解决办法：需要清除DNS缓存，清空DNS缓存命令为： 1ipconfig/flushdns 另外可以使用如下命令，用于查看本机已经缓存了的DNS信息 1ipconfig/displaydns]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux自用的一些基础命令]]></title>
    <url>%2F2017%2F11%2F30%2Flinux-e4-b8-80-e4-ba-9b-e5-b8-b8-e7-94-a8-e5-9f-ba-e7-a1-80-e5-91-bd-e4-bb-a4%2F</url>
    <content type="text"><![CDATA[su 进入root权限 输入密码，如果su:鉴定错误就输入sudo su halt 关机 reboot 重新启动 pwd 显示当前目录 ls列出文件和目录的命令 ls 文件名 列出这个文件名的文件或者文件夹 例子: ls -A 列出当前目录下的所有的文件和文件夹，包括隐藏的（隐藏的前面有个“.”，例子： .文件夹 这个是隐藏文件夹） ll 直接用ll代替ls-l ls -l 详细列出当前目录下的所有的文件文件夹，不包括隐藏的，其中信息包括修改的权限，大小，修改日期，文件名 ls -R R的意思就是recursive递归。显示当前目录下的所有的文件和文件夹，不包括隐藏文件，而且显示所有文件夹的子文件和子文件夹里面的所有信息，显示所有能看到的信息，不包括隐藏的文件和文件夹 ls –color 让ls命令用不同的眼色代表不同的文件类型。比如可执行文件用绿色，普通文件是白色，目录是蓝色 cd / 代表链接到根目录 例子: cd /usr/local 就是链接到 usr文件夹下的local文件夹，这样的cd /usr/local在任意目录下都可以连接过去，cd usr 链接到usr文件夹 然后cd local会连接到usr/local文件夹下,没有用斜杠的，在当前目录下必须有该文件夹才可以 如果想要返回上一级 就输入 cd ../ 返回根目录就 cd / 返回自己的目录 用 cd ~ 返回上一步的目录就用 cd - mkdir aaa 生成文件夹aaa touch bbb 生成文件bbb（文件可以不用定义后缀名也可读写） rm bbb 删除文件bbb，后面加-rf 可以将文件夹内的所有文件删掉，慎用 rmdir aaa 删除aaa文件夹 cp bbb /home 将文件bbb复制到根目录下home文件夹内 cp -r /root/text/* /root/text2 将/root/text目录下的所有文件复制到/root/text2中，* 号可以用 . 代替，效果相同 mv bbb /home 将文件bbb移动至根目录下home文件夹内 mv bbb aaa 重命名bbb为aaa vi bbb 进入后按键盘上面的insert（或者Ctrl+i和Ctrl+r）键可以切换insert（插入）和replace（替换） 就可以进行输入，要保存时按ESC键打“：”冒号 输入w（write写入）保存，输入q（quit退出，如果文件有更改会提示错误，这时候要用q!命令） 也可以wq一起输入即为保存并退出 所有命令之后都是按回车 如果进入没有做任何的编辑操作就直接按shift+q，然后q退出 vi编辑器是个强大的工具，需要做深入学习，各种快捷键，非常好用，此处只做简单介绍。cat bbb 查看bbb文件里面输入的所有内容， 如果打开了大型的文件会停不下来 我没有发下什么方法 对文件不知情的情况下慎用more bbb 这个是查看文件查看bbb文件中的第一页内容 如果内容过多就会自动在每一页暂停，等用户按空格键才能继续查看，按q键结束查看 个人建议不确定的情况下用这个man 命令 这是查看命令的内容 例子: man ls 查看查看列表的命令 man mkdir 查看创建文件夹的命令 等等等 进入之后如果内容过多不会一下全部显示出来，会显示一部分，这时候每按一次回车就会出来一点，如果不想看了就直接按q键退出wget 下载链接 会下载文件 chkconfig –list 查看所有程序启动状况，是不是启动项之类的 chkconfig –list | grep mysqld 查看mysqld的程序启动状况，全是off说明不是启动项 chkconfig mysqld on设置mysqld为启动项，之后查询为on lynx http://www.google.com 图形界面的访问谷歌····]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[github搜索10000Star的项目]]></title>
    <url>%2F2017%2F11%2F30%2Fgithub%E6%90%9C%E7%B4%A210000Star%E7%9A%84%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[以java语言为例：github 搜索 java stars:&gt;10000]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[连接打印机需要打开TCP/IP服务]]></title>
    <url>%2F2017%2F11%2F30%2F%E8%BF%9E%E6%8E%A5%E6%89%93%E5%8D%B0%E6%9C%BA%E9%9C%80%E8%A6%81%E6%89%93%E5%BC%80TCPIP%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[连接打印机一直没反应，打印机也可以ping通； 网上查了一下，连接打印机需要打开TCP/IP NetBIOS Helper服务]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[jquery validate 无法验证hidden隐藏域问题]]></title>
    <url>%2F2017%2F11%2F30%2Fjquery%20validate%20%E6%97%A0%E6%B3%95%E9%AA%8C%E8%AF%81hidden%E9%9A%90%E8%97%8F%E5%9F%9F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[表单中出现hidden隐藏域时，jQuery.validate就无法验证隐藏域，这时只要设置ignore就行，但不是在input中这样设：validate=”{ required:true,ignore:’’}”。加入form表单的id为inputForm， 123456789$(document).ready(function()&#123; $('#inputForm').validate(&#123; ignore:\[\], &#125;);&#125;);]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring框架中的@Transactional（事务控制标签）]]></title>
    <url>%2F2017%2F11%2F30%2FSpring%E6%A1%86%E6%9E%B6%E4%B8%AD%E7%9A%84%40Transactional%EF%BC%88%E4%BA%8B%E5%8A%A1%E6%8E%A7%E5%88%B6%E6%A0%87%E7%AD%BE%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Spring事务控制，至关重要，一个service会产生一个事务，自己也可以多方试验与尝试，积累实战经验。 @Transactional spring 事务注解 默认遇到throw new RuntimeException(“…”);会回滚 需要捕获的throw new Exception(“…”);不会回滚 // 指定回滚 @Transactional(rollbackFor=Exception.class) public void methodName() { // 不会回滚 throw new Exception(&quot;...&quot;); } //指定不回滚 @Transactional(noRollbackFor=Exception.class) public ItimDaoImpl getItemDaoImpl() { // 会回滚 throw new RuntimeException(&quot;注释&quot;); } // 如果有事务,那么加入事务,没有的话新建一个(不写的情况下) @Transactional(propagation=Propagation.REQUIRED) // 不管是否存在事务,都创建一个新的事务,原来的挂起,新的执行完毕,继续执行老的事务@Transactional(propagation=Propagation.REQUIRES_NEW) -------------------------------------------------------------------------------------------------- // 必须在一个已有的事务中执行,否则抛出异常 @Transactional(propagation=Propagation.MANDATORY) // 必须在一个没有的事务中执行,否则抛出异常(与Propagation.MANDATORY相反) @Transactional(propagation=Propagation.NEVER) -------------------------------------------------------------------------------------------------- // 如果其他bean调用这个方法,在其他bean中声明事务,那就用事务.如果其他bean没有声明事务,那就不用事务. @Transactional(propagation=Propagation.SUPPORTS) // 容器不为这个方法开启事务 @Transactional(propagation=Propagation.NOT_SUPPORTED) /* public void methodName(){ // 本类的修改方法 1 update(); // 调用其他类的修改方法 otherBean.update(); // 本类的修改方法 2 update(); } other失败了不会影响 本类的修改提交成功 本类update的失败,other也失败 */@Transactional(propagation=Propagation.NESTED) // readOnly=true只读,不能更新,删除 @Transactional (propagation = Propagation.REQUIRED,readOnly=true) // 设置超时时间 @Transactional (propagation = Propagation.REQUIRED,timeout=30) // 设置数据库隔离级别 @Transactional (propagation = Propagation.REQUIRED,isolation=Isolation.DEFAULT)]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PE装系统的坑]]></title>
    <url>%2F2017%2F11%2F30%2Fpe-e8-a3-85-e7-b3-bb-e7-bb-9f-e7-9a-84-e5-9d-91%2F</url>
    <content type="text"><![CDATA[PE工具用的是非常纯净的wepe；不知道其他的PE会不会有同样的情况； 硬盘分区不要用DiskGenius工具，快速分区的时候不要创建MSR分区，不然onekey工具无法找到硬盘，但是ESP分区要创建（不是很确定）。 为了避免这些不必要的麻烦，改用分区助手软件，也有快速分区功能]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[javascript 对象的 可写性，可枚举性，可配置性]]></title>
    <url>%2F2017%2F11%2F29%2Fjavascript%20%E5%AF%B9%E8%B1%A1%E7%9A%84%20%E5%8F%AF%E5%86%99%E6%80%A7%EF%BC%8C%E5%8F%AF%E6%9E%9A%E4%B8%BE%E6%80%A7%EF%BC%8C%E5%8F%AF%E9%85%8D%E7%BD%AE%E6%80%A7%2F</url>
    <content type="text"><![CDATA[可写性（writable）：是否可为属性赋予新值。 可枚举性（enumerable）： 是否可以用for…in、Object.keys( )循环遍历出，是否可以使用JSON.stringify() 序列化为json格式，若不可枚举，仍可访问。 可配置性（configurable）：是否可以修改对象属性的行为。只有可配置的对象才可以delete 删除掉。可以使用Object.definedProperty()定义是否可写、可枚举。若不可配置，只可以将对象的可写性（writable） 由true改为false。 创建对象并定义其属性描述： var tempObj = Object.create(Object.prototype, { a: { writable: true, enumerable: true, value: 1 }, b: { enumerable: true, value: 2 } }); 检测属性： var tempObj = { a:1}; var r = ‘a’; if( tempObj.a ){…} if( tempObj[r] ){…} if( ‘a’ in tempObj ){..} if( r in tempObj ){..} if( tempObj.hasOwnProperty(‘a’) ){…} if( tempObj.hasOwnProperty(r) ){…} if( tempObj.propertyIsEnumerable(‘a’)){…} if( tempObj.hasOwnProperty(r) ){…} 设置属性： Object.defineProperty(tempObj, ‘a’, { value: 222, enumerable: false, writable: false, configurable: false }); 获取属性描述： Object.getOwnPropertyDescriptor(tempObj, ‘a’); 获取对象所有自有属性名： Object.getOwnPropertyNames(); Object.keys();(获取可枚举键)； for…in;(获取可枚举键)；]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[js身份证验证]]></title>
    <url>%2F2017%2F11%2F29%2Fjs-e8-ba-ab-e4-bb-bd-e8-af-81-e9-aa-8c-e8-af-81%2F</url>
    <content type="text"><![CDATA[function checkCardId(socialNo) { if (socialNo == “”) { alert(“输入身份证号码不能为空!”); return (false); } if (socialNo.length != 15 &amp;&amp; socialNo.length != 18) { alert(“输入身份证号码格式不正确!”); return (false); } var area = { 11: “北京”, 12: “天津”, 13: “河北”, 14: “山西”, 15: “内蒙古”, 21: “辽宁”, 22: “吉林”, 23: “黑龙江”, 31: “上海”, 32: “江苏”, 33: “浙江”, 34: “安徽”, 35: “福建”, 36: “江西”, 37: “山东”, 41: “河南”, 42: “湖北”, 43: “湖南”, 44: “广东”, 45: “广西”, 46: “海南”, 50: “重庆”, 51: “四川”, 52: “贵州”, 53: “云南”, 54: “西藏”, 61: “陕西”, 62: “甘肃”, 63: “青海”, 64: “宁夏”, 65: “新疆”, 71: “台湾”, 81: “香港”, 82: “澳门”, 91: “国外” }; if (area[parseInt(socialNo.substr(0, 2))] == null) { alert(“身份证号码不正确(地区非法)!”); return (false); } if (socialNo.length == 15) { pattern = /^\d{15}$/; if (pattern.exec(socialNo) == null) { alert(“15位身份证号码必须为数字！”); return (false); } var birth = parseInt(“19” + socialNo.substr(6, 2)); var month = socialNo.substr(8, 2); var day = parseInt(socialNo.substr(10, 2)); switch (month) { case ‘01’: case ‘03’: case ‘05’: case ‘07’: case ‘08’: case ‘10’: case ‘12’: if (day &gt; 31) { alert(‘输入身份证号码不格式正确!’); return false; } break; case ‘04’: case ‘06’: case ‘09’: case ‘11’: if (day &gt; 30) { alert(‘输入身份证号码不格式正确!’); return false; } break; case ‘02’: if ((birth % 4 == 0 &amp;&amp; birth % 100 != 0) || birth % 400 == 0) { if (day &gt; 29) { alert(‘输入身份证号码不格式正确!’); return false; } } else { if (day &gt; 28) { alert(‘输入身份证号码不格式正确!’); return false; } } break; default: alert(‘输入身份证号码不格式正确!’); return false; } var nowYear = new Date().getYear(); if (nowYear - parseInt(birth) &lt; 15 || nowYear - parseInt(birth) &gt; 100) { alert(‘输入身份证号码不格式正确!’); return false; } return (true); } var Wi = new Array( 7, 9, 10, 5, 8, 4, 2, 1, 6, 3, 7, 9, 10, 5, 8, 4, 2, 1 ); var lSum = 0; var nNum = 0; var nCheckSum = 0; for (i = 0; i &lt; 17; ++i) { if (socialNo.charAt(i) &lt; ‘0’ || socialNo.charAt(i) &gt; ‘9’) { alert(“输入身份证号码格式不正确!”); return (false); } else { nNum = socialNo.charAt(i) - ‘0’; } lSum += nNum * Wi[i]; } if (socialNo.charAt(17) == ‘X’ || socialNo.charAt(17) == ‘x’) { lSum += 10 * Wi[17]; } else if (socialNo.charAt(17) &lt; ‘0’ || socialNo.charAt(17) &gt; ‘9’) { alert(“输入身份证号码格式不正确!”); return (false); } else { lSum += (socialNo.charAt(17) - ‘0’) * Wi[17]; } if ((lSum % 11) == 1) { return true; } else { alert(“输入身份证号码格式不正确!”); return (false); } }]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[javascript 是解释型、动态的、弱类型语言]]></title>
    <url>%2F2017%2F11%2F29%2Fjavascript%20%E6%98%AF%E8%A7%A3%E9%87%8A%E5%9E%8B%E3%80%81%E5%8A%A8%E6%80%81%E7%9A%84%E3%80%81%E5%BC%B1%E7%B1%BB%E5%9E%8B%E8%AF%AD%E8%A8%80%2F</url>
    <content type="text"><![CDATA[解释型：直至程序运行前一刻，只有源代码，无可执行文件。后逐步解释-&gt;执行-&gt;解释-&gt;执行…( 速度要求不高、对系统兼容性有要求); 编译型：一次性将所有代码全部转换成机器语言，后写成可执行文件（执行速度快、系统要求低); 动态类型语言：只在运行期间才去做数据类型检查的语言。 静态类型语言：编译期检查。 Static typing when possible, dynamic typing when needed. 动态类型语言不需要给任何变量指定数据类型，在第一次赋值时候，将内部数据类型记录。 弱类型定义语言:数据类型可被忽略，同一变量可被多次赋予不同类型的值（速度快） 强类型定义语言：强制数据类型定义的语言（一旦定义，后不经强制类型转换，数据类型不可改变，严谨，安全）;]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[DNS解析]]></title>
    <url>%2F2017%2F11%2F28%2FDNS%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[当访问一个网站时，系统将从DNS缓存中读取该域名所对应的IP地址，当查找不到时就会到系统中查找hosts文件，如果还没有那么才会向DNS服务器请求一个DNS查询，DNS服务器将返回该域名所对应的IP，在你的系统收到解析地址以后将使用该IP地址进行访问，同时将解析缓存到本地的DNS缓存中。]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[多态]]></title>
    <url>%2F2017%2F08%2F01%2F%E5%A4%9A%E6%80%81%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829class Base&#123; public void method() &#123; System.out.println("Base"); &#125;&#125;class Son extends Base&#123; public void method() &#123; System.out.println("Son"); &#125;public void methodB()&#123; System.out.println("SonB");&#125;&#125;public class Test01&#123; public static void main(String\[\] args) &#123; Base base = new Son(); base.method(); base.methodB(); &#125;&#125; 这句new 了一个子类，赋值给父类，所以下面的操作编译器认为base对象就是Base类型的 Base类中不存在methodB()方法，所以编译不通过 要想调用的话需要先通过SON son=(SON)base;强制转换，然后用son.methodB()调用就可以了。 多态编程：编译看左边运行看右边]]></content>
      <categories>
        <category>java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql基础操作命令]]></title>
    <url>%2F2017%2F07%2F27%2Fmysql%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[用户名密码登录数据库 mysql -uUSERNAME -pPASSWORD 显示所有数据库 mysql&gt;show databases; 选择一个库 mysql&gt;use databaseNmae; 显示use的库的所有表 mysql&gt;show tables; 退出use直接执行show databases；或者use另外一个表]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux安装jdk1.8]]></title>
    <url>%2F2017%2F07%2F14%2FLinux%E5%AE%89%E8%A3%85jdk1.8%2F</url>
    <content type="text"><![CDATA[查看jdk的yum源 yum -y list java*安装jdk yum install java-1.8.0-openjdk.x86_64配置环境变量（添加到底部，JAVA_HOME路径一般在/usr/lib/jvm下） JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.191.b12-1.el7_6.x86_64 JRE_HOME=$JAVA_HOME/jre PATH=$PATH:$JAVA_HOME/bin CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export JAVA_HOME export JRE_HOME export PATH export CLASSPATH刷新环境变量 source /etc/profile查看版本 java -version压缩包安装更加简单，只需要下载解压tar.gz压缩包，然后重复上述配置环境变量步骤即可 发现一个问题：如果用yum安装是没有jdk的？在编辑器配置jdk环境死活找不到jdk的位置，可能是还没有下足心思去找，最后无奈下载压缩包解压配置环境变量才能配置编辑器的jdk环境]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[控制反转（IoC）与依赖注入（DI）]]></title>
    <url>%2F2017%2F06%2F13%2F%E6%8E%A7%E5%88%B6%E5%8F%8D%E8%BD%AC%EF%BC%88IoC%EF%BC%89%E4%B8%8E%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5%EF%BC%88DI%EF%BC%89%2F</url>
    <content type="text"><![CDATA[转自：http://blog.xiaohansong.com/2015/10/21/IoC-and-DI/ 前言 最近在学习Spring框架，它的核心就是IoC容器。要掌握Spring框架，就必须要理解控制反转的思想以及依赖注入的实现方式。下面，我们将围绕下面几个问题来探讨控制反转与依赖注入的关系以及在Spring中如何应用。 什么是控制反转？ 什么是依赖注入？ 它们之间有什么关系？ 如何在Spring框架中应用依赖注入？ 什么是控制反转在讨论控制反转之前，我们先来看看软件系统中耦合的对象。 图1：软件系统中耦合的对象 从图中可以看到，软件中的对象就像齿轮一样，协同工作，但是互相耦合，一个零件不能正常工作，整个系统就崩溃了。这是一个强耦合的系统。齿轮组中齿轮之间的啮合关系,与软件系统中对象之间的耦合关系非常相似。对象之间的耦合关系是无法避免的，也是必要的，这是协同工作的基础。现在，伴随着工业级应用的规模越来越庞大，对象之间的依赖关系也越来越复杂，经常会出现对象之间的多重依赖性关系，因此，架构师和设计师对于系统的分析和设计，将面临更大的挑战。对象之间耦合度过高的系统，必然会出现牵一发而动全身的情形。 为了解决对象间耦合度过高的问题，软件专家Michael Mattson提出了IOC理论，用来实现对象之间的“解耦”。 控制反转（Inversion of Control）是一种是面向对象编程中的一种设计原则，用来减低计算机代码之间的耦合度。其基本思想是：借助于“第三方”实现具有依赖关系的对象之间的解耦。 图2：IOC解耦过程 由于引进了中间位置的“第三方”，也就是IOC容器，使得A、B、C、D这4个对象没有了耦合关系，齿轮之间的传动全部依靠“第三方”了，全部对象的控制权全部上缴给“第三方”IOC容器，所以，IOC容器成了整个系统的关键核心，它起到了一种类似“粘合剂”的作用，把系统中的所有对象粘合在一起发挥作用，如果没有这个“粘合剂”，对象与对象之间会彼此失去联系，这就是有人把IOC容器比喻成“粘合剂”的由来。 我们再来看看，控制反转(IOC)到底为什么要起这么个名字？我们来对比一下： 软件系统在没有引入IOC容器之前，如图1所示，对象A依赖于对象B，那么对象A在初始化或者运行到某一点的时候，自己必须主动去创建对象B或者使用已经创建的对象B。无论是创建还是使用对象B，控制权都在自己手上。 软件系统在引入IOC容器之后，这种情形就完全改变了，如图2所示，由于IOC容器的加入，对象A与对象B之间失去了直接联系，所以，当对象A运行到需要对象B的时候，IOC容器会主动创建一个对象B注入到对象A需要的地方。 通过前后的对比，我们不难看出来：对象A获得依赖对象B的过程,由主动行为变为了被动行为，控制权颠倒过来了，这就是“控制反转”这个名称的由来。 控制反转不只是软件工程的理论，在生活中我们也有用到这种思想。再举一个现实生活的例子： 海尔公司作为一个电器制商需要把自己的商品分销到全国各地，但是发现，不同的分销渠道有不同的玩法，于是派出了各种销售代表玩不同的玩法，随着渠道越来越多，发现，每增加一个渠道就要新增一批人和一个新的流程，严重耦合并依赖各渠道商的玩法。实在受不了了，于是制定业务标准，开发分销信息化系统，只有符合这个标准的渠道商才能成为海尔的分销商。让各个渠道商反过来依赖自己标准。反转了控制，倒置了依赖。 我们把海尔和分销商当作软件对象，分销信息化系统当作IOC容器，可以发现，在没有IOC容器之前，分销商就像图1中的齿轮一样，增加一个齿轮就要增加多种依赖在其他齿轮上，势必导致系统越来越复杂。开发分销系统之后，所有分销商只依赖分销系统，就像图2显示那样，可以很方便的增加和删除齿轮上去。 什么是依赖注入依赖注入就是将实例变量传入到一个对象中去(Dependency injection means giving an object its instance variables)。 什么是依赖如果在 Class A 中，有 Class B 的实例，则称 Class A 对 Class B 有一个依赖。例如下面类 Human 中用到一个 Father 对象，我们就说类 Human 对类 Father 有一个依赖。 public class Human { ... Father father; ... public Human() { father = new Father(); } }仔细看这段代码我们会发现存在一些问题： 如果现在要改变 father 生成方式，如需要用new Father(String name)初始化 father，需要修改 Human 代码； 如果想测试不同 Father 对象对 Human 的影响很困难，因为 father 的初始化被写死在了 Human 的构造函数中； 如果new Father()过程非常缓慢，单测时我们希望用已经初始化好的 father 对象 Mock 掉这个过程也很困难。 依赖注入上面将依赖在构造函数中直接初始化是一种 Hard init 方式，弊端在于两个类不够独立，不方便测试。我们还有另外一种 Init 方式，如下： public class Human { ... Father father; ... public Human(Father father) { this.father = father; } }上面代码中，我们将 father 对象作为构造函数的一个参数传入。在调用 Human 的构造方法之前外部就已经初始化好了 Father 对象。像这种非自己主动初始化依赖，而通过外部来传入依赖的方式，我们就称为依赖注入。 现在我们发现上面 1 中存在的两个问题都很好解决了，简单的说依赖注入主要有两个好处： 解耦，将依赖之间解耦。 因为已经解耦，所以方便做单元测试，尤其是 Mock 测试。 控制反转和依赖注入的关系我们已经分别解释了控制反转和依赖注入的概念。有些人会把控制反转和依赖注入等同，但实际上它们有着本质上的不同。 控制反转是一种思想 依赖注入是一种设计模式 IoC框架使用依赖注入作为实现控制反转的方式，但是控制反转还有其他的实现方式，例如说ServiceLocator，所以不能将控制反转和依赖注入等同。 Spring中的依赖注入上面我们提到，依赖注入是实现控制反转的一种方式。下面我们结合Spring的IoC容器，简单描述一下这个过程。 class MovieLister... private MovieFinder finder; public void setFinder(MovieFinder finder) { this.finder = finder; } class ColonMovieFinder... public void setFilename(String filename) { this.filename = filename; }我们先定义两个类，可以看到都使用了依赖注入的方式，通过外部传入依赖，而不是自己创建依赖。那么问题来了，谁把依赖传给他们，也就是说谁负责创建finder，并且把finder传给MovieLister。答案是Spring的IoC容器。 要使用IoC容器，首先要进行配置。这里我们使用xml的配置，也可以通过代码注解方式配置。下面是spring.xml的内容 &lt;beans&gt; &lt;bean id=&quot;MovieLister&quot; class=&quot;spring.MovieLister&quot;&gt; &lt;property name=&quot;finder&quot;&gt; &lt;ref local=&quot;MovieFinder&quot;/&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;MovieFinder&quot; class=&quot;spring.ColonMovieFinder&quot;&gt; &lt;property name=&quot;filename&quot;&gt; &lt;value&gt;movies1.txt&lt;/value&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt;在Spring中，每个bean代表一个对象的实例，默认是单例模式，即在程序的生命周期内，所有的对象都只有一个实例，进行重复使用。通过配置bean，IoC容器在启动的时候会根据配置生成bean实例。具体的配置语法参考Spring文档。这里只要知道IoC容器会根据配置创建MovieFinder，在运行的时候把MovieFinder赋值给MovieLister的finder属性，完成依赖注入的过程。 下面给出测试代码 public void testWithSpring() throws Exception { ApplicationContext ctx = new FileSystemXmlApplicationContext(&quot;spring.xml&quot;);//1 MovieLister lister = (MovieLister) ctx.getBean(&quot;MovieLister&quot;);//2 Movie[] movies = lister.moviesDirectedBy(&quot;Sergio Leone&quot;); assertEquals(&quot;Once Upon a Time in the West&quot;, movies[0].getTitle()); } 根据配置生成ApplicationContext，即IoC容器。 从容器中获取MovieLister的实例。 总结 控制反转是一种在软件工程中解耦合的思想，调用类只依赖接口，而不依赖具体的实现类，减少了耦合。控制权交给了容器，在运行的时候才由容器决定将具体的实现动态的“注入”到调用类的对象中。 依赖注入是一种设计模式，可以作为控制反转的一种实现方式。依赖注入就是将实例变量传入到一个对象中去(Dependency injection means giving an object its instance variables)。 通过IoC框架，类A依赖类B的强耦合关系可以在运行时通过容器建立，也就是说把创建B实例的工作移交给容器，类A只管使用就可以。]]></content>
      <categories>
        <category>java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[30个Java集合面试问题及答案]]></title>
    <url>%2F2017%2F06%2F08%2F30%E4%B8%AAJava%E9%9B%86%E5%90%88%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E5%8F%8A%E7%AD%94%E6%A1%88%2F</url>
    <content type="text"><![CDATA[Java集合框架为Java编程语言的基础，也是Java面试中很重要的一个知识点。这里，我列出了一些关于Java集合的重要问题和答案。 1.Java集合框架是什么？说出一些集合框架的优点？ 每种编程语言中都有集合，最初的Java版本包含几种集合类：Vector、Stack、HashTable和Array。 随着集合的广泛使用，Java1.2提出了囊括所有集合接口、实现和算法的集合框架。在保证线程安全的情况下使用泛型和并发集合类，Java已经经历了很久。 它还包括在Java并发包中，阻塞接口以及它们的实现。集合框架的部分优点如下： （1）使用核心集合类降低开发成本，而非实现我们自己的集合类。 （2）随着使用经过严格测试的集合框架类，代码质量会得到提高。 （3）通过使用JDK附带的集合类，可以降低代码维护成本。 （4）复用性和可操作性。 2.集合框架中的泛型有什么优点？ 1.Java1.5引入了泛型，所有的集合接口和实现都大量地使用它。 2.泛型允许我们为集合提供一个可以容纳的对象类型，因此，如果你添加其它类型的任何元素，它会在编译时报错。 3.这避免了在运行时出现ClassCastException，因为你将会在编译时得到报错信息。 4.泛型也使得代码整洁，我们不需要使用显式转换和instanceOf操作符。 5.它也给运行时带来好处，因为不会产生类型检查的字节码指令。 3.Java集合框架的基础接口有哪些？ Collection为集合层级的根接口。一个集合代表一组对象，这些对象即为它的元素。Java平台不提供这个接口任何直接的实现。 Set是一个不能包含重复元素的集合。这个接口对数学集合抽象进行建模，被用来代表集合，就如一副牌。 List是一个有序集合，可以包含重复元素。你可以通过它的索引来访问任何元素。List更像长度动态变换的数组。 Map是一个将key映射到value的对象.一个Map不能包含重复的key：每个key最多只能映射一个value。 一些其它的接口有Queue、Dequeue、SortedSet、SortedMap和ListIterator。 4.为何Collection不从Cloneable和Serializable接口继承？ Collection接口指定一组对象，对象即为它的元素。如何维护这些元素由Collection的具体实现决定。例如，一些如List的Collection实现允许重复的元素，而其它的如Set就不允许。 很多Collection实现有一个公有的clone方法。然而，把它放到集合的所有实现中也是没有意义的。这是因为Collection是一个抽象表现。重要的是实现。 当与具体实现打交道的时候，克隆或序列化的语义和含义才发挥作用。所以，具体实现应该决定如何对它进行克隆或序列化，或它是否可以被克隆或序列化。 在所有的实现中授权克隆和序列化，最终导致更少的灵活性和更多的限制。特定的实现应该决定它是否可以被克隆和序列化。 5.为何Map接口不继承Collection接口？ 尽管Map接口和它的实现也是集合框架的一部分，但Map不是集合，集合也不是Map。因此，Map继承Collection毫无意义，反之亦然。 如果Map继承Collection接口，那么元素去哪儿？Map包含key-value对，它提供抽取key或value列表集合的方法，但是它不适合“一组对象”规范。 6.Iterator是什么？ Iterator接口提供遍历任何Collection的接口。我们可以从一个Collection中使用迭代器方法来获取迭代器实例。迭代器取代了Java集合框架中的Enumeration。迭代器允许调用者在迭代过程中移除元素。 7.Enumeration和Iterator接口的区别？ Enumeration的速度是Iterator的两倍，也使用更少的内存。Enumeration是非常基础的，也满足了基础的需要。但是，与Enumeration相比，Iterator更加安全，因为当一个集合正在被遍历的时候，它会阻止其它线程去修改集合。 迭代器取代了Java集合框架中的Enumeration。迭代器允许调用者从集合中移除元素，而Enumeration不能做到。为了使它的功能更加清晰，迭代器方法名已经经过改善。 8.为何没有像Iterator.add()这样的方法，向集合中添加元素？ 语义不明，已知的是，Iterator的协议不能确保迭代的次序。然而要注意，ListIterator没有提供一个add操作，它要确保迭代的顺序。 9.为何迭代器没有一个方法可以直接获取下一个元素，而不需要移动游标？ 它可以在当前Iterator的顶层实现，但是它用得很少，如果将它加到接口中，每个继承都要去实现它，这没有意义。 10.Iterater和ListIterator之间有什么区别？ （1）我们可以使用Iterator来遍历Set和List集合，而ListIterator只能遍历List。 （2）Iterator只可以向前遍历，而LIstIterator可以双向遍历。 （3）ListIterator从Iterator接口继承，然后添加了一些额外的功能，比如添加一个元素、替换一个元素、获取前面或后面元素的索引位置。 11.通过迭代器fail-fast属性，你明白了什么？ 每次我们尝试获取下一个元素的时候，Iterator fail-fast属性检查当前集合结构里的任何改动。如果发现任何改动，它抛出ConcurrentModificationException。Collection中所有Iterator的实现都是按fail-fast来设计的（ConcurrentHashMap和CopyOnWriteArrayList这类并发集合类除外）。 12.fail-fast与fail-safe有什么区别？ Iterator的fail-fast属性与当前的集合共同起作用，因此它不会受到集合中任何改动的影响。Java.util包中的所有集合类都被设计为fail-fast的， 而java.util.concurrent中的集合类都为fail-safe的。 Fall—fast迭代器抛出ConcurrentModificationException， fall—safe迭代器从不抛出ConcurrentModificationException。 13.在迭代一个集合的时候，如何避免？ ConcurrentModificationException？ 在遍历一个集合的时候； 我们可以使用并发集合类来避免ConcurrentModificationException，比如使用CopyOnWriteArrayList，而不是ArrayList。 14.为何Iterator接口没有具体的实现？ Iterator接口定义了遍历集合的方法，但它的实现则是集合实现类的责任。每个能够返回用于遍历的Iterator的集合类都有它自己的Iterator实现内部类。 这就允许集合类去选择迭代器是fail-fast还是fail-safe的。比如，ArrayList迭代器是fail-fast的，而CopyOnWriteArrayList迭代器是fail-safe的。 15.UnsupportedOperationException是什么？ UnsupportedOperationException是用于表明操作不支持的异常。在JDK类中已被大量运用， 在集合框架java.util.Collections.UnmodifiableCollection将会在所有add和remove操作中抛出这个异常。 16.hashCode()和equals()方法有何重要性？ HashMap使用Key对象的hashCode()和equals()方法去决定key-value对的索引。 当我们试着从HashMap中获取值的时候，这些方法也会被用到。如果这些方法没有被正确地实现，在这种情况下，两个不同Key也许会产生相同的hashCode()和equals()输出，HashMap将会认为它们是相同的，然后覆盖它们，而非把它们存储到不同的地方。 同样的，所有不允许存储重复数据的集合类都使用hashCode()和equals()去查找重复，所以正确实现它们非常重要。equals()和hashCode()的实现应该遵循以下规则： 1.如果o1.equals(o2)，那么o1.hashCode() == o2.hashCode()总是为true的。 2.如果o1.hashCode() == o2.hashCode()，并不意味着o1.equals(o2)会为true。 17.Map接口提供了哪些不同的集合视图？ Map接口提供三个集合视图： （1）Set keyset()：返回map中包含的所有key的一个Set视图。 集合是受map支持的，map的变化会在集合中反映出来，反之亦然。 当一个迭代器正在遍历一个集合时，若map被修改了（除迭代器自身的移除操作以外），迭代器的结果会变为未定义。 集合支持通过Iterator的Remove、Set.remove、removeAll、retainAll和clear操作进行元素移除，从map中移除对应的映射。 它不支持add和addAll操作。 （2）Collection values()：返回一个map中包含的所有value的一个Collection视图。 这个collection受map支持的，map的变化会在collection中反映出来，反之亦然。 当一个迭代器正在遍历一个collection时，若map被修改了（除迭代器自身的移除操作以外），迭代器的结果会变为未定义。 集合支持通过Iterator的Remove、Set.remove、removeAll、retainAll和clear操作进行元素移除，从map中移除对应的映射。 它不支持add和addAll操作。 （3）Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet()：返回一个map钟包含的所有映射的一个集合视图。 这个集合受map支持的，map的变化会在collection中反映出来，反之亦然。 当一个迭代器正在遍历一个集合时，若map被修改了（除迭代器自身的移除操作，以及对迭代器返回的entry进行setValue外），迭代器的结果会变为未定义。 集合支持通过Iterator的Remove、Set.remove、removeAll、retainAll和clear操作进行元素移除，从map中移除对应的映射。它不支持add和addAll操作。 18.HashMap和HashTable有何不同？ （1）HashMap允许key和value为null，而HashTable不允许。 （2）HashTable是同步的，而HashMap不是。所以HashMap适合单线程环境，HashTable适合多线程环境。 （3）在Java1.4中引入了LinkedHashMap，HashMap的一个子类，假如你想要遍历顺序，你很容易从HashMap转向LinkedHashMap，但是HashTable不是这样的，它的顺序是不可预知的。 （4）HashMap提供对key的Set进行遍历，因此它是fail-fast的，但HashTable提供对key的Enumeration进行遍历，它不支持fail-fast。 （5）HashTable被认为是个遗留的类，如果你寻求在迭代的时候修改Map，你应该使用CocurrentHashMap。 19.如何决定选用HashMap还是TreeMap？ 对于在Map中插入、删除和定位元素这类操作，HashMap是最好的选择。然而，假如你需要对一个有序的key集合进行遍历，TreeMap是更好的选择。基于你的collection的大小，也许向HashMap中添加元素会更快，将map换为TreeMap进行有序key的遍历。 20.ArrayList和Vector有何异同点？ ArrayList和Vector在很多时候都很类似。 （1）两者都是基于索引的，内部由一个数组支持。 （2）两者维护插入的顺序，我们可以根据插入顺序来获取元素。 （3）ArrayList和Vector的迭代器实现都是fail-fast的。 （4）ArrayList和Vector两者允许null值，也可以使用索引值对元素进行随机访问。 以下是ArrayList和Vector的不同点。 （1）Vector是同步的，而ArrayList不是。然而，如果你寻求在迭代的时候对列表进行改变，你应该使用CopyOnWriteArrayList。 （2）ArrayList比Vector快，它因为有同步，不会过载。 （3）ArrayList更加通用，因为我们可以使用Collections工具类轻易地获取同步列表和只读列表。 21.Array和ArrayList有何区别？什么时候更适合用Array？ Array可以容纳基本类型和对象，而ArrayList只能容纳对象。 Array是指定大小的，而ArrayList大小是固定的。 Array没有提供ArrayList那么多功能，比如addAll、removeAll和iterator等。尽管ArrayList明显是更好的选择，但也有些时候Array比较好用。 （1）如果列表的大小已经指定，大部分情况下是存储和遍历它们。 （2）对于遍历基本数据类型，尽管Collections使用自动装箱来减轻编码任务，在指定大小的基本类型的列表上工作也会变得很慢。 （3）如果你要使用多维数组，使用[][]比List&lt;List&lt;&gt;&gt;更容易。 22.ArrayList和LinkedList有何区别？ ArrayList和LinkedList两者都实现了List接口，但是它们之间有些不同。 （1）ArrayList是由Array所支持的基于一个索引的数据结构，所以它提供对元素的随机访问，复杂度为O(1)，但LinkedList存储一系列的节点数据，每个节点都与前一个和下一个节点相连接。 所以，尽管有使用索引获取元素的方法，内部实现是从起始点开始遍历，遍历到索引的节点然后返回元素，时间复杂度为O(n)，比ArrayList要慢。 （2）与ArrayList相比，在LinkedList中插入、添加和删除一个元素会更快，因为在一个元素被插入到中间的时候，不会涉及改变数组的大小，或更新索引。 （3）LinkedList比ArrayList消耗更多的内存，因为LinkedList中的每个节点存储了前后节点的引用。 23.哪些集合类提供对元素的随机访问？ ArrayList、HashMap、TreeMap和HashTable类提供对元素的随机访问。 24.哪些集合类是线程安全的？ Vector、HashTable、Properties和Stack是同步类，所以它们是线程安全的，可以在多线程环境下使用。Java1.5并发API包括一些集合类，允许迭代时修改，因为它们都工作在集合的克隆上，所以它们在多线程环境中是安全的。 25.并发集合类是什么？ Java1.5并发包（java.util.concurrent）包含线程安全集合类，允许在迭代时修改集合。 迭代器被设计为fail-fast的，会抛出ConcurrentModificationException。 一部分类为：CopyOnWriteArrayList、 ConcurrentHashMap、CopyOnWriteArraySet。 26.队列和栈是什么，列出它们的区别？ 栈和队列两者都被用来预存储数据。java.util.Queue是一个接口，它的实现类在Java并发包中。队列允许先进先出（FIFO）检索元素，但并非总是这样。Deque接口允许从两端检索元素。 栈与队列很相似，但它允许对元素进行后进先出（LIFO）进行检索。 Stack是一个扩展自Vector的类，而Queue是一个接口。 27.Collections类是什么？ Java.util.Collections是一个工具类仅包含静态方法，它们操作或返回集合。 它包含操作集合的多态算法，返回一个由指定集合支持的新集合和其它一些内容。 这个类包含集合框架算法的方法，比如折半搜索、排序、混编和逆序等。 28.Comparable和Comparator接口有何区别？ Comparable和Comparator接口被用来对对象集合或者数组进行排序。Comparable接口被用来提供对象的自然排序，我们可以使用它来提供基于单个逻辑的排序。 Comparator接口被用来提供不同的排序算法，我们可以选择需要使用的Comparator来对给定的对象集合进行排序。 29.我们如何对一组对象进行排序？ 如果我们需要对一个对象数组进行排序，我们可以使用Arrays.sort()方法。如果我们需要排序一个对象列表，我们可以使用Collection.sort()方法。 两个类都有用于自然排序（使用Comparable）或基于标准的排序（使用Comparator）的重载方法sort()。Collections内部使用数组排序方法，所有它们两者都有相同的性能，只是Collections需要花时间将列表转换为数组。 30.当一个集合被作为参数传递给一个函数时，如何才可以确保函数不能修改它？ 在作为参数传递之前，我们可以使用Collections.unmodifiableCollection(Collection c)方法创建一个只读集合， 这将确保改变集合的任何操作都会抛出UnsupportedOperationException。]]></content>
      <categories>
        <category>java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[java将字符串放到数组]]></title>
    <url>%2F2017%2F05%2F31%2Fjava%E5%B0%86%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%94%BE%E5%88%B0%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[12345678String str1="aaa";String str2="bbb";String str3="ccc";List&lt;String&gt; list = new ArrayList&lt;String&gt;();list.add(str1);list.add(str2);list.add(str3);String\[\] arr = list.toArray(new String\[list.size()\]);]]></content>
      <categories>
        <category>java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java反射]]></title>
    <url>%2F2017%2F05%2F21%2Fjava%E5%8F%8D%E5%B0%84%2F</url>
    <content type="text"></content>
      <categories>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML 属性]]></title>
    <url>%2F2017%2F04%2F27%2Fhtml-shu-xing%2F</url>
    <content type="text"><![CDATA[属性为HTML 元素提供附加信息HTML 属性 HTML 标签可以拥有属性。属性提供了有关HTML元素的更多的信息。 属性总是以名称/值对的形式出现，比如 name = &quot;value&quot;。 属性总是在HTML 元素的开始标签中规定。 属性实例HTML 链接由 &lt;a&gt; 标签定义。链接的地址在href属性中指定。&lt;a href = &quot;http://www.baidu.com&quot;&gt; This is a link&lt;/a&gt;HTML 提示：使用小写属性属性和属性值对大小写不敏感。万维网联盟在HTML 4 推荐标准中推荐小写的属性/属性值。而在(X)HTML 要求使用小写属性。 始终为属性值加引号属性值应该始终被包括在引号内。 ￼ ￼]]></content>
      <categories>
        <category>Layout</category>
      </categories>
      <tags>
        <tag>HTML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML 简介]]></title>
    <url>%2F2017%2F04%2F27%2Fhtml-jian-jie%2F</url>
    <content type="text"><![CDATA[什么是HTML？HTML 是用来描述网页的一种语言。 HTML 指的是超文本标记语言（Hyper Text Markup Language） HTML 不是一种编程语言，而是一种标记语言 标记语言是一套 标记标签（markup tag） HTML 使用标记标签来描述网页 HTML标签标记标签通常被称为HTML 标签（HTML tag）。 HTML 标签是由尖括号包围的关键词，&lt;html&gt; HTML 标签通常是成对出现的，比如 &lt;b&gt; 和 &lt;/b&gt; 标签对中的第一个标签是开始标签，第二个标签是结束标签 开始标签和结束标签也被称为开放标签和闭合标签 HTML文档 = 网页 HTML 文档描述网页 HTML 文档包含HTML标签 和 纯文本 HTML 文档也被称为网页 Web 浏览器的作用就是读取HTML文档，并以网页的形式显示它们。浏览器不会显示HTML标签，而是使用标签来解释页面的内容。&lt;html&gt; &lt;body&gt; &lt;h1&gt;我的第一个标题。&lt;/h1&gt; &lt;p&gt;我的第一个段落。&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; &lt;html&gt;与&lt;/html&gt;之间的文本描述网页。 &lt;body&gt;与&lt;/body&gt;之间的文本是可见的页面内容。]]></content>
      <categories>
        <category>Layout</category>
      </categories>
      <tags>
        <tag>HTML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML 样式]]></title>
    <url>%2F2017%2F04%2F09%2Fhtml-yang-shi%2F</url>
    <content type="text"><![CDATA[style属性用于改变HTML元素的样式。HTML 属性的style：提供了一种改变所有HTML元素的样式的通用方法。样式是HTML4 引入的，它是一种首选的改变HTML元素样式的方式。]]></content>
      <categories>
        <category>Layout</category>
      </categories>
      <tags>
        <tag>HTML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML 段落]]></title>
    <url>%2F2017%2F04%2F09%2Fhtml-duan-luo%2F</url>
    <content type="text"><![CDATA[可以把HTML 文档分隔成若干段落。HTML 段落HTML段落是通过&lt;p&gt;标签定义的。注释： 浏览器会自动的在段落的前后添加空行。&lt;p&gt; 元素是块级元素。使用空段落&lt;p&gt;&lt;/p&gt;去插入一个空行是个坏习惯。使用&lt;br /&gt;代替。 &lt;p&gt; display:block;margin-top:1em;margin-bottom:1emHTML 折行如果期望在不产生一个新的段落的情况下进行换行（新行），使用&lt;br /&gt;。 &lt;br /&gt; 还是 &lt;br&gt;&lt;br&gt;、&lt;br /&gt;很相似。在XHTML、XML、以及未来的HTML版本中。不允许使用没有结束标签（闭合标签）的HTML元素。即使&lt;br&gt;目前在所有浏览器中显示都没有问题，但使用&lt;br /&gt;也是更长远的保障。 HTML 输出-有用的提示无法确定HTML 被显示的确切效果。屏幕大小，以及对窗口的调整都可能导致不同的结果。对于HTML， 无法通过在HTML代码中添加额外的空格或者换行来改变输出的结果。当显示页面时候，浏览器会移除源代码中多余的空格和空行。所有连续的空格或空行都会被算作是一个空格。]]></content>
      <categories>
        <category>Layout</category>
      </categories>
      <tags>
        <tag>HTML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML 标题]]></title>
    <url>%2F2017%2F04%2F09%2Fhtml-biao-ti%2F</url>
    <content type="text"><![CDATA[在HTML 文档中，标题很重要。HTML 标题 标题（Heading）是通过 &lt;h1&gt; ~ &lt;h6&gt;等标签定义的。 &lt;h1&gt;定义最大的标题。&lt;h6&gt;定义最小的标题。 浏览器默认字体宋体/simsun 16px 凡是浏览器默认字体不起作用的页面都是在CSS中设置了中文字体的页面。 font-family 属性是会在最后带有sans-serif 或serif ，其分别表示“无衬线字体”，和“衬线字体”，其作用为终结，终结font-family属性，也就是跟在sans-serif/serif 后面的字体都是摆设。 html{font-family:Arial,Helvetica,sans-serif,&quot;宋体&quot;;} html{font-family:Arial,Helvetica,‘&quot;宋体&quot;’,sans-serif;} 为什么需要在body中设置font-family属性，其一在于统一，其二更好看的英文字符...可用性之浏览器默认字体与css字体 标题很重要 确保将HTML heading 标签只用于标题。 不要仅仅是为了产生粗体或大号的文本而使用标题。 搜索引擎使用标题为网页的结构和内容编制索引。 用户可以通过标题来快速浏览。呈现文档结构。 HTML 水平线&lt;hr /&gt;水平线标签&lt;hr /&gt;来分隔内容。 HTML 注释&lt;!-- This is a commont --&gt;注释： 开始括号之后（左边的括号）需要紧跟一个惊叹号，结束括号之前（左边的括号）不需要。 合理使用注释可以对编码产生帮助 元素的默认行为font 默认为宋体 16pxbody display:block;margin:8px;h1 display:block;font-size:2em;h2 display:block;font-size:1.5em;h3 display:block;font-size:1.17em;h4 display:block;font-size:1emh5 display:block;font-size:0.83em;h6 display:block;font-size:0.67em;]]></content>
      <categories>
        <category>Layout</category>
      </categories>
      <tags>
        <tag>HTML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML 元素]]></title>
    <url>%2F2017%2F04%2F07%2FHTML%20%E5%85%83%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[HTML文档是由HTML 元素定义的。HTML 元素指的是从开始标签（start tag）到结束标签（end tag）的所有代码。开始标签 元素内容 结束标签 &lt;p&gt; This is a paragraph &lt;/p&gt; &lt;a href=“default.html”&gt; This is a link &lt;/a&gt; &lt;br/&gt; 注释: 开始标签通常被称为开放标签（opening tag），结束标签通常被称为闭合标签(closing tag)。 HTML 元素语法 HTML 元素以开始标签起始 HTML 元素以结束标签终止 元素的内容是开始标签与结束标签之间的内容 某些 HTML 元素具有空内容 （empty content） 空元素在开始标签中进行关闭（以开始标签的结束而结束） 大多数HTML 元素可以拥有属性 嵌套的 HTML 元素大多数的 HTML 元素可以嵌套 （可以包含其他 HTML元素）。HTML 文档由嵌套的HTML 元素构成。 &lt;html&gt; &lt;body&gt; &lt;p&gt;This is my first paragraph&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; &lt;html&gt;元素定义了整个HTML文档。这个元素拥有一个开始标签&lt;html&gt;,以及一个结束标签&lt;/html&gt;。元素的内容是另一个HTML 元素（body元素）。 &lt;body&gt;元素定义了HTML文档的主体，这个元素拥有了一个开始标签&lt;body&gt;,以及一个结束标签&lt;/body&gt;。内容元素是另一个HTML元素（p元素）。 &lt;p&gt;元素定义了HTML文档中的一个段落。这个元素拥有一个开始标签&lt;p&gt;,以及一个结束标签&lt;/p&gt;.元素的内容是：This is my first paragraph。 不要忘记结束标签即使忘记了使用结束标签，大多数浏览器也会正确地显示HTML: &lt;p&gt; This is a paragraph &lt;p&gt; This is a paragraph大多数浏览器中都没有问题，但是不能够依赖这种写法。忘记使用结束标签会产生不可预料的结果或错误。注释： 未来的HTML版本不允许省略结束标签。 空的 HTML 元素没有内容的HTML元素被称为空元素。空元素是在开始标签中关闭的。&lt;br&gt; 就是没有关闭标签的空元素（&lt;br&gt;标签定义换行）。在 XHTML、XML以及未来版本的HTML中，所有元素都必须被关闭。在开始标签中添加斜杠，比如&lt;br /&gt;是关闭空元素的正确方法，HTML、XHTML和XML都接受这种方式。即使&lt;br&gt; 目前在所有浏览器中都是有效的，但使用&lt;br/&gt; 是更长远的保障。 HTML 提示: 使用小写标签HTML 标签对大小写不敏感：&lt;P&gt; 等同于 &lt;p&gt;。许多网站都使用大写的HTML标签。W3School 使用的是小写标签，万维网联盟（W3C）在HTML4 中推荐使用小写，而在未来（X）HTML 版本中强制使用小写。]]></content>
      <categories>
        <category>Layout</category>
      </categories>
      <tags>
        <tag>HTML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[方法重写]]></title>
    <url>%2F2017%2F03%2F27%2F%E6%96%B9%E6%B3%95%E9%87%8D%E5%86%99%2F</url>
    <content type="text"><![CDATA[在继承关系的子类中； 方法名相同； 参数相同； 不能缩小访问权限； 不能抛出比父类方法更多或者范围更大的异常； 子类方法的返回值比父类方法的返回值更小或者相同； 父类的静态方法不能重写； 父类的非静态方法不能重写为静态方法； 私有方法不能继承所以也无法重写]]></content>
      <categories>
        <category>java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[类的执行顺序]]></title>
    <url>%2F2017%2F03%2F26%2F%E7%B1%BB%E7%9A%84%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[类的执行顺序父类静态变量或者代码块 &gt;&gt; 子类静态变量或者方法 &gt;&gt; 父类变量、实例代码块、构造方法 &gt;&gt; 子类变量、实例代码块、构造方法 子类123456789101112131415161718public class Son extends Parent&#123; int num1 = 30; static int num2 = 40; &#123; System.out.println(num1); System.out.println("Son实例代码块"); &#125; static &#123; System.out.println(num2); System.out.println("Son静态代码块"); &#125; public Son()&#123; System.out.println("Son构造方法"); &#125;&#125; 父类123456789101112131415161718public class Parent &#123;int num1 = 10;static int num2 = 20; &#123; System.out.println(num1); System.out.println("Parent实例代码块"); &#125;static &#123; System.out.println(num2); System.out.println("Parent静态代码块"); &#125;public Parent()&#123; System.out.println("Parent构造方法"); &#125;&#125; 测试12345public class Test &#123; public static void main(String[] args) &#123; Son son = new Son(); &#125;&#125; 输出结果:1234567891020Parent静态代码块40Son静态代码块10Parent实例代码块Parent构造方法30Son实例代码块Son构造方法]]></content>
      <categories>
        <category>java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[方法重载]]></title>
    <url>%2F2017%2F03%2F26%2F%E6%96%B9%E6%B3%95%E9%87%8D%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[如果实参是基本数据类型：先找带有该类型参数的方法，找不到则扩大范围继续找，如果还是没有才会去找包装类； 如果实参是包装类型：优先找有该包装类型的方法，如果没有匹配，则会找父类，还找不到就会将包装类拆箱匹配。 下面例子控制台会输出“Integer”，如果注释掉 void method(Integer i){ System.out.println(“Integer”); } 就会输出“Object”； public class Test { void method(int i){ System.out.println(&quot;int&quot;); } void method(Integer i){ System.out.println(&quot;Integer&quot;); } void method(Object i){ System.out.println(&quot;Object&quot;); } void method(float i){ System.out.println(&quot;float&quot;); } void method(double i){ System.out.println(&quot;double&quot;); } public static void main(String\[\] args) { Test test = new Test(); test.method(new Integer(2)); }}]]></content>
      <categories>
        <category>java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[构造方法]]></title>
    <url>%2F2017%2F03%2F23%2F%E6%9E%84%E9%80%A0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[方法名和类名完全一致；不能有返回值；没有显式构造方法时，编译器会提供一个默认的无参构造方法，其访问控制修饰符与类一致；如果提供了显式的构造方法，就没有默认的构造方法；构造方法由new或其他构造方法调用。12345678910111213141516171819202122232425public class User &#123; int age; String name; public User()&#123; System.out.println("这是User的无参构造方法"); &#125; public User(int age,String name)&#123; this.age = age; this.name = name; System.out.println("这是User的无参构造方法"); &#125; public static void main(String[] args) &#123; //调用的User类的无参构造方法 User user = new User(); user.age = 47; user.name = "Pony"; System.out.println("年龄："+user.age+",名字："+user.name); //调用的User类的无参构造方法 User user2 = new User(54,"Jack"); System.out.println("年龄："+user2.age+",名字："+user2.name); &#125;&#125; 运行结果1234这是User的无参构造方法 年龄：47,名字：Pony 这是User的无参构造方法 年龄：54,名字：Jack]]></content>
      <categories>
        <category>java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[static关键字]]></title>
    <url>%2F2017%2F03%2F23%2Fstatic-e5-85-b3-e9-94-ae-e5-ad-97%2F</url>
    <content type="text"><![CDATA[static变量static变量属于类，称作类变量； 所有实例共享该变量的值； 用类直接访问； 不允许给静态变量直接赋值非静态变量； int i = 10; static int i2 =num;//这样是不允许的 static方法也成为类方法（普通方法称作实例方法）； 一般是工具类中的方法（不需要频繁创建实例，直接 “类名.静态方法名”的方式调用）； 使用类直接调用（非静态方法和变量需要创建实例才可以访问）； static导入用于导入类中的静态方法或静态属性 import static 包路径.静态方法； 或者用通配符导入所有的静态方法： import static 包路径.*； static代码块类的组成部分之一； 实例化之前执行； 只会执行一次； 作为拓展学习： https://blog.csdn.net/yy304935305/article/details/52456771]]></content>
      <categories>
        <category>java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[equals使用的注意点]]></title>
    <url>%2F2017%2F03%2F21%2Fequals%E4%BD%BF%E7%94%A8%E7%9A%84%E6%B3%A8%E6%84%8F%E7%82%B9%2F</url>
    <content type="text"><![CDATA[equals左边不能为null否则会报空指针异常，如果在不确定equals左边参数的值是否为null的情况下，最好能够谨慎一点，多加判断，以保证程序的健壮。 下面的代码执行会直接报空指针异常 12String s = null;if(s.equals(""))&#123;&#125; 优化一下,这样在执行&amp;&amp;左边第一个判断的时候直接false，&amp;&amp;后面的代码将不再进行运算，这样就不会报空指针异常了（这里使用了org.apache.commons.lang3的StringUtils类用作非null值判断，返回boolean值） 12String s = null;if(StringUtils.isNotEmpty(s)&amp;&amp;s.equals(""))&#123;&#125;]]></content>
      <categories>
        <category>java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sql为什么要写where 1=1]]></title>
    <url>%2F2016%2F11%2F29%2Fsql-e4-b8-ba-e4-bb-80-e4-b9-88-e8-a6-81-e5-86-99where-11%2F</url>
    <content type="text"><![CDATA[这段代码应该是由程序（例如Java）中生成的，where条件中 1＝1 之后的条件是通过 if 块动态变化的。例如: String sql=”select * from table_name where 1=1”; if( conditon 1) { sql=sql+&quot; and var2=value2&quot;; } if(conditon 2) { sql=sql+&quot; and var3=value3&quot;; } where 1=1 是为了避免where 关键字后面的第一个词直接就是 “and”而导致语法错误。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL那些我没怎么用过的函数]]></title>
    <url>%2F2016%2F08%2F22%2Fsql%2F</url>
    <content type="text"><![CDATA[concat(str1,str2,str…)//拼接字符串 转数据类型，两个方法，语法不同效果相同 cast(str as SIGNED) //将字符串转数字 convert(str,SIGNED)//将字符串转数字 substring····· 时间相关：curdate()、now()、DATE_FORMAT()、to_days()、str_to_date() to_days(date) //从0年开始计算到date日期的天数； select to_days(“0000-01-01”) #这里以0000-01-01年为例查询结果就是1； select to_days(“0001-01-01”) #这里以0001-01-01年为例查询结果就是366； select now() //当前时间(格式：2018-01-01 20:30:00)； select curdate() //当前日期(格式：2018-01-01)； select date_sub(curdate(),interval 1 day) #前一天日期; select date_sub(curdate(),interval 1 month) #前个月日期; select date_sub(curdate(),interval 1 year) #前一年日期; select day(date_sub(curdate(),interval 0 day)) #当前几号; select month(date_sub(crurdate(),interval 0 month)) #当前月份; select year(date_sub(curdate(),interval 0 year)) #当前年份;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
</search>
